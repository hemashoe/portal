post_id'post_id'title'description'source_link'body'image'images
0'722980'Пиксельные Пои или Pixel Poi на основе Black Pill'"В этой публикации речь пойдет о применении модуля WeAct  Black Pill V3.0  (MiniF4 V3.0) или ""Черная таблетка"" с AliExpress для самостоятельно изготовления пиксельных пои. Пиксельные пои -..."'https://habr.com/ru/post/722980/'"В этой публикации речь пойдет о применении модуля WeAct Black Pill V3.0 (MiniF4 V3.0) или ""Черная таблетка"" с AliExpress для самостоятельно изготовления пиксельных пои. Пиксельные пои - представляют собой световой реквизит на основе управляемых светодиодов, при вращении которого можно отображать разнообразные картинки, эффекты, надписи и т.п.

Итак, модуль разработки ""STM32F411CEU6 Black Pill"" - отладочный модуль, построен на базе микроконтроллера ARM 32-bit Cortex-M4 STM32F411CEU6. Данный модуль на своем борту имеет три кнопки, SWD порт, кварцевый резонатор, коннектор USB Type-C, два светодиода, место для установки дополнительной SPI Flash памяти и многочисленные выводы по периметру. Используя внешнюю микросхему памяти для хранения настроек и текстур и несколько дополнительных внешних компонентов, Black Pill V3.0 можно превратить в полноценный пои контроллер управления цифровыми пикселями.

В качестве цифровых управляемых светодиодов можно использовать готовые IC LED ленты высокой плотности 144 led/m или даже 200 led/m. Тип IC LED чипа APA102, SK9822 в корпусах 5050, 3535 или 2020. Эти чипы являются высокоскоростными, LED PWM до 20 кГц, частота сигала управления до 30 Мегагерц.

Для более медленных задач, например: колесо Сира, LED веера и т.п. подойдут чипы: WS2812B, WS2813, WS2815 и т.п. Для этих чипов LED PWM составляет до 2 кГц, скорость передачи сигнала управления 800 Kbps.

Схема подключения

Кнопки UP и DOWN служат для управления. Тумблером осуществляется выбор схемы питания: штатная работа, отключено или заряд аккумулятора. Диод SS16 отсекает напряжение USB хоста от питания аккумулятора и LED ленты. Li-On Battery Charger - внешнее зарядное устройство для аккумуляторных батареек выбирается исходя из выбранного аккумулятора. Аккумулятор 18650 +3.7V - высокотоковый литиевый оригинальный аккумулятор, с током отдачи не менее 15-20А. SK9822/APA102 LED лента с количеством пикселей от 1 до 512. Количество и шаг пикселей выбирается исходя из задачи.



Для прошивки микроконтроллера используем встроенные возможности DFU

После прошивки, модуль можно подключить к ПК используя USB кабель и записать исполняемые файлы. Исполняемые файлы имеют расширение *.exepoi, являются контейнерами отображаемых текстур.

Далее можно запустить устройство и проверить корректность работы тестовых файлов. Видео демонстрирует работу тестового файла с частотой отрисовки 100 строк/сек.

Подготовка уникального контента осуществляется в ПО ""Corona POI creator"".

Видео демонстрирует подготовку контента для пиксельных пои в ПО ""Corona POI creator"".

Приведенное ПО бесплатное. Работает в ОС Windows. ПО позволяет проектировать и визуализировать пиксельные пои с различным количеством пикселей: 1...512, выбирать частоту вращения пои и подбирать необходимую частоту отрисовки строк для корректного отображения картинки. Corona POI Creator 1.xx довольно просто для освоения, имеет интуитивно-понятный интерфейс. Имеет возможность совместной работы с ПО Vegas Pro, Adobe Premiere Pro, для синхронизации видео (костюмы например) и пои (текстурных) таймингов.

В итоге, за небольшие деньги и доступной элементной базой, в домашних условиях возможно собрать довольно мощные пиксельные пои со следующими характеристиками:

Количество программ: до 128 количество пикселей: до 512 частота отрисовки: 1...4000 строк/сек объем памяти: 32 MB (более 100 текстур 200x200 pix).

Эта была первая, сокращенная часть публикации. Полный текст с прошивками, тестовыми файлами, ссылками и д.р. материалами Вы можете найти по ссылке: https://iqled.org/forum/viewtopic.php?f=4&t=4023

Вторая часть публикации будет покрывать практическое использование пиксельных пои совместно с работой пиксельных костюмов.

Успешно и продуктивно Вам поработать!"'https://habrastorage.org/getpro/habr/upload_files/d19/3e9/a08/d193e9a08cff2b808ddde53d07766a89.png'"['https://habrastorage.org/r/w1560/getpro/habr/upload_files/c42/7e1/375/c427e1375b90b01d9ee55a48e684667b.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/069/5b3/b5d/0695b3b5d299c517013c3a153d3681a6.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/1db/8dd/035/1db8dd035d8ce6128ded04c437047b03.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/67f/ea3/900/67fea3900d6140b95a25e06ec0279f7d.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/93d/7e3/a7b/93d7e3a7b141c64295ffe8e7f4f8bfb9.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/830/b6e/120/830b6e120b8fc820ff2075a45a714ecb.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/95d/242/ec1/95d242ec1727ff2bd9710b39c3d96aa6.png', 'https://habrastorage.org/getpro/habr/upload_files/d19/3e9/a08/d193e9a08cff2b808ddde53d07766a89.png']"
1'722972'[Перевод] Как создать монорепозиторий с несколькими сервисами, используя Lerna и Yarn?'Для разработчиков построение масштабируемых и поддерживаемых приложений может быть значительным вызовом, особенно при работе с большими кодовыми базами и несколькими сервисами. К счастью,...'https://habr.com/ru/post/722972/'"Для разработчиков построение масштабируемых и поддерживаемых приложений может быть значительным вызовом, особенно при работе с большими кодовыми базами и несколькими сервисами. К счастью, использование подхода монорепозитория в сочетании с мощными инструментами, такими как NX, Lerna и Yarn, может обеспечить упрощенный процесс разработки, который повышает общность кода и возможность повторного использования.

Это подробное руководство нацелено на помощь разработчикам в настройке монорепозитория с использованием NX, Lerna и Yarn. Руководство включает подробную информацию о начальной настройке, структуре каталогов, управлении зависимостями, тестировании и развёртывании.

После освоения туториала разработчик получат полное понимание того, как создавать масштабируемые и поддерживаемые приложения, используя подход монорепозитория. Он также получит знания о необходимых инструментах и знаниях для перехода на новый уровень разработки. Давайте приступим!

Требования

Lerna и Yarn для управления монорепозиторием

NX для контроля версий монорепозиториев

NestJS в качестве API-сервиса, который обслуживает запросы администрирования под префиксом API-эндпоинта /admin

Приложение Angular для административного фронтенда управления сервисом NestJS

Настройка монорепозитория

Пакет common будет содержать код, который может использоваться любым приложением, такой как интерфейсы, enum и т.д.

Пакет common-be предназначен для общих библиотек, используемых исключительно сервисом API

Пакет common-fe должен содержать общие библиотеки только фронтенда Angular

Докеризация приложения с помощью docker-compose.yml и Dockerfile

Репозиторий Git

Все файлы для этого туториала можно найти тут.

Предварительные требования

Yarn

Устанавливаем Yarn и проверяем версию.

С этого момента мы будем использовать Yarn для управления пакетами.

~$ npm install -g yarn** ~$ yarn --version**

Lerna

Устанавливаем Lerna глобально и проверяем версию.

~$ yarn global add lerna ~$ lerna -v

NestJS CLI

Добавляем NestJS CLI в глобальные зависимости и проверяем версию.

~$ yarn global add @nestjs/cli ~$ nest -v

Angular CLI

Устанавливаем Angular CLI как глобальный пакет и проверяем версию.

~$ yarn global add @angular/cli ~$ ng --version

Часть 1: Настройка монорепозитория и структуры каталогов

Что нужно сделать:

Инициализировать монорепозиторий Инициализировать сервисы как рабочие пространства Yarn Добавить приложение NestJS Добавить приложение Angular

Дорожная карта

Сделайте следующее чтобы создать и запустить монорепозиторий с несколькими службами с помощью Lerna и Yarn.

Инициализация монорепозитория

Создайте новый пустой каталог для вашего монорепозитория, в моем случае это lehcode .

~$ mkdir [your-repo-name] && cd [your-repo-name]

Следующая команда инициализирует каталог как Yarn workspace

~/lehcode$ yarn init yarn init v1.22.19 success Saved package.json

Добавьте Lerna в devDependency

~/lehcode$ yarn add -D lerna

Инициализируйте Lerna, выполнив следующую команду; она создаст файл конфигурации lerna.json и каталог пакетов в корне монорепозитория. Ознакомьтесь с документацией по Lerna.

~/lehcode$ npx lerna init

Инициализация сервисов как рабочих пространств Yarn

~/lehcode$ mkdir packages/{api,admin,shared,common-be,common-fe}

Эта команда создает новую структуру каталогов в каталоге packages монорепозитория с пятью подкаталогами: api , admin для приложений; а также shared, common-be, common-fe для общих библиотек. Эти каталоги будут использоваться для исходного кода и файлов конфигурации сервиса API, интерфейса администратора и различных общих библиотек, соответственно. На этом этапе структура папок должна выглядеть примерно так:

. ├── lerna.json ├── node_modules ├── package.json ├── packages │ ├── admin │ │ ├── node_modules │ │ └── package.json │ ├── api │ │ ├── node_modules │ │ └── package.json │ ├── shared │ │ └── package.json │ ├── common-be │ │ └── package.json │ └── common-fe │ │ └── package.json └── yarn.lock

Добавить приложение NestJS в монорепозиторий

Следующие шаги объяснят как добавить приложение NestJS в наш существующий монорепозиторий в качестве рабочей области (workspace) Yarn.

Создайте новое приложение NestJS, выполнив команду:

~/lehcode$ nest new [--dry-run] --directory packages/api/ -p yarn --strict

Эта команда создаёт новое приложение NestJS и настраивает Yarn в качестве менеджера пакетов в каталоге packages/api . Флаг ‑-strict включает строгий режим для компилятора TypeScript.

Попробуйте добавить флаг --dry-run или -d при первом запуске, чтобы имитировать создание приложения NestJS без внесения реальных изменений в файловую систему.

Установите пакет @nrwl/nest в корневую рабочую область, выполнив команду

~/lehcode$ yarn add -D @nrwl/nest ~/lehcode$ cd packages/api/

Настройте сервис API NestJS для обслуживания запросов на администрирование в точке входа /admin , клиентских запросов в точке входа /client . Это можно сделать, изменив конфигурацию маршрутизации в файле app.module.ts.

Вернитесь в корневой каталог монорепозитория.

~/lehcode/packages/api$ cd ../../

Добавить приложение Angular в монорепозиторий

Инициализация внешнего приложения Angular для администрирования серверной части.

~/lehcode$ ng new --create-application --directory=packages/admin --new-project-root=./ --package-manager=yarn --style=scss ? What name would you like to use for the new workspace and initial project? admin ? Would you like to add Angular routing? Yes

Вы можете добавить флаг --dry-run при первом запуске, чтобы имитировать создание приложения Angular без применения фактических изменений.

Перейдите в папку пакета:

~/lehcode$ cd packages/admin/

Вручную запустите yarn dev один раз, чтобы ответить на запрос об обмене данными. В противном случае при сборке контейнер зависнет.

Вернитесь в корень репозитория:

~/lehcode/packages/admin$ cd ../../

Заключительные шаги

Обновите файл package.json в каталоге каждого сервиса (в ./packages/ ), чтобы поле имени было в формате "" <myorg>/<service-name>"" . Это гарантирует, что у каждого сервиса будет уникальное имя пакета, которое можно использовать для управления им с помощью Yarn.

Установите необходимые пакеты @nrwl , выполнив следующую команду

~/lehcode$ yarn add -D @nrwl/workspace @nrwl/nest @nrwl/angular

Общие библиотеки

Добавьте общие библиотеки для API, выполнив команду

~/lehcode$ nx g @nrwl/workspace:lib common-be

Добавьте любой необходимый код в библиотеку common-be и настройте сервис API для использования библиотеки, импортировав её в файл app.module.ts.

Добавьте общие библиотеки для интерфейса администратора, выполнив команду

~/lehcode$ nx g @nrwl/workspace:lib common-fe

Добавьте любой необходимый код в библиотеку common-fe и настройте интерфейс администратора для использования библиотеки, импортировав её в соответствующие файлы.

Часть 2. Настройка и управление версиями Git в монорепозитории с помощью NX

Конфигурация

NX предоставляет удобный способ настройки управления версиями Git в монорепозитории, что позволяет легко управлять тегами версий. С помощью команды nx version вы можете настроить управление версиями для каждого пакета в вашем монорепозитории, и NX будет автоматически генерировать и обновлять теги версии в Git при внесении изменений в пакеты.

Эта функция особенно полезна, когда в вашем монорепозитории есть несколько пакетов, которые зависят друг от друга. Благодаря поддержке версий Git вы можете легко отслеживать изменения и зависимости между пакетами и обеспечивать использование правильных версий.

Команда nx version используется для настройки управления версиями Git в монорепозитории, управляемом NX. Эта часть покажет вам процесс создания и обновления тегов версий в Git для пакетов в вашем монорепозитории.

Выполните следующие действия, чтобы настроить управление версиями Git для своего монорепозитория с помощью NX. NX автоматически обновит номера версий в файлах package.json и создаст теги Git для каждой из новых версий.

Убедитесь, что вы инициализировали Git в своем монорепозитории, выполнив команду в корневом каталоге. Запустите команду $ git init в корневом каталоге монорепозитория. Запустите команду $ nx version в корневом каталоге монорепозитория. Это откроет интерактивную подсказку, которая проведет вас через процесс управления версиями

Выберите, следует ли повышать версии всех пакетов в монорепозитории или только определенного пакета. Выберите версию для обновления (major, minor или patch). Выберите, хотите ли вы создать тег Git для новой версии. Вам будет предложено предварительно просмотреть изменения, которые будут внесены в файлы package.json и историю Git. Подтвердите изменения версии. Нажмите Enter."'https://habr.com/share/publication/722972/cf0082a9e8225b94933625c2f4ddae9d/'"['https://mc.yandex.ru/watch/24049213', 'https://habr.com/share/publication/722972/cf0082a9e8225b94933625c2f4ddae9d/']"
2'706470'Pastilda: ещё одна прошивка'Существует один очень удобный аппаратный менеджер паролей, называемый Пастльда. Подробнее про него можно почитать тут https://habr.com/ru/post/305594/ Так как устройство Open Source(ное), то я...'https://habr.com/ru/post/706470/'"Существует один очень удобный аппаратный менеджер паролей, называемый Пастльда. Подробнее про него можно почитать тут https://habr.com/ru/post/305594/

Так как устройство Open Source(ное), то я составил еще одну прошивку для Пастильды ( Плата Pas~ r 1.1) главным образом, с учетом своих пожеланий.

вот блок-схема гаджета Pas~ r1.1. Всё максимально просто: MCU, USB 2x, SD card, RGB LED и SWD.

Архитектура аппаратной части Пастильды r1.1

Об неудобствах оригинальной прошивки было писано в отчете Betta-тестирования тут https://habr.com/ru/post/694970/

Так как я с 2011 года занимаюсь программированием STM32, то к 2019 году у меня уже была кодовая база в которой было порядка 70% функционала для прошивки Пастильды. Я решил попробовать скомпоновать свою версию *.bin(аря).

В качестве основного стека технологий пришлось выбрать Win10, GCC, C, CMSIS, HAL, FatFs, NoRTOS, Make, Eclipse.

Вы спросите, а что тут, собственно, сложного -то? Написать прошивку для STM32 это же классическая заурядная задача.

С какими трудностями пришлось столкнуться мне?

Трудность №0--Изготовить harness для подключения программатора. Как видите, шаг вилки очень экзотический это 2мм. Пришлось насаживать на PCB вилку и спаивать шлейф из рассыпухи компонентов.

Трудность №1-- На оригинальном устройстве Pas~ r 1.1 отсутствует отладочный UART. Как по мне, дак это очень печально, так как UART нужен для отладки в RunTime. Поэтому пришлось искать отладочные платы с STM32, USB-Host, USB-Device, HW AES256, SDIO, RGB-LED и UART. В интернет магазинах такой платы c полным набором не нашлось. Пришлось отлаживать функционал Пастильды по частям, подобно тому как в математике интегрируют функции по частям, на четырех разных платах: Olimex-STM32-H407, STM32 MINI-M4 и nRF5340-ADK. А программно-независимый код (KeePass Decrypter) я отлаживал и вовсе на x86-64 LapTop(е).

HW атрибут Pas~ r 1.1 Olimex-STM32-H407 STM32 MINI-M4 nRF5340-ADK USB-Host OTG_HS OTG_FS +/- +/- USB-Device OTG_FS OTG_HS +/- +/- HW AES256 + - + + SDIO + + - - UART - + + + SD-micro + + - + SPI - + + + RGB Led + - - + MCU STM32F415RGT6 STM32F407ZGT6 STM32F415RGT6 nrf5340

Трудность №2 --В Pas~ r 1.1 USB FS и HS порты не соответствуют отладочной плате Olimex-STM32-H407. Пришлось писать код с полностью программно-конфигурируемой USB Host и USB Device периферией.

USB-port Pas~ r 1.1 Olimex-STM32-H407 USB-Host OTG_HS OTG_FS USB-Device OTG_FS OTG_HS

вот блок-схема платы Olimex-STM32-H407

Olimex-STM32-H407



Трудность №3 -- Обычно боевой файл *.kdbx не помещается в RAM память микроконтроллера STM32. То, что KeePass расшифровывается в консольном приложении на PC еще не значит, что этот же С-код выполнит эту работу на MCU. У микроконтроллера STM32F415RG всего 128 KByte непрерывной RAM памяти, а мой типичный *.kdbx файл занимал 154K Byte памяти.

Поэтому пришлось реализовывать алгоритм потокового расшифровывания AES256_CBC и потокового синтаксического разбора расшифрованного XML файла кусочками по N(1...1k) byte.

Трудность №4--На микроконтроллере STM32F407ZG на предельной частоте 168 MHz программное AES256 расшифрование KeePass 153kByte файла длится 53 сек! Это очень долго. Поэтому пришлось искать отладочную плату с STM32 и с аппаратным модулем AES256 ECB и AES256 CBC (HW CRYP). Желательно с тем же MCU STM32F415RGT6. Такой платой оказалась сербская плата STM32 MINI-M4. Удалось очень удачно купить её на Avito всего-навсего за 800 RUR (в 11 раз дешевле чем на ЧипДип).

STM32 MINI-M4

Трудность №5 -- На отладочной плате STM32 MINI-M4 отсутствует SDIO, только SPI. Не развели сербы один самый важный провод PD2. Ну бывает. Пришлось писать и покрывать тестами драйвер SD-карты по SPI.

Трудность 7 -- В финальной версии на плате Pas~r1.1 SWD пошаговая отладка показывала, что постоянно возникала, то нехватка стека, то нехватка кучи. Однако удалось настроить константы прошивки так, что модульные тесты стали проходить.

Программные зависимости в моей прошивке для Pas~r1.1 выстроились примерно в такой граф

Дерево зависимостей программных компонентов в прошивке Pas~ r1.1

Что я добавил в новую прошивку для Pas~ r1.1?

1--Возможность вставлять только пароль (Right Enter) не только логин + пароль (Left Enter). Очень многие сайты требуют только пароль.

2--Возможность прокручивать пароли в циклическом массиве. Можно быстро получить доступ к старому паролю в конце KeePass файла.

3--Переключение состояние LED при нажатии кнопки, чтобы была связь, что устройство работает.

4--Диагностическую консоль ~~> при нажатии Left Crtl+Right Crtl.

Вывод:

Если у вас есть Пастильда Pas~ r 1.1 или отладка Olimex-STM32-H407 и вы хотите протестировать альтернативную прошивку аппаратного менеджера паролей, то обращайтесь в личку я пришлю вам *.hex *.bin. Могу даже добавить custom(ные) пожелания.

О том как перепрошить Пастильду v1.1 можно прочитать инструкцию тут https://habr.com/ru/post/698964/

Links

https://habr.com/ru/post/698964/

https://habr.com/ru/post/694970/

https://habr.com/ru/post/346820/

https://habr.com/ru/post/305594/

https://habr.com/ru/company/thirdpin/blog/407633/

https://habr.com/ru/company/thirdpin/blog/466533/

https://habr.com/ru/post/305602/"'https://habrastorage.org/getpro/habr/upload_files/2bb/cbe/48c/2bbcbe48cb4001eb1a8dc382f802c193.JPG'"['https://habrastorage.org/r/w1560/getpro/habr/upload_files/7db/b0f/039/7dbb0f039692fe3e763fc2b86e3680bd.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/038/13a/b1c/03813ab1c401a466036f8b16c682c085.png', 'https://habrastorage.org/getpro/habr/upload_files/72e/cb5/dd8/72ecb5dd8caca4f425bc33ef1ab63256.JPG', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/d03/313/7f7/d033137f778e251ce5c00b746d8824b7.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/73e/404/45a/73e40445a6772a0b71d8703a63119966.png', 'https://habrastorage.org/getpro/habr/upload_files/2bb/cbe/48c/2bbcbe48cb4001eb1a8dc382f802c193.JPG', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/0ef/9c9/6ac/0ef9c96ac742c4b522cdd529a6cb41c6.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/7d8/e77/0a1/7d8e770a137447e81270ec9bac0d27ad.png', 'https://habrastorage.org/getpro/habr/upload_files/9ed/c1c/a4f/9edc1ca4fe6ff5f3c200e98f4462c368.JPG']"
3'722952'Гугл таблица как бд для телеграм бота'В этом посте рассмотрим в деталях, как непосредственно использовать гугл таблицы в качестве базы данных. Попробуем с нуля написать бота, который забирает вопросы квиза с вариантами ответов из таблицы...'https://habr.com/ru/post/722952/'"В этом посте рассмотрим в деталях, как непосредственно использовать гугл таблицы в качестве базы данных.

Попробуем с нуля написать бота, который забирает вопросы квиза с вариантами ответов из таблицы и записывает ответы назад.

Ниже представлен скрин из чата с ботом.

Дисклеймер: если вы здесь в первый раз, пожалуйста ознакомьтесь с первым постом, где более детально разобраны скрипты в гугл таблицах (тыц).



Начнем с создания контейнера и написания скрипта в нем. Создаем новый Spreadsheet.

В качестве забираемых значений укажу 4 вопроса на листе Questions.

И варианты ответов для каждого вопроса на листе Answers. Данные из двух таблиц связаны по ИД – первая колонка.

В таблицу Users буду вносить ответы от пользователей, но об этом позже.

Переходим к скрипту. Укажу в качестве глобальных переменных следующие значения:

const doc = SpreadsheetApp.getActive(); const questionsSheet = doc.getSheetByName(""Questions""); const answersSheet = doc.getSheetByName(""Answers""); const usersSheet = doc.getSheetByName(""Users"");

Так как скрипт мы разворачиваем в том же документе (контейнере), то нам достаточно использовать метод getActive(). Если вы ссылаетесь на другой Spreadsheet, то можно на него сослаться методами .openByUrl() или .openById().

Каждый лист я записываю в отдельные переменные, чтобы было удобно обращаться к листам из любого места скрипта.

Теперь попробуем забрать значения из таблицы и вывести в логере. Следующая функция забирает значения из таблицы и возвращает их в виде массива.

function sendQuestions() { const questionsArr = questionsSheet.getRange(1,1,questionsSheet.getLastRow(), questionsSheet.getLastColumn()).getValues() Logger.log(questionsArr) }

Запустить функцию можно из меню, выбрав имя функции и кликнув Run

Меню

Результатом выполнения функции будет массив в логере.

Логер

Разберем строку в функции по частям.

const questionsArr = questionsSheet.getRange(1,1,questionsSheet.getLastRow(), questionsSheet.getLastColumn()).getValues()

Здесь мы объявляем переменную для записи в нее возвращаемого массива. Далее указываем лист, из которого будем забирать значения questionsSheet. Для листа выбираем метод .getRange() и указываем ячейки, к которым функция должна обратиться.

Перевожу написанное в скобках метода getRange на понятный язык:

getRange(номер строки начала диапазона , номер столбца начала диапазона , номер строки конца диапазона , номер столбца конца диапазона)

Методами листа .getLastRow() и .getLastColumn() получаем номер последней строки и столбца, которые будут равны 4 и 2 соответственно. При этом методы возвращают последние столбец и строку, в которых указано значение (ячейка не пуста) или есть валидация.

Границы заданного диапазона

В то же время можно указать в скобках questionsSheet.getRange(""A1:B4"").

Возвращаемся к функции и к методу диапазона .getValues(). Здесь обратим внимание на то, что существует также метод .getValue(), применять который следует, если мы обращаемся к одной ячейке, например questionsSheet.getRange(3,4) или questionsSheet.getRange(""D3"").

getValues() при этом возвращает двумерный массив, а getValue() - значение.

*Попробуйте обратиться к разным диапазонам с использованием разного синтаксиса и понаблюдайте за возвращаемыми значениями. После нескольких попыток обращение к диапазонам станет интуитивным.

Итак, функция вернула двумерный массив, соответственно мы можем продолжить работу с массивом и его методами.

По задумке бот отправляет все вопросы одновременно с кнопками. Сначала попробуем просто отправить вопросы без вариантов ответов.

Обратимся к функции send().

function send(msg, chat_id) { const payload = { 'method': 'sendMessage', 'chat_id': String(chat_id), 'text': msg, 'parse_mode': 'HTML' } const data = { 'method': 'post', 'payload': payload, 'muteHttpExceptions': true } UrlFetchApp.fetch('https://api.telegram.org/bot' + token + '/', data); }

В функцию мы передаем текст отправляемого ботом сообщения и ид чата, в который это сообщение будет отправлено. Внутри функции объявляем объект payload и указываем передаваемые параметры. Я использую только необходимые в рамках задачи параметры (method, chat_id, text, parse_mode), но их может быть и больше (ссылка на апи телеграмма для метода sendMessage).

В переменную data передаем payload (передаваемые параметры для метода апи ""post"") и указываем сам метод post.

В строке UrlFetchApp мы обращаемся к АПИ телеги по ссылке (таким же образом можно обратиться к любому другому доступному АПИ). В этой же строке мы передаем значение переменной token и data. Про token чуть позже.

Вернемся к функции sendQuestions() и вызовем из нее send() для каждого элемента массива questionsArr.

Чтобы обратиться к каждому элементу можно воспользоваться циклами типа for или while. Но я предпочитаю использовать методы массивов и стрелочные функции, которые умещаются в одну строку.

Таким образом, вместо конструкции

for (let i=0; i<questionsArr.length; i++) { send(questionsArr[i][1],chat_id) }

я могу написать

questionsArr.forEach(e => send(e[1],chat_id))

Итак, в качестве текста сообщения я забираю из вложенного массива элемент на позиции [1] (на позиции 0 указан ид вопроса).

Нам не достает только чат ид, который я укажу в качестве аргумента функции sendQuestions() и получу следующее:

function sendQuestions(chat_id) { const questionsArr = questionsSheet.getRange(1,1,questionsSheet.getLastRow(), questionsSheet.getLastColumn()).getValues(); Logger.log(questionsArr); questionsArr.forEach(e => send(e[1],chat_id)); }

Наконец, функцию sendQuestions(chat_id) мы вызовем, когда пользователь запустит бота по команде /start. Пропишем эту логику.

Сначала добавлю функцию, которая парсит возвращаемый из телеграма json пакет.

function doPost(e) { const update = JSON.parse(e.postData.contents); let msgData = {} if (update.hasOwnProperty('message')) { msgData = { id : update.message.message_id, chat_id : update.message.chat.id, user_name : update.message.from.username, text : update.message.text, is_msg : true }; } }

Результатом функции является объект msgData с ключами id, chat_id, user_name, text, is_msg. Возвращаемые ключи вы можете определить сами, для простоты я указала только те, которые нужны для задачи.

Здесь же я передам в функцию отправки вопросов значение ключа chat_id.

function doPost(e) { const update = JSON.parse(e.postData.contents); let msgData = {} if (update.hasOwnProperty('message')) { msgData = { id : update.message.message_id, chat_id : update.message.chat.id, user_name : update.message.from.username, first_name : update.message.from.first_name, text : update.message.text, date : update.message.date/86400+25569.125, is_msg : true }; } sendQuestions(msgData.chat_id); }

Как создать бота и получить токен уже описывалось здесь. Берем этот токен и записываем в глобальную переменную token. Далее деплоим приложение (смотреть ссылку выше) и записываем URL веб приложения в переменную appLink.

И наконец функция api_connector() для установки веб хука.

function api_connector() { const appLink = ""Ваш URL""; UrlFetchApp.fetch(""https://api.telegram.org/bot""+token+""/setWebHook?url=""+ appLink); }

В этой функции мы опять же отправляем запрос на сервер по адресу https://api.telegram.org и дополнительно указываем метод и/или параметры. В данном случае используется метод setWebhook и параметр url, куда мы передаем значение переменной appLink.

Запускаем эту функцию по кнопке Run.

После успешной установки вебхука запускаем бота в телеграме. В ответ получаем вопросы

Продолжение следует...

Здесь я пожалуй прервусь, т.к. не хочу писать методичку для лабы. Попробуйте решить эту маленькую задачку, поиграйте с методами для указания диапазонов и методами массивов.

Как всегда, рада обратной связи, дайте знать, если пишу слишком сложно\ неразборчиво\ без деталей..

Все вопросы по реализации можете адресовать напрямую в мой телеграм.

Продолжение постараюсь подготовить в разумные сроки и показать, как массивы с ответами на вопросы превратить в кнопки. Будет интересно! 😉"'https://habrastorage.org/getpro/habr/upload_files/23b/cc9/c15/23bcc9c1571e20137df4758e959c2099.png'"['https://habrastorage.org/r/w1560/getpro/habr/upload_files/9ae/7ab/e9e/9ae7abe9e74f5c0979d04643ba1108be.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/cbc/2f8/2a0/cbc2f82a0ad646769dfc597e59a5e2f2.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/a98/cf9/537/a98cf9537542573553146baca3005499.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/23b/cc9/c15/23bcc9c1571e20137df4758e959c2099.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/27d/455/a4e/27d455a4e43e15d358084519668fd0ad.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/082/983/622/08298362227735338453fe7b8ae27c81.png', 'https://habrastorage.org/getpro/habr/upload_files/23b/cc9/c15/23bcc9c1571e20137df4758e959c2099.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/139/a48/56f/139a4856f32a2891c0af3da262cfb434.png']"
4'722946'Как я проводил стажировку продактов'"Привет! Меня зовут Владислав, я Head of Product в онлайн-школе  ProductStar.ru , где мы обучаем IT-профессиям (в том числе и Product Manager'ов). В этой статьей расскажу про свой первый опыт..."'https://habr.com/ru/post/722946/'"Привет! Меня зовут Владислав, я Head of Product в онлайн-школе ProductStar.ru, где мы обучаем IT-профессиям (в том числе и Product Manager'ов). В этой статьей расскажу про свой первый опыт проведения стажировки для продакт менеджеров.

В ноябре 2022 года я решил нанять Junior Product Manager, чтобы усилить нашу команду продукта. В этом же месяце запустил конкурс на стажировку продактов среди наших студентов ProductStar с последующим трудоустройством в отдел продукта. Кратко расскажу как это было и какие выводы я сделал.

Конкурс на стажировку

Я сразу предполагал, что желающих будет много, а на стажировку запустить больше двух человек физически не смогу, т.к. с ребятами нужно постоянно работать во время стажировки и уделять им много времени, а мои задачи никто не отменял. Поэтому подготовил конкурс из нескольких этапов, которые нужно было успешно пройти для того, чтобы попасть на стажировку.

Этапы конкурса

Я сразу предполагал, что желающих будет много, а на стажировку запустить больше двух человек физически не смогу, т.к. с ребятами нужно постоянно работать во время стажировки и уделять им много времени, а мои задачи никто не отменял. Поэтому подготовил конкурс из нескольких этапов, которые нужно было успешно пройти для того, чтобы попасть на стажировку.

Отклик и заполнение анкеты: 28 человек. Решение тестового задания: Получили тестовое задание: 21 человек.

Сдали тестовое задание: 19 человек.

Успешно выполнили тестовое: 11 человек. Собеседование со мной: 11 человек. Решение практического кейса во время созвона: 4 человека. Приглашение на стажировку: 2 человека.

Было сложно принимать решение кого все же брать на стажировку, т.к. ребята, которые дошли до 4 этапа были сильные и некоторые даже с опытом. По итогу на стажировку прошло два студента (Света и Рома), которые ранее продактами не работали. А весь конкурс занял чуть меньше месяца.

Стажировка

И вот, 15 декабря ребята вышли на стажировку, которая продлилась 5 недель.

Продуктом нашего будущего Junior Product Manager будет обучающая платформа ProductStar, поэтому вся стажировка строилась вокруг платформы и только на реальных задачах, которые мы потом брали в работу.

За время стажировки ребятам нужно было провести исследования, понять основные боли и потребности пользователей при обучении на платформе ProductStar. А также сделать выводы, определить ключевую задачу на ближайший квартал и начать ее проработку.

План стажировки

1 неделя. Знакомство с командой разработки и изучение документации.

2 неделя. Проведение CustDev активных студентов ProductStar.

3 неделя. Проведение анкетирования среди всех студентов.

4 неделя. Подведение итогов исследования, выводы и определение ключевой задачи на текущий квартал.

5 неделя. Проработка ключевой задачи квартала: описание плана действий, оценка рисков и необходимых ресурсов, описание необходимого функционала и тех. требований, составление прототипа интерфейса. Подведение итогов стажировки и самоанализ результатов.

Итоги стажировки

За это время стажеры проявили себя только с лучшей стороны и выложились на 100%, что иногда у меня даже возникал вопрос ""А вы точно продактами раньше не работали?"". И очень круто использовали свой предыдущий опыт из других сфер. Рома ранее работал бренд-менеджером и этот опыт ему помог с проведением анкетирования пользователей, а Света до этого работала в EdTech с LMS-системой iSpring, что позволило ей лучше проработать прототип интерфейса.

Для подведения итогов стажировки я создал себе таблицу, где указал 20 параметров (soft skills, hard skills, качество и скорость выполнения задач, оценка коллег и т.д.), по которым оценивал каждого стажера. А все потому что очевидного лидера за время стажировки не образовалось и выбор сделать было не просто!

Свой выбор я остановил на Свете (14 баллов у нее, против 10 баллов у Ромы) и не прогадал. Света уже больше месяца работает в нашей команде, быстро погрузилась во все задачи, оптимизировала несколько процессов и активно работает над задачей квартала. Рому мы тоже не бросили, с трудоустройством ему активно помогает наш Карьерный Центр.

Выводы

Однозначно рекомендую и другим компаниям проводить стажировки для специалистов уровня Junior. Полученный результат стоил затраченного времени на конкурс и стажировку, я смог на практике проверить понравившихся мне кандидатов и выбрать лучшего, а ребята получили реальный опыт работы продактом в IT-компании. Хоть это и заняло у меня достаточно много времени.

Уже в этом месяце я запустил еще одну стажировку, но уже для Junior QA инженеров. Все новые вакансии и стажировку публикую у себя в linkedin.

И также ниже приведу топ-3 плюса и минуса стажировки со стороны компании, а именно нанимающего менеджера.

Плюсы стажировки

К моменту трудоустройства новый сотрудник будет больше погружен в специфику работы отдела, благодаря чему он быстрее покажет результат.

Возможность проверить навыки кандидатов на практике.

Поймете насколько кандидату близки ценности компании и формат работы, а также насколько вам комфортно работать друг с другом.

Минусы стажировки

Долгий цикл вывода сотрудника в штат (2 месяца от начала отбора кандидатов до завершения стажировки).

Стажировка займет у вас (как у руководителя) много времени.

Есть большой шанс, что во время стажировки кто-то из кандидатов отвалится. Поэтому на стажировку лучше набирать сразу несколько человек.

А вы проходили стажировки или может быть тоже организовывали их? Делитесь своим опытом в комментариях!"'https://habrastorage.org/getpro/habr/upload_files/ea9/2a5/fad/ea92a5fad92de45ad368e421d71cd4a0.png'"['https://habrastorage.org/getpro/habr/upload_files/ea9/2a5/fad/ea92a5fad92de45ad368e421d71cd4a0.png', 'https://habrastorage.org/r/w48/getpro/habr/avatars/09e/fb8/0e9/09efb80e9b451a24bef1ce4a5ef6fc15.jpg', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/ea9/2a5/fad/ea92a5fad92de45ad368e421d71cd4a0.png', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/978/80d/82a/97880d82a74688d9da6d320bf28858c8.jpeg', 'https://habrastorage.org/getpro/habr/avatars/09e/fb8/0e9/09efb80e9b451a24bef1ce4a5ef6fc15.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/1fb/197/360/1fb19736008bfa1fa5cf26fd339b6aec.png']"
5'722940'Что в DI-Контейнере твоем, С++? Пробуем написать'Доброго времени суток, жители Хабра. Из-за наличия довольно большого опыта разработки на C# мне хотелось наличия таких же удобных DI-контейнеров на C++. Особенно после того, как побывал на нескольких...'https://habr.com/ru/post/722940/'"Доброго времени суток, жители Хабра.

Из-за наличия довольно большого опыта разработки на C# мне хотелось наличия таких же удобных DI-контейнеров на C++. Особенно после того, как побывал на нескольких плюсовых проектах, где были фабрики, синглтоны и прочие штуки. И я начал искать, хотя ожидал, что многого я не найду.

И был прав, так как рефлексию в C++ ещё не завезли. Хотя и существует reflection ts и даже была статья на хабре, но пока частью стандарта это не стало. Я бы желал, чтобы ещё из этого выросла бы библиотека для создания динамической рефлексии, по умолчанию выключенной, включаемой атрибутами. Но это всё мечты.

Итак, что же было найдено?

Фреймворки:

Несколько статей и видеороликов:

Эти контейнеры имеют те или иные недостатки. Fruit, например, подразумевает использование макросов на конструкторе класса, подвергаемого инъекции. Wallaroo использует шаблоны на полях класса. Kangaru уже использует внешние классы сервисы для связывания зависимостей, чем связывает их статически. Hypodermic уже выглядит основательно, и код на гитхабе приятный и читаемый. Boost.DI... ну... я попытался посмотреть исходник...

Всё это для было довольно громоздко, хотелось своего и простого, по-велосипедному понятного... и вот я начал писать.

Перед началом пути

В начале я так или иначе задавался вопросом: ""А чего я хочу от контейнера?"". И для себя вывел несколько требований:

Небольшой объём и относительную простоту кода.

Отсутствие необходимости вмешиваться конструкцию класса, в который будут передаваться зависимости.

Отсутствие макросов. Не то чтобы я их не люблю. У них есть прикольные возможности, но я, как пользователь библиотеки, не хотел бы, чтобы сторонняя библиотека мусорила мне в подсказки IDE. Учитывая любовь C++ втягивать в себя возможности из других языков может когда-нибудь подтянут макросы по типу Rust'а.

Использовать разные умные и не очень указатели, и ссылки.

Постройка контейнера не во время компиляции.

Сначала я хотел попробовать внедрять в конструктор любые типы, а не только указатели, но это сразу порождает проблемы.

Основа контейнера

Основой DI контейнеров в С++ является конечно же рефлексия шаблоны. Hypodermic умудряется с их помощью выводить типы аргументов для конструктора. Конечно, в мире С++ существуют инструменты для генерации метаданных, вроде moc от Qt или UnrealHeaderTool от UnrealEngine. Просто всё это сторонние инструменты, и люди не всегда хотят от них зависеть.

Для того, чтобы создать объект, необходимо определить типы аргументов для конструктора. А в чём проблема, если исключить отсутствие рефлексии в плюсах? Конструктор не является обычной функцией. Если для функции и есть довольной простой способ получить её аргументы, то с конструктором такой способ не подходит.

Hypodermic это делает с помощью нескольких вспомогательных шаблонных классов. Код ниже из самого Hypodermic

struct ArgumentResolverInvoker { template <class T, class = /*scary code*/> operator T() { return ArgumentResolver< typename std::decay< T >::type >::template resolve(m_registration, m_resolutionContext); } }

Компилятор пытается вывести тип оператора приведения для ArgumentResolverInvoker и тем самым сообщает тип аргумента конструктора, и там позади ещё несколько классов которые в этом помогают.

Я же выбрал путь через функции, что автоматически означает необходимость написание фабричного метода. Да в общем то оно и неплохо. Этот метод я подсмотрел в одной из книг , там был пример реализации быстрых делегатов на плюсах.

template<class FunctionType> struct Factory; template<class Type, class ... Args> struct Factory<Type(Args...)> { static void* Create(Container* container) { return TypeTraits<Type>::Create(container->resolve<Args>()...); } }

Этот метод использует основан на том, что компилятор старается использовать, как можно более узкоспециализированную версию шаблона, и когда мы напишем

Factory<decltype(TypeTraits<Foo>::Create)>

Компилятор отправится ко второй версии и выведет для нас все необходимые типы.

TypeTraits это вспомогательный шаблон, который нужно будет специализировать под каждый тип, который мы захотим добавить в контейнер. Он должен будет содержать метод Create, повторяющий сигнатуру конструктора создаваемого типа.

Код примерный и не будет просто так работать. Например, Type скорее всего нужно будет отчистить от указателя.

Но описывать каждый раз метод Create немного... грустно. Но здесь мы опять можем выкрутиться шаблоном

template<class T, class... Args> struct Constructor { T* Create(Args... args) { return new T(std::forward<Args>(args)...); } }; template<> struct TypeTraits<Foo> : Constructor<Foo, Dependency1...> { // TypeTraits не только для Create существует constexpr LifeTimeScope LifeTime = LifeTimeScope::Singleton; // ... };

Отдельный метод Create в TypeTraits и сам TypeTraits позволяет на много полезных вещей:

Классы из сторонних библиотек можно поместить в контейнер, так как нет необходимости изменять их, описав TypeTraits. Использовать собственные аллокаторы в Create Вызвать дополнительные методы пост-инициализации.

Впрочем, я догадываюсь, что упомянутые DI фреймворки способны позволить сделать всё то же самое.

Находим Id для типа

Хорошо, у нас есть класс фабрики. Дальше нам нужно этот класс зарегистрировать в контейнере. Для этого всего лишь нужно взять адрес Factory::Create и положить его в map. Затем, когда мы захотим получить объект нужного нам типа, мы достаем метод из мапы и создаём объект. А как мы его создаём? Ведь аргумент шаблона метода Resolve это объект времени компиляции, а map это объект времени выполнения: одно в другое просто так не переходит. Для этого мы воспользуемся ещё одним вспомогательным шаблонный методом.

template<class T> size_t GetTypeId() { return reintepret_cast<size_t>(&GetTypeId); }

Да, мы воспользуемся адресом шаблонной функции как ключом в map. Благодаря тому, что каждая функция будет иметь своё место памяти, будет достигнута уникальность id. Мне сначала пришла идея использовать адрес метода Create для этой цели, но если мы захотим сопоставить тип интерфейса и тип реализации этого интерфейса, метода Create для интерфейса у нас может и не быть.

Управление временем жизни и хранение указателей

Практически весь остальной код контейнера это дело техники. Следующие вопросы, которые могут возникнуть это, например, как обращаться с временем жизни.

Я выделил 3 типа:

Синглтон - живет всё время, пока жив контейнер и клиенты. Подсчет ссылок - объект существуют, пока живы клиенты пользующиеся этим объектом Никак - клиент сам решает, что с этим делать.

Первые 2 отличаются тем, какой умный указатель хранить в контейнере: shared_ptr или weak_ptr, и что можно передавать клиенту. Передавать ссылку, имея weak_ptr в контейнере, как можно догадаться, приведёт к проблемам.

И как же указатели хранить в контейнере? Для это решил использовать std::variant. Благодаря ему можно свести все указатели в один. К тому же я использую shared_ptr<void> и т.п. для хранения указателей. Несмотря на немного дурной тон это позволило мне написать обобщенный не шаблонный метод Resolve, и оставить в шаблоне, только то, что действительно в этом нуждается.

Приведение к типу, требуемому клиентом

Когда мы вызываем метод Resolve в фабрике мы получаем тип аргумента

static void* Create(Container* container) { return TypeTraits<Type>::Create(container->resolve<Args>()...); }

И, вероятно, что типом будет какой-нибудь shared_ptr<Dependency> или Dependency&, но в контейнере зарегистрирован Dependency, а не перечисленный из этих двух.

Поэтому мы говорим: ""Больше шаблонов богу шаблонов"" и идём дальше.

template<class T, class ... Args> using Reference = T&; template<class T, class ... Args> using SharedPtr = std::shared_ptr<T>; template<class T> struct WrapperInfo { }; template<class T> struct WrapperInfo<T&> { using Type = T; template<class P, class ... PArgs> using Wrapper = Reference<P>; }; template<class T, template <class P, class ... PArgs> class TWrapper, class ... TArgs> struct WrapperInfo<TWrapper<T, TArgs...>> { using Type = T; template<class P, class ... PArgs> using Wrapper = TWrapper<P>; };

Снова используем частичную специализацию шаблонов для выяснения, какой же тип у нас в оригинале. И можем переписать метод Create в соответствие со следующим кодом

static void* Create(Container* container) { return TypeTraits<Type>::Create( container->resolve<typename WrapperInfo<Args>::Type, WrapperInfo<Args>::template Wrapper>()... ); }

Теперь метод Resolve знает, что и в каком виде ему надо вернуть.

Завершение

Всего около 500 строк кода, и в наличии вполне рабочий контейнер. Код своего мини DI контейнера я выложил на гитхабе. Пусть это и не сравнится серьезными фреймворками, но надеюсь, что этой базовой функциональности в моих дальнейших разработках хватит."'https://habr.com/share/publication/722940/587d82e293efcb7678fa29ac3c6dbf33/'"['https://habrastorage.org/getpro/habr/avatars/39d/157/034/39d157034be3f043418be1ce5bb7f6ba.jpg', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w48/getpro/habr/avatars/39d/157/034/39d157034be3f043418be1ce5bb7f6ba.jpg', 'https://habr.com/share/publication/722940/587d82e293efcb7678fa29ac3c6dbf33/']"
6'722922'Пошаговое руководство: как склеить старые дропы с молодым доменом'Продвижение молодого сайта может быть сложной задачей, особенно если конкуренция в нише высокая. Одним из способов ускорить процесс и повысить видимость сайта является использование старых дроп...'https://habr.com/ru/post/722922/'"Продвижение молодого сайта может быть сложной задачей, особенно если конкуренция в нише высокая. Одним из способов ускорить процесс и повысить видимость сайта является использование старых дроп доменов. В этом пошаговом руководстве мы рассмотрим, как правильно выбрать и склеить старые дропы с молодым доменом для улучшения SEO-показателей.

Шаг 1: Выбор подходящих дроп доменов

Искать дроп домены можно на специальных площадках, аукционах и через сервисы, которые предоставляют информацию о свободных доменах.

Рассмотрим основные способы и ресурсы для поиска дроп доменов.

Регистраторы доменов

Некоторые регистраторы доменов предоставляют списки доменов, которые скоро освободятся или уже свободны. Проверьте информацию на сайтах популярных регистраторов, таких как GoDaddy, Reg.ru, Ru-Center и других.

Аукционы доменов

На аукционах доменов можно найти интересные дроп домены, которые продавцы готовы продать по разным причинам.

Некоторые популярные аукционы доменов:

GoDaddy Auctions (https://auctions.godaddy.com/)

NameJet (https://www.namejet.com/)

SnapNames (https://www.snapnames.com/)

Sedo (https://www.sedo.com/)

Сервисы для поиска и анализа дроп доменов

Существует ряд специализированных сервисов, которые помогают найти и оценить дроп домены, учитывая такие параметры, как возраст, трафик, внешние ссылки и другие SEO-показатели.

Вот некоторые из них:

Поиск в поисковых системах и форумах

Иногда полезные дроп домены можно найти, просто исследуя списки доменов в поисковых системах или на форумах, посвященных продаже и покупке доменов. Введите в поисковике запросы, связанные с дроп доменами, и изучите результаты. Также посетите тематические форумы и сообщества, где пользователи могут предлагать дроп домены на продажу.

Когда вы найдете потенциально интересный дроп домен, не забудьте провести его детальный анализ, чтобы убедиться в отсутствии проблем и оценить его возможности для использования в вашем проекте.

Для успешного приклеивания дроп доменов к молодому сайту, учти следующие критерии при выборе домена:

Domain Authority (DA) - не менее 30. Этот показатель отражает авторитет домена в глазах поисковых систем.

Page Authority (PA) - также желательно иметь значение не менее 30. PA оценивает авторитетность отдельных страниц домена.

Возраст домена - старше 2-3 лет. Более старые домены обычно имеют больше доверия со стороны поисковых систем.

Количество внешних ссылок (Backlinks) - не менее 100. Наличие большого числа качественных ссылок указывает на авторитетность домена.

Количество реферальных доменов (Referring Domains) - не менее 50. Это количество уникальных доменов, ссылающихся на дроп домен. Чем больше разнообразие источников ссылок, тем лучше.

Trust Flow (TF) - не менее 20. Этот показатель оценивает доверие к домену на основе качества ссылок, которые на него ссылаются.

Citation Flow (CF) - не менее 20. CF измеряет влияние домена на основе количества ссылок, ссылающихся на него.

Отсутствие санкций от поисковых систем. Убедись, что домен не подвергался штрафам или блокировкам со стороны Google или других поисковых систем. Это можно проверить через инструменты вроде Google Search Console или специализированных сервисов.

Шаг 2: Проверка истории дроп домена

Перед покупкой домена убедись, что он не имеет негативной репутации или санкций со стороны поисковых систем. Воспользуйся инструментами, такими как Ahrefs, SEMrush, Majestic и др.

Проверка истории домена с использованием сервиса Wayback Machine

Откройте сайт Wayback Machine (https://archive.org/web/)

Введите домен, который хотите проверить, и нажмите Enter

Изучите сохраненные снимки сайта в разные периоды времени, чтобы узнать, какой контент размещался на домене ранее

Анализ истории внешних ссылок

Воспользуйтесь SEO-инструментами, такими как Ahrefs, SEMrush или Majestic

Введите домен и изучите информацию о внешних ссылках, включая их количество, источники и качество

Проверка наличия санкций со стороны поисковых систем

Вы можете использовать сервисы, такие как SeoLik (https://seolik.ru/check-sanctions-yandex), для определения возможных проблем

Проверка наличия активных страниц и индексации

Введите в поисковую систему Google запрос ""site:вашдомен.com"" (без кавычек), заменив ""вашдомен.com"" на проверяемый домен

Убедитесь, что поисковая система индексирует страницы домена и не обнаруживает проблем с доступом или индексацией

Шаг 3: Покупка дроп домена и настройка 301-редиректа

Купи выбранный дроп домен и настрой на нем 301-редирект на свой молодой сайт. Это позволит передать существующий вес старого домена на новый сайт, улучшая его позиции в поисковой выдаче.

Настройка 301-редиректа с дроп домена на основной сайт может быть выполнена разными способами, в зависимости от используемого веб-сервера и платформы управления сайтом. Рассмотрим два наиболее распространенных способа для серверов Apache и Nginx, а также для платформы WordPress.

Настройка 301-редиректа на сервере Apache с помощью файла .htaccess:

Откройте файл .htaccess на дроп домене или создайте новый, если его нет

Вставьте следующий код, заменив ""основнойсайт.com"" на адрес вашего основного сайта:

RewriteEngine On RewriteCond %{HTTP_HOST} ^дропдомен\.com$ [OR] RewriteCond %{HTTP_HOST} ^www\.дропдомен\.com$ RewriteRule ^(.*)$ https://основнойсайт.com/$1 [R=301,L]

Сохраните изменения и закройте файл

Настройка 301-редиректа на сервере Nginx:

Откройте файл конфигурации Nginx для дроп домена, обычно он находится в /etc/nginx/sites-available/ или /etc/nginx/conf.d/

Добавьте следующий код в блок ""server"", заменив ""дропдомен.com"" и ""основнойсайт.com"" на соответствующие адреса:

server { server_name дропдомен.com www.дропдомен.com; return 301 https://основнойсайт.com$request_uri; }

Сохраните изменения и перезапустите Nginx с помощью команды:

sudo service nginx restart

Настройка 301-редиректа для WordPress с помощью плагина:

Установите и активируйте плагин ""Redirection"" в панели управления WordPress на дроп домене

Перейдите в раздел ""Инструменты"" > ""Redirection"" и создайте новое правило перенаправления:

Source URL: ^/.*

Target URL: https://основнойсайт.com/"" class=""formula inline"">1

Regex: отметьте галочкой

Group: выберите ""Redirections""

Position: оставьте по умолчанию

Нажмите кнопку ""Add Redirect"" для сохранения настройки

После выполнения одного из этих методов, все запросы к дроп домену будут автоматически перенаправляться на основной сайт с помощью 301-редиректа, что позволит передать ссылочный вес и посетителей с дроп домена на основной сайт.

Шаг 4: Оптимизация профиля внешних ссылок

Работай над профилем внешних ссылок, удаляя некачественные и токсичные ссылки, а также стремись получать новые качественные ссылки на молодой домен. Это усилит эффект от приклеивания дроп домена и улучшит показатели сайта.

Оптимизация профиля внешних ссылок – это процесс анализа и улучшения качества ссылок, которые ведут на ваш сайт. Четкие цифры для оценки качества ссылок могут отличаться для разных сайтов и ниш, однако существуют некоторые общие параметры, на которые стоит обратить внимание.

Авторитетность источника

Доменный авторитет (DA): хорошие ссылки имеют DA от 20 и выше, плохие – ниже 10.

PageRank (PR): хорошие ссылки имеют PR от 2 и выше, плохие – ниже 1.

Цитатный и трестовый индексы (Yandex): хорошие ссылки имеют значения от 10 и выше, плохие – ниже 5.

Релевантность источника

Тематическая связь: хорошие ссылки получены с сайтов, тематика которых схожа с вашей нишей, плохие – с несвязанных ресурсов.

Контекстность: хорошие ссылки встроены в содержательный контент, плохие – размещены в футерах, сайдбарах или в качестве баннеров.

Разнообразие анкоров

Хорошие ссылки имеют разнообразные анкоры, включая брендовые, ключевые слова и нейтральные фразы (например, ""сайт"", ""источник"").

Плохие ссылки имеют однообразные или переоптимизированные анкоры, что может вызвать подозрение со стороны поисковых систем.

Разнообразие типов ссылок

Хорошие ссылки исходят с разных источников: статей, блогов, форумов, социальных сетей, комментариев и т.д.

Плохие ссылки получены только из одного или двух типов источников, что также может вызвать подозрение.

Качество и уникальность контента на источнике

Хорошие ссылки размещены на сайтах с качественным, уникальным и полезным контентом.

Плохие ссылки находятся на сайтах с некачественным, дублирующимся или спамным контентом.

Заключение:

Приклеивание старых дроп доменов к молодому сайту может быть отличным решением для ускорения продвижения и улучшения позиций в поисковой выдаче. Однако, важно помнить, что приклеивание доменов – это лишь один из инструментов в комплексной стратегии продвижения. Работай над улучшением контента, оптимизацией сайта и профиля внешних ссылок, чтобы добиться наилучших результатов.

Если моя статья вам пришлась по душе, то заходите в мой Телеграм-канал! Там вы найдете много увлекательного и интересного контента: https://t.me/lazy_mar"'https://habrastorage.org/getpro/habr/upload_files/c2c/2b0/ee1/c2c2b0ee18da72d4196075d60bf633a9.jpeg'"['https://habrastorage.org/getpro/habr/avatars/3ec/c74/db3/3ecc74db31de88ea8b2385e749ca7ecf.jpg', 'https://habrastorage.org/getpro/habr/upload_files/c2c/2b0/ee1/c2c2b0ee18da72d4196075d60bf633a9.jpeg', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/c2c/2b0/ee1/c2c2b0ee18da72d4196075d60bf633a9.jpeg', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w48/getpro/habr/avatars/3ec/c74/db3/3ecc74db31de88ea8b2385e749ca7ecf.jpg']"
7'722918'Установка OpenCV под Windows'Введение В данной статье будет описан способ сборки OpenCV из исходников с помощью cmake и Microsoft Visual Studio, и пример запуска своего проекта на cmake. Большая часть статьи является повторением...'https://habr.com/ru/post/722918/'"В данной статье будет описан способ сборки OpenCV из исходников с помощью cmake и Microsoft Visual Studio, и пример запуска своего проекта на cmake. Большая часть статьи является повторением официальной документации

Добавить в path для всех пользователей папку build/install/.../bin

Для дальнейших действий Вам потребуется:

Около 2х часов времени на сборку

Около 10Гб свободного места (может больше, в зависимости от задачи)

Если чего-то из этого не хватает - можете посмотреть другие способы установки в официальной документации Выше.

Если Вы никогда не пытались ставить OpenCV, пропустите этот шаг. Убедитесь, что у Вас нет следов предыдущей версии OpenCV, в переменнных окружения нет OpenCV_DIR и подобных, а в переменная path не содержит ничего связанного с opencv

Клонировать репозиторий:

Для этого нужно создать папку (недалеко от корня диска), без пробелов и на английском языке. Открыть в ней консоль или git bash Прописать git clone.

В этой статье далее будет использоваться путь С:\OpenCV\opencv

Запустить cmake-gui Выбрать папку с клонированным репозиторием и папку, в который будет помещён сконфигурированный проект Лирическое отступление: cmake - система управления сборкой проекта. OpenCV - болшой проект, содержащий множество подпроектов, компонент. С помощью cmake мы определяем, какие именно подпроекты нам нужны, каким компилятором их собирать, под какую платформу.

Нажать Configure и выбрать компилятор

Выбрать необходимые компоненты Здесь лучше руководствоваться названием компонент, Вашими целями и здравым смыслом. По умолчанию будет предложена наиболее полная рабочая сборка, однако исходя из целей, многие вещи можно исключить. Например, есть блоки компонент, которые отвечают за интеграцию в java и python. Есть блоки компонент, содержащие нейросетевую обработку, они оканчиваются на ""nn"". Есть компоненты для открытия разных форматов изображений, соответственно, часть из них можно отключть. Однако, компоненты core, highgui, world, imgproc вероятнее всего понадобятся. Компоненты без группировки Компоненты после группировки .В оригинальной статье есть рекомендации по выбору компонент. Чтобы получить такое отображение, необходимо в cmake-gui поставить галочку grouped. Я отмечу лишь два из них: BUILD_SHARED_LIBS -> включаем сборку динамических библиотек. Особенно нужно, если планируется несколько проектов с OpenCV.

BUILD_opencv_world -> объединение всех выбранных компонент в одну библиотеку opencv_world.

Переконфигурировать проект: снова нажимаем Configure

Если ошибок нет, то нажимаем Generate. Если есть, гуглим и играемся с компонентами, пока не исчезнут. После каждого изменения в компонентах - переконфигурируем.

Открываем решение в Visual Studio (рядом с Generate - OpenProject, либо соответствующий sln в папке build)

Выставляем конфигурацию Debug

Собираем всё решение (Ctrl+Shift+B). Запаситесь терпением - у меня на этот шаг ушло около 40 минут.

Выставляем конфигурацию Release и снова собираем всё (ещё 40 минут)"'https://habr.com/share/publication/722918/a4bdb6f868f9b610d2204da2609745b8/'"['https://habrastorage.org/r/w1560/getpro/habr/upload_files/71e/03e/d73/71e03ed73cc39da6818b7d777908588b.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/114/f76/357/114f763573746892e509967fd11895a3.png', 'https://mc.yandex.ru/watch/24049213', 'https://habr.com/share/publication/722918/a4bdb6f868f9b610d2204da2609745b8/', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/0ca/49d/b81/0ca49db81f10f1b72a843b669a10697a.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/566/438/1af/5664381af341ab87c7a4223b4a23495e.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/a7a/835/206/a7a835206d9c4eb4b60874459d22a6a7.png']"
8'722812'Разбираем ошибки начинающих iOS-разработчиков'Привет! Я Александра Башкирова, iOS-инженер в Clover и старший код-ревьюер на курсе «iOS-разработчик» в Яндекс Практикуме. На момент подготовки статьи мы уже проверили более тысячи студенческих работ...'https://habr.com/ru/post/722812/'"Привет! Я Александра Башкирова, iOS-инженер в Clover и старший код-ревьюер на курсе «iOS-разработчик» в Яндекс Практикуме. На момент подготовки статьи мы уже проверили более тысячи студенческих работ и успели заметить повторяющиеся ошибки.

В этой статье я разберу несколько популярных ошибок начинающих iOS-разработчиков, чтобы вы могли не повторять их в своей работе. Мы вместе посмотрим, к чему они приводят и как их можно избежать. Начнём с вёрстки, а потом посмотрим на код и выработаем стратегии для хороших практик.

Часть 1. Вёрстка

Мы учим студентов верстать в Storyboard — это самый быстрый способ получить готовое и работающее приложение. Вам не нужно особых знаний для того, чтобы создать несколько стандартных элементов в Interface Builder и расположить их как нужно. Интерфейс достаточно простой и чем-то даже напоминает создание работ в Paint.

Связь между Storyboard и кодом

Многим новичкам в iOS-разработке непросто понять взаимосвязь элементов Storyboard с кодом. Важно знать, что из себя представляет эта связь и за чем стоит следить в первую очередь.

Связывание графических элементов с кодом происходит с помощью аутлетов (outlets) и экшенов (actions). @IBOutlet — это ссылка на графический элемент (например, кнопка или текстовое поле) в классе вью-контроллера или вью, которая позволяет получить доступ к его свойствам и методам. @IBAction — это метод в классе вью-контроллера или вью, который вызывается при определённом действии пользователя (например, при нажатии на кнопку).

Допустим, если вы добавили кнопку на экран в Storyboard, то вы должны связать её с кодом, чтобы обработать нажатие. Если вы хотите поменять текст в лейбле, понадобится аутлет, чтобы поменять свойство text .

Ошибки и решения

Научившись связывать элементы с кодом, можно уже уверенно написать простенький счётчик шагов или приложение с приветствием пользователей. Но что произойдёт, если мы захотим немного поправить код — например, переименовать переменную или удалить лишнюю функцию? Если сделаем такие изменения только в коде и попробуем запустить приложение, увидим ошибку:

this class is not key value coding-compliant for the key helloLabel .

Приложение упало в runtime и сообщило, что не нашло подходящего значения для ключа helloLabel .

В примере я переименовала helloLabel в label

helloLabel — это название элемента в момент, когда мы создавали связь со сторибордом. Дело в том, что в сториборде эта связь осталась неизменной, хотя во вью-контроллере тот же элемент уже называется label . Storyboard вполне текстовый документ, его можно открыть в виде XML-документа и найти лейбл, описанный в таком блоке:

<connections> <outlet property=""helloLabel"" destination=""jeX-w7-wVM"" id=""hGw-e6-vqM""/> </connections>

Получается, что Storyboard не узнал о том, что мы поменяли название ссылки на элемент. Чтобы решить эту проблему, нужно привести название к одинаковому состоянию в коде и в Storyboard. Для этого можно удалить старую связь с аутлетом в Connection Inspector и создать новый аутлет либо поменять название в коде на то, которое понимает ваш Storyboard.

Когда вы меняете название аутлета или экшена в коде, нужно гарантировать, что это же название поменялось и в вашем Storyboard. К счастью, таких проблем можно избежать, если для изменения названий пользоваться встроенным в Xcode рефакторингом Refactor → Rename — тут Xcode подскажет, что нужно обновить имя ссылки в сториборде тоже.

Проверяйте, что рядом с изменяемыми файлами стоит чекбокс: тем самым подтверждаете, что в этих местах тоже нужно исправить повторяющееся название

Ошибки нас ждут также, если удалим в Connection Inspector связь у Storyboard с кодом, но продолжим обращаться к оставшимся аутлетам из кода. @IBOutlet создаётся неявным опционалом, потому что сама инициализация элемента не может быть сделана в момент создания контроллера. Инициализация будет выполнена системой позже, а если такой связи на сториборде нет, то не будет выполнена никогда.

Попробуем обновить значение в helloLabel , но удалим связь в Storyboard: как только приложение дойдёт до строчки с установкой значения — произойдёт краш в runtime. Мы попробовали установить свойство text несуществующему объекту, и такое обращение было небезопасным, ведь свойство было неявным опционалом.

Обращайте внимание на логи в дебаг-панели.

Если приложение упало, там будет подсказка, в чём причина

В этом примере Xcode подсказывает пустым кружочком рядом с аутлетом, что связи с элементом нет. Если мы хотим поменять значение лейбла на Хабр, привет! , то нужно будет восстановить ссылку на него, добавив связь заново. Например, соединив кружочки напротив new referencing outlet со свойством helloLabel .

Итак, если вы столкнулись с runtime-ошибкой, которая упоминает ваш аутлет или экшен, обязательно проверьте:

Совпадение названий элемента или функции в коде и в сториборде. Наличие связи с кодом у элемента на вкладке Connection Inspector.

Использование Auto Layout

Пожалуй, самая типичная ошибка новичка — создать красивый интерфейс для новенького iPhone Pro Max, но не адаптировать его под размеры других девайсов.

Адаптивный интерфейс верстаем с помощью Auto Layout . В Storyboard есть целый набор различных способов, как привязать элементы к краям других элементов или ограничить размеры View.

На старте может быть сложно подружиться с Auto Layout. Ведь важно не только научиться пользоваться базовыми инструментами, но и понять, почему Auto Layout работает именно так и как заставить его делать то, что вам нужно.

Математическое представление Constraint

Важное свойство системы неравенств: она может иметь несколько решений. Именно поэтому возникают ошибки и предупреждения в Interface Builder, а также предложение «Добавить недостающие констрейнты». Из-за этого можно получить совершенно неожиданный результат: предполагаешь одно конкретное решение, а в ходе расчёта Auto Layout может подобрать совершенно другое решение неравенств :)

Изучив основы Auto Layout, можно переходить к работе с Figma .

Часто замечаю, что новички начинают верстать экран с фиксации размеров элементов. Это не лучшее решение, потому что элементы могут по-разному помещаться на разных экранах в зависимости от содержимого. Например, текст или изображение могут самостоятельно определять свои размеры по содержимому и выставленным настройкам. В этом случае правильнее ориентироваться на отступы элементов относительно других элементов и краёв экрана.

Например, на скриншотах выше можно задать серой View размер 335×502, что будет попадать в макет для iPhone X, но, вероятно, это будет многовато для iPhone SE 1-го поколения. Поэтому нужно определить в макете отступы у серой View сверху, слева, справа и снизу относительно блока с текстом. Или можно задать отступ сверху от края экрана 590, а не от соседней серой View, что точно вытолкнет её на самый низ экрана для iPhone 8 (а у нас там ещё кнопки должны расположиться). Поэтому правильно в этом случае задавать отступ для текста сверху от серой View, а снизу — от кнопок.

Верстать адаптивный интерфейс действительно непросто, но программисту не нужно самостоятельно принимать решение, как должны работать констрейнты при масштабировании экрана, — в этом ему помогает дизайнер. Удачная практика — создавать дизайн-макеты, которые будут хорошо выглядеть и под большой, и под маленький экран. В Figma есть две версии вёрстки — по ним можно определить, какие элементы могут сжиматься или что экран можно скроллить.

Пара слов о горизонтальных отступах

Новичкам сложно запомнить названия leading и trailing для горизонтальных привязок слева и справа. Возникает вопрос, почему Apple не назвала их left и right? На самом деле у Auto Layout есть привязки (они же anchors), которые так и называются left и right. Но Apple не просто так ввела понятия leading и trailing для описания границ элементов. Положения leading и trailing могут быть различными в зависимости от текущей Locale , и для арабской локали или иврита приложение с помощью таких констрейнтов будет корректно отображено с зеркальным размещением элементов. Если же вы будете использовать привязки left и right, то Auto Layout не будет переворачивать интерфейс для RTL-локалей. Мы советуем прислушиваться к рекомендации Apple и использовать leading и trailing, а запомнить их можно так: leading — ведущий, а trailing — замыкающий. Ведущим мы начинаем верстать, а замыкающим — заканчиваем.

И пара слов о вертикальных отступах

Уже несколько лет Apple выпускает iPhone с чёлкой и закруглёнными краями экрана, на которых расположены элементы системы Status Bar и Home Indicator. Пользователю должно быть удобно с ними взаимодействовать. Это означает, что мы, как разработчики приложений, не должны размещать свой UI в этих областях, чтобы не мешать пользователю. Для этого на вью экрана существует специальный гайд (layout guide), который описывает безопасную область — Safe Area .

Обычно у новичков с этим проблем нет, ведь в сториборде практически все констрейнты по умолчанию будут привязываться не к границам вью экрана, а сразу к Safe Area. Трудность заключается в интерпретации отступа в Figma. В примере выше есть отступ в 34 пункта у кнопок до края экрана, и можно невнимательно поставить один из констрейнтов:

34 пункта от нижнего края View,

34 пункта от нижнего края Safe Area.

И оба варианта неверны.

Первый вариант даст небольшую свободную область: там расположится Home Indicator. Но это заблуждение, во-первых, потому что существуют девайсы с прямоугольным экраном и физической кнопкой Home, для которых это пространство будет ненужным. Во-вторых, нет гарантии, что Apple не поменяет интерфейс iOS и размер под безопасное пространство рядом с Home Indicator, поэтому жёстко фиксироваться на значении 34 нельзя.

Второй вариант происходит из-за невнимательности: по умолчанию сториборды делают любую привязку к краям главной View экрана сразу к Safe Area, и поставить значение 34 — значит увеличить область снизу в два раза (верно для девайсов, у которых эта область уже 34).

Поэтому разработчикам нужно быть внимательными к вертикальным отступам у экрана в области Safe Area и в Figma обязательно отделять визуально отступы от неё, а не от края макета. В примере выше кнопки прикреплены к краю Safe Area, и их стоит прикрепить к ней с нулевым отступом.

Вёрстка в iOS может показаться лёгкой, но важно учитывать всю линейку девайсов и их размеры, а также особенности устройства инструментов для вёрстки. Сталкиваться с такими сложностями в начале пути — нормально. Помните, что верно свёрстанный экран — ответственность не только ваша, но и дизайнеров, не стесняйтесь спрашивать и привлекать к совместной работе коллег.

Часть 2. Кодинг

Использование фишек языка не по назначению

Наши студенты сначала учат основы Swift и применяют знания по мере создания различных приложений. На начальном этапе будущий джун часто собирает свой код из громоздких конструкций — рассмотрим некоторые из них.

Избыточный force unwrapping

var lastPage: Int? = nil lastPage! += 1 // Произойдёт ошибка: операция на nil не может быть выполнена

Swift — типобезопасный язык. В нём достаточно красивых способов написать аккуратный код без использования force unwrap. В сообществе iOS-разработчиков принято писать безопасный код и избегать force-операций, ведь они могут привести к сбою приложения, и оно будет вылетать. Пока вы изучаете разработку под iOS, будьте бдительны в следующих ситуациях:

Когда Xcode предлагает «исправить» код на операции с восклицательным знаком: это и есть force-операции, их стоит избегать и переписывать на безопасные конструкции.

Присмотритесь к различным туториалам — там часто встречается код с force unwrap, но в учебных материалах force unwrap удобен именно для упрощения объяснения концепции и быстрого старта.

Вы можете отмахнуться от этого совета: «Ничего страшного не произойдёт в моём пет-проекте!» Да, не произойдёт. Относитесь к этому не только как совету писать безопасный код, но ещё и как к формированию best practice. В тестовых заданиях и на coding-интервью работодатели особенно пристально следят за форсами и могут уже на этом этапе забраковать вашу кандидатуру. Лучше сразу привыкать писать код так, как принято в сообществе.

Итак, если вы разобрались, что обязательно нужно проверить, безопасны ли операции на опционалах, вы можете написать что-то подобное:

var lastPage: Int? // Заведём некоторую опциональную переменную // Проверим её на nil и в тернарном операторе // возьмём ноль, если значение nil, // или прибавим единицу, если значение существует let nextPage = lastPage == nil ? 0 : lastPage! + 1

Это действительно безопасный вариант кода, но это тот случай, когда без него можно обойтись. Мы рекомендуем тренировать привычку использовать возможности языка:

Optional Binding — самый используемый способ, когда мы распаковываем элемент, тем самым сразу проверяя на опциональность:

// Объявим опциональную переменную. Если не присвоить в неё значение, то по умолчанию будет присвоен nil var lastPage: Int? // 1. Можно распаковать в новую константу и безопасно использовать константу внутри блока if if let unwrappedLastPage = lastPage { // используйте unwrappedLastPage здесь } // 2. Часто используют то же имя, создавая неопциональную копию значения, если значение имеется :) if let lastPage = lastPage { // несколько удобнее, так как не добавляет лишний мусор в нейминг (unwrapped) } // 3. Можно неявно определить с таким же именем, доступно для Swift 5.7 if let lastPage { // Этот вариант технически аналогичен предыдущему, но заметно короче! }

Изначальный пример мы можем написать так:

var lastPage: Int? let nextPage: Int if let lastPage { nextPage = lastPage + 1 // Избавились от force unwrap } else { nextPage = 0 // использовали начальное значение }

Nil Coalescing Operator — позволяет взять дефолтное значение в случае опциональности:

let value = optionalValue ?? defaultValue

Снова попробуем переписать наш изначальный пример:

var lastPage: Int? // Возьмём значение, если оно есть, и прибавим единицу, чтобы получить следующую страницу // Возьмём -1 в случае nil, прибавим единицу — получим индекс первой страницы, 0 let nextPage = (lastPage ?? -1) + 1

Guard Statement — позволяет прервать выполнение блока, если значение опционально.

guard let lastPage = lastPage else { return } // используйте lastPage здесь guard let currentPage else { // Короткий вариант в Swift 5.7, аналогичный if let return } // используйте currentPage здесь

Если вы работаете с опциональным значением, сначала попробуйте подобрать подходящую стратегию. Нужно ли в этом случае выполнять какой-то код, или вы можете заменить его предзаданным значением? Стоит ли показать ошибку? Смело подбирайте подходящую конструкцию, в большинстве случаев ваш код можно написать, используя их.

Force unwrap можно использовать в редких ситуациях, если вы на 100% уверены, что переменная никогда не будет зависеть от внешнего окружения и не превратится в optional.

Например, константа с указанием адреса:

let appleURL = URL(string: ""<https://www.apple.com>"")!

Здесь мы, во-первых, уверены, что описали верную ссылку на сайт, во-вторых, это константа, и её значение статично независимо от состояния приложения.

Однако если мы немного изменим этот код, используя внешний аргумент, то гарантировать то же самое нельзя. В таком случае стоит переписать код на безопасный вариант разворачивания опционала:

var website = ""https://www.apple.com"" var url: URL { URL(string: website)! } print(url) // ""https://www.apple.com"" – Ух! Пока работает (потому что ссылка верная) website = ""one more thing..."" // Перезапишем значение переменной другой многозначительной строкой print(url) // Ошибка! Форс анврапнули внутри блока var url, всё взорвалось!

guard

Этот оператор — мой любимый в Swift. Конструкция нужна для проверки условий и распаковки опциональных значений с использованием выхода из функции или блока, если условия не выполнились. Этот оператор удобно использовать, чтобы распрямить код, то есть сделать его менее вложенным.

Однако новички любят условный оператор if и используют его очень активно:

func processName(name: String?) { if let name = name { // делаем что-то с именем print(""Привет, \(name)!"") } }

Получился вполне безопасный вариант, ведь аргумент name распакован в константу с помощью if let , однако вложенность такого кода равна двум: один за функцию, два за блок if . Если нам понадобится проверить ещё и длину имени, чтобы вывести фразу, то вложенность сразу возрастёт до трёх:

func processName(name: String?) { if let name = name { // делаем что-то с именем print(""Привет, \(name)!"") if name.count < 3 { print(""\(name), какое короткое у тебя имя!"") } } }

Разработчики не любят глубокую вложенность, потому что такой код сложнее тестировать, читать и в нём легче ошибиться. Поэтому всячески стараются распрямлять код — например, с помощью guard :

func processName(name: String?) { guard let name = name else { return } // делаем что-то с именем print(""Привет, \\(name)!"") guard name.count < 3 else { return } print(""\\(name), какое короткое у тебя имя!"") }

С помощью guard мы уменьшили вложенность кода и не потеряли в функциональности, а читать такой код стало проще, каждой проверкой мы отсекли лишние случаи. До конца функции мы дойдём только в случае, когда имя не опциональное и короткое: то есть мы описали идеальный путь работы функции, как будто никаких опционалов и неверных условий не произойдёт, а если произойдут, то мы просто не выполним никаких лишних операций.

Итак: используйте guard для распрямления кода и построения позитивного сценария работы функции.

Иногда guard из лаконичного оператора превращается в раздутый if из первого примера:

func handle(message: Message?, error: String?) { guard error == nil else { // сообщаем об ошибке и выходим! print(""Упс! \\(error!)!"") removeHistory() fastLogout() return } guard let message else { return } // делаем что-то ещё }

Мы уже посмотрели, как можно избегать force unwrap, и в данном случае с error! стоит поступить так же: аккуратно распаковать. Обратите внимание на раздутый блок guard else . Блок else в этом случае не предполагает выполнения сложных действий. Если вы обнаружили в блоке guard else большой код, то, возможно, ваша функция делает слишком много. Поэтому код, предложенный выше, можно разбить на несколько дополнительных функций:

func handle(message: Message?, error: String?) { if let error = error { // Распаковали error, чтобы избавиться от force unwrap processError(error) } else if let message = message { processMessage(message) } } func processError(_ error: String) { print(""Упс! \(error)!"") removeHistory() fastLogout() } func processMessage(_ message: Message) { // Обрабатываем сообщение }

Этот пример кода имеет и архитектурную проблему: функция принимает два параметра, хотя предполагается, что только один из них будет неопциональным.

Возможны ситуации с другими вариантами: message и error — nil, тогда ничего не выполнится. Ещё один вариант: когда есть и ошибка, и сообщение — непонятно, как верно обработать эту ситуацию. Если есть ошибка, сообщение будет проигнорировано. В данном случае в Swift нам доступен специальный generic-тип Result<Value, Error> , который и поможет исключить эти случаи:

guard — очень удобный оператор ветвления. Мои рекомендации по нему:

Используйте guard для проверки значений опциональных переменных на nil. Помещайте guard как можно ближе к началу функции или блока. Используйте guard , чтобы отсечь ситуации, когда условия не выполняются. Так вы предотвратите глубокую вложенность кода. Используйте блок else для обработки случаев негативных сценариев: это может быть выход из тела функции, ошибка или другое поведение. Следите за размером блока else : если в нём получилось много кода, то, возможно, пора рефакторить. Используйте guard только для проверки условий, которые могут вызвать проблемы или ошибки в дальнейшем коде. Помните о подходе с позитивным сценарием функции.

switch

Этот оператор можно использовать для проверки значений различных типов данных, таких как числа, строки, перечисления (enum), булевы значения и другие. switch включает в себя блоки case , каждый из которых содержит определённое значение, которое нужно проверить и каким-то образом обработать. Давайте посмотрим на следующий пример, который я часто встречаю у студентов.

switch isAvailable { case true: print(""Достпуно!"") case false: print(""Ограничено"") }

Новичок использует switch для описания поведения для булева значения. В данном случае конструкция простая и легко читается. Однако лучше использовать вариант с if else , потому что он чаще употребляется и гораздо легче кастомизируется, если понадобится добавить дополнительные проверки:

if isAvailable { print(""Достпуно!"") } else { print(""Ограничено"") }

Рассмотрим ещё пару особенностей использования switch . Допустим, есть switch , обрабатывающий стороны света:

switch direction { case .west: goToWest() case .east: goToEast() case .north: break default: break }

В блоке case .north: break приложение не делает ничего, так как используется break , что может быть неочевидным для других разработчиков, которые будут читать этот код. Вы можете добавить причину в виде комментария. Также можно обратить внимание, что в данном случае мощность switch использована не полностью: только два направления обработаны, — и вы могли бы упростить код, использовав default и для north тоже:

switch direction { case .west: goToWest() case .east: goToEast() default: // Ничего не делаем break }

Использование варианта default хорошо для случаев, когда мы описываем фиксированное количество кейсов и можем гарантировать, что не собираемся добавлять новые элементы в enum’ы. Однако enum’ы в разработке используются очень часто, и возникает потребность их менять и дополнять новыми кейсами. Что произойдёт с этим кодом, если вы добавите ещё четыре промежуточные стороны света? default по умолчанию сделает всё за вас: пропустит их обработку. Это может быть неудобно, если подобных свитчей в приложении много, и было бы здорово сразу поправить код везде, где это требуется. Если мы откажемся от default , то при добавлении сценария компилятор выдаст ошибку и попросит описать все сценарии. Знание этой особенности позволяет сэкономить время на отладке. И исходя из этой мысли стоит переписать свитч с полным перебором вариантов:

switch direction { case .west: goToWest() case .east: goToEast() case .north, .south: // Если направление на север или на юг, то ничего не делаем break }

На что стоит обратить внимание при работе со switch :

Использование оператора не должно быть избыточно, и если его легко можно переписать с помощью if , то лучше отказаться от switch .

Перечислять все возможные случаи, чтобы убедиться, что ничего не упущено, и иметь возможность заметить ошибку при дополнении новыми случаями.

Использовать блок default , чтобы обработать неожиданные значения или ошибки, но не для пропуска очевидных значений.

Ошибки при реализации паттерна delegate

Начинающим разработчикам непросто даётся понимание делегирования, хотя это самый часто используемый паттерн проектирования в iOS-разработке.

Представьте: в магазин ворвался грабитель, продавец нажимает тревожную кнопку под столом и вызывает охрану, то есть делегирует ответственность борьбы с нарушителем. В этом случае грабитель — это произошедшее событие, которое нужно обработать. В роли делегата выступает охранник, а продавец магазина играет роль объекта, который делегирует выполнение функции охраны магазина.

Аналогично в программировании: делегирование используется для передачи ответственности от одного объекта к другому. Например, если объект A не может выполнить определённую функцию, он может делегировать эту функцию объекту B, который имеет необходимые навыки и знания для выполнения этой функции.

В Swift паттерн реализуют с помощью протокола с описанием делегируемых действий и ссылочной связи между объектами. Объект, который будет делегировать свои действия, имеет ссылку на объект-делегат, который должен реализовать соответствующий протокол. Когда происходит событие, объект, который хочет делегировать свои действия, вызывает соответствующий метод делегата. Делегат получает этот вызов и выполняет необходимые действия.

Наши студенты знакомятся с делегатами, когда им ставится задача реализовать загрузку элемента во вью-контроллере с помощью внешнего сервиса. Нужны следующие шаги:

Добавить новый сервис, который будет получать новые данные. Вью-контроллер сохраняет ссылку на сервис, чтобы просить его загрузить новые данные. Сервис возвращает данные через делегирование действий. Для этого ему необходимо связаться с вью-контроллером по протоколу, то есть иметь ссылку на делегат.

ViewController необязательно должен быть делегатом для выполнения делегированных работ. Можно использовать и другие объекты, однако в этой задаче обработать элемент должен был ViewController .

final сlass MyViewController: UIViewController { var service: LoadItemService? func viewDidLoad() { super.viewDidLoad() let service = LoadItemServiceImplementation() service.delegate = self // 1. Создаём связь с делегатом, в качестве делегата — контроллер self.service = service // 2. Сохраняем ссылку на сервис } func viewDidAppear() { super.viewDidAppear() service.loadElement() } } protocol LoadElementDelegate { /// Действие по обработке полученного элемента func handleLoaded(_ item: VeryImportantItem) } extension MyViewController: LoadElementDelegate { func handleLoaded(_ item: VeryImportantItem) { // Делаем что-то очень важное с элементом } } final class LoadItemServiceImplementation: LoadItemService { var delegate: LoadElementDelegate? func loadElement() { let item = VeryImportantItem() delegate.handleLoaded(item) // Делегируем обработку полученного элемента } }

Тут новичков поджидает такая ошибка: создание сильной связи между двумя ссылочными объектами.

Retain cycle, или сильный ссылочный цикл — причина утечки памяти в iOS, когда два объекта держат друг друга и не могут освободиться из памяти. Он может возникнуть, когда у двух объектов есть сильные ссылки друг на друга.

В данном случае retain cycle возникает из-за установки сильной ссылки между вью-контроллером и объектом сервиса, а также сохранения реализации сервиса в переменной service в классе вью-контроллера.

Когда мы устанавливаем delegate в self , создаём сильную ссылку между вью-контроллером и сервисом. А когда сохраняем реализацию сервиса в переменную service в классе вью-контроллера, создаём вторую сильную ссылку, и в результате образуется ссылочный цикл.

Для освобождения памяти в iOS используется система подсчёта сильных ссылок на каждый объект, называемая ARC (Automatic Reference Counting). Она определяет, нужно ли ещё держать объект в памяти. Когда счётчик ссылок достигнет нуля, объект автоматически освободится из памяти.

Чтобы исправить ошибку со ссылочным циклом в данном случае, необходимо ослабить одну из сильных ссылок, используя ключевое слово weak . В частности, мы можем сделать делегат слабой ссылкой, используя weak var delegate: LoadElementDelegate? . Это ослабит ссылку между вью-контроллером и сервисом и предотвратит возникновение сильного ссылочного цикла.

weak var delegate: LoadElementDelegate?

Важный момент: ссылка на делегат всегда слабая, а обратная ссылка — сильная.

Здесь необходимо понять принцип паттерна, владеть знаниями о reference type и об устройстве памяти в iOS. Это несложно, особенно когда приложение одностраничное, там эта ошибка вряд ли приведёт к проблемам. Самое интересное происходит, когда начинающий разработчик собирает приложение из нескольких экранов, а ещё лучше, если эти экраны будут создаваться несколько раз.

Рассмотрим другую ошибку с проектированием связей между элементами. Предположим, новая задача состоит из трёх экранов:

Вспомогательный экран SplashViewController — который при необходимости предложит пользователю пройти авторизацию, иначе покажет главный экран.

За авторизацию будет отвечать AuthViewController , о результатах он будет сообщать с помощью делегата на вызвавший его SplashViewController .

В качестве главного экрана — профиль пользователя ProfileViewController . С которого пользователь может разлогиниться.

И пусть первый вариант кода у студента будет приблизительно таким:

final class SplashViewController: UIViewController { func showAuthVC() { let authVC = AuthViewController() authVC.delegate = self present(authVC, animated: true) } func showProfile() { let profileVC = ProfileViewController() present(profileVC, animated: true) } func loginSuccessful() { showProfile() } } final class AuthViewController: UIViewController { var delegate: AuthViewControllerDeleate? func handleSuccessResult() { delegate?.loginSuccessful() } } final class ProfileViewController: UIViewController { func logout() { let authVC = AuthViewController() // для упрощения примера пусть эта функция устанавливает // новый вью-контроллер в иерархии в качестве корневого replaceCurrentRootVC(with: authVC) } }

Здесь мы легко находим ту же самую ошибку, что и в прошлый раз, — ослабляем ссылку на вью-контроллер:

weak var delegate: AuthViewControllerDeleate?

Может показаться, что никто не держит AuthViewController , но это не так. Контроллер, размещённый в иерархии экранов, уже сильно удерживается. Поэтому нам нужно ослаблять ссылку тоже.

Интереснее следующее: в сценарии logout повторно создаётся AuthViewController и размещается в представлении. Здесь новичок забыл установить ему delegate . Кто тогда будет обрабатывать результат авторизации? В этом случае пользователь попадёт в тупик, потому что никто не обработает действия от AuthViewController и некому определять, куда навигировать дальше. Здесь мы сталкиваемся с неправильным проектированием связей между элементами.

Здесь студент мог бы реализовать ещё один делегат AuthViewController и повторить некоторые действия после авторизации. Однако правильнее эту ошибку исправить так: заменить переход не на AuthViewController , а на вспомогательный контроллер-координатор SplashViewController , чтобы иметь возможность повторить сценарий входа в приложение и переиспользовать уже написанный код. Ответственностью SplashViewController будет установить делегата и обработать делегируемые методы.

Эту ошибку я связываю с неловкостью в обращении с этим паттерном. Если сразу вспомнить, что AuthViewController не будет работать без компаньона-делегата, то такой ошибки бы не возникло.

На самом деле такое случается и в более тривиальном случае: разработчики могут забыть установить делегат таблице и долго дебажить, почему их код не работает как нужно.

Будьте внимательны:

избегайте создания сильной связи между объектами,

не забывайте устанавливать делегат и проверяйте осмысленность созданных связей.

Мы разобрали ошибки, которые я встречаю у новичков чаще всего. Если у вас появились вопросы по поводу ошибок, которые мы разобрали, или если есть другие решения, я буду рада обсудить их в комментариях и помочь вам :)

Что ещё поможет разобраться в iOS-разработке:"'https://habrastorage.org/getpro/habr/upload_files/001/540/151/0015401519a3724cf95f8d0bd3ec043e.png'"['https://habrastorage.org/getpro/habr/upload_files/001/540/151/0015401519a3724cf95f8d0bd3ec043e.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/893/c36/3cb/893c363cbfc75a9a3c6d279ea369d879.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w48/getpro/habr/avatars/4e5/c80/e01/4e5c80e0101cf503455af5e69c5a3e95.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/cc8/9ac/c1f/cc89acc1f74d825fb33c0c98e1e30287.png', 'https://habrastorage.org/getpro/habr/branding/971/cf2/948/971cf29489de0783fe21e644735a1870.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/958/3a9/3c1/9583a93c1129d9510e7b6be5e29a2f4b.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/53a/0be/de8/53a0bede8010c128a9e1014e98f1c50b.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/47c/a94/8ec/47ca948ec82179d184dd3eecaf7f9a86.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/588/745/8d6/5887458d6660c456099e3a9494d0a79e.png', 'https://habrastorage.org/getpro/habr/avatars/4e5/c80/e01/4e5c80e0101cf503455af5e69c5a3e95.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/889/da8/0ad/889da80addf406677a9ffa0e8ccef034.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/dbe/367/7b9/dbe3677b92578c80cf4c3d20a9d1ff45.png', 'https://habrastorage.org/getpro/habr/company/361/259/86f/36125986f4f45846a087d0a71d88d32b.jpg']"
9'722668'Краткий обзор нововведений C++23: deducing this'Документ «deducing this» , принятый в последний стандарт C++, вводит новый, третий тип методов классов, сочетающий в себе свойства двух уже существующих: нестатических и статических , открывающий...'https://habr.com/ru/post/722668/'"Документ «deducing this», принятый в последний стандарт C++, вводит новый, третий тип методов классов, сочетающий в себе свойства двух уже существующих: нестатических и статических, открывающий перед нами новые горизонты:

Дедупликация большого количества кода. Вытеснение CRTP (Curiously Recuring Template Pattern) на свалку истории, его замена более простой и очевидно понятной записью. Рекурсивные лямбды.

И другое.

Но прежде чем рассмотреть само нововведение и его практические применения, углубимся немного в историю и попытаемся понять, почему в нем собственно возникла необходимость.

Мотивация

Начиная с C++03, методы могут иметь cv-квалификаторы, так что стали возможны сценарии, когда есть необходимость как в const , так и не- const перегрузке определенного метода (для краткости воздержимся от рассмотрения volatile перегрузок).

Во многих случаях между их логикой нет никакой разницы — отличаются лишь квалификаторы используемых типов, так что приходится или копировать определение, подгоняя квалификаторы, или использовать такие механизмы как const_cast :

class TextBlock { public: char const& operator[](size_t position) const { // ... return text[position]; } char& operator[](size_t position) { return const_cast<char&>( static_cast<TextBlock const&>(*this)[position] ); } // ... };

Начиная с C++11, методы могут иметь также и ref-квалификаторы, так что теперь вместо двух перегрузок одного метода нам могут понадобиться четыре: & , const& , && , const&& , и у нас есть три способа решить данную задачу (ссылка на код, приведенный ниже):

Писать реализацию одного метода четырежды. Делегировать три перегрузки четвертой, используя static_cast и const_cast . Использовать вспомогательную шаблонную функцию.

Но ни один из этих способов не избавляет нас от необходимости четырежды определять практически один и тот же метод.

Если бы могли написать что-то вроде функции ниже, но ведущей себя как член класса, это решило бы все наши проблемы:

template <typename T> class optional { // ... template <typename Opt> friend decltype(auto) value(Opt&& o) { if (o.has_value()) { return forward<Opt>(o).m_value; } throw bad_optional_access(); } // ... };

Но мы не можем. Точней, не могли. До C++23.

Мечты воплощаются в реальность

Теперь же мы можем определять методы, явно принимающие в качества аргумента объект, над которым они были вызваны:

struct X { template <typename Self> void foo(this Self&&) { } }; void example(X& x) { x.foo(); // Self = X& move(x).foo(); // Self = X X{}.foo(); // Self = X }

Таким образом, теперь мы можем переписать всю ту простыню кода, реализующую метод value у optional , гораздо более компактно, меньше чем в десяток строк:

template <typename T> struct optional { template <typename Self> constexpr auto&& value(this Self&& self) { if (!self.has_value()) { throw bad_optional_access(); } return forward<Self>(self).m_value; }

Мы подчинили своим целям механизм вывода типов во всей своей мощи! И важно отметить, что это все тот же механизм, проявляющий себя в обычных шаблонных функциях и методах.

Deducing this не вносит никаких изменений в правила вывода типов, он лишь позволяет явное объявление объектного параметра (explicit object parameter) в списке аргументов методов. Параметра, который до этого в методах присутствовал лишь неявно (implicit object parameter), в виде указателя this .

Важно отметить, что вывод типов способен выводить производные типы:

struct X { template <typename Self> void foo(this Self&&, int); }; struct D : X { }; void example(X& x, D& d) { x.foo(1); // Self=X& move(x).foo(2); // Self=X d.foo(3); // Self=D& }

На методы, объявленные с явным объектным параметром, также накладывается ряд ограничений:

Они не могут быть объявлены статическими. Причина этого заключается в том, что хоть и внешне эти функции выглядят и ведут себя как обычные нестатические методы, но внутренне они ведут себя в точности как статические: в них недоступен указатель this , и единственный способ взаимодействовать в них с объектом класса — через явный объектный параметр. Кроме того, указатель на такие методы — это указатель на функцию, а не член класса. Они не могут быть объявлены виртуальными. Они не могут быть объявлены с использованием ref или cv квалификаторов.

Так как объявление явного объектного параметра уже несет в себе всю необходимую информацию о его типе:

Но есть нюансы

Однако с большой силой приходит и большая ответственность. Так, используя новый тип методов, мы должны постоянно помнить о том, насколько могуществен механизм вывода типов:

struct B { int i = 0; template <typename Self> auto&& f1(this Self&& self) { return forward<Self>(self).i; } }; struct D: B { double i = 3.14; };

Тогда как B().f1() в коде выше вернет ссылку на B::i , D().f5() вернет ссылку на D::i , так как self является ссылкой на D .

Если же мы хотим получать ссылку на B::i всегда, нам необходимо явно предусмотреть это в коде:

template <typename Self> auto&& f1(this Self&& self) { return forward<Self>(self).B::i; }

Другой опасностью на нашем пути может служить проблема приватного наследования. Рассмотрим следующий код:

class B { int i; public: template <typename Self> auto&& get(this Self&& self) { return forward<Self>(self).B::i; } }; class D: private B { double i; public: using B::get; }; D().get(); // error

Мы, казалось бы, предохранились как могли. Однако недостаточно. Мы не можем получить доступ к B::i из D , так как наследование является приватным.

Но и для этой проблемы существует решение:

class B { int i; public: template <typename Self> auto&& get(this Self&& self) { // like_t — функция, применяющая ref и cv квалификаторы // первого переданного типа ко второму // Например, like_t<int&, double> = double& return ((like_t<Self, B>&&)self).i; } }; class D : private B { double i; public: using B::get; }; D().get(); // now ok, and returns B::i

Выполнив приведение в стиле си для избежания проверки доступа, мы реализовали желаемое поведение.

Практическое применение

Итак, немножко поговорив о сущности нововведения и об опасностях, которые могут нас поджидать при его использовании, перейдем к рассмотрению некоторых из его практических приложений.

Дедупликация кода

Это первое и самое очевидное. Помимо уже рассмотренных примеров, приведу еще несколько:

Внедрение методов в классы-наследники

Да-да, вы подумали правильно. CRTP (Curiously Recurring Template Pattern) больше не нужен. И хоть код от использования deducing this в простейшем случае ниже не становится короче, но очевидно становится более простым и интуитивно понятным.

Рекурсивные лямбды

О чем я ранее еще не упоминал — это то, что теперь мы можем определять лямбды с явным объектным параметром, благодаря чему становится возможным определение рекурсивных лямбд:

auto fib = [](this auto self, int n) { if (n < 2) return n; return self(n-1) + self(n-2); };

struct Leaf { }; struct Node; using Tree = variant<Leaf, Node*>; struct Node { Tree left; Tree right; }; int num_leaves(Tree const& tree) { return visit(overload( // <-----------------------------------+ [](Leaf const&) { return 1; }, // | [](this auto const& self, Node* n) -> int { // | return visit(self, n->left) + visit(self, n->right); // <----+ } ), tree); }

Передача self по значению

Передача self по значению открывает для нас возможность более естественного выражения желаемой семантики, например когда смысл метода заключается в одном лишь возврате модифицированной копии:

struct my_vector : vector<int> { auto sorted(this my_vector self) -> my_vector { sort(self.begin(), self.end()); return self; } };

Но, что многие справедливо посчитают более важным применением, позволяет в некоторых случаях достичь лучшей производительности.

Например, все мы знаем, что маленькие типы во избежание порождения лишних уровней косвенности (indirection) лучше передавать по значению.

К одному из таких типов относится std::string_view . Который мы можем передавать по значению всюду: в наши функции, конструкторы, другие методы. Но только не в его собственные методы. До принятия deducing this у разработчиков стандартной библиотеки не было способов избежать разыменований указателя this в собственных методах std::string_view .

Теперь же мы можем переписать все его методы, не выполняющие модификаций, с передачей явного объектного параметра по значению, практически бесплатно этим получая улучшение производительности:

template <class charT, class traits = char_traits<charT>> class basic_string_view { private: const_pointer data_; size_type size_; public: constexpr const_iterator begin(this basic_string_view self) { return self.data_; } constexpr const_iterator end(this basic_string_view self) { return self.data_ + self.size_; } constexpr size_t size(this basic_string_view self) { return self.size_; } constexpr const_reference operator[](this basic_string_view self, size_type pos) { return self.data_[pos]; } };

Заключение

На самом деле, это далеко не все, что можно сказать про deducing this, но самое главное и основное. Целью данной статьи не являлось углубление в детали.

Если вы хотите постичь все нюансы — вы можете обратиться к оригинальному документу.

Также довольно интересы и увлекательны статьи, написанные некоторыми из его авторов: «C++23’s Deducing this: what it is, why it is, how to use it» (Sy Brand), «Copy-on-write with Deducing this» (Barry Revzin).

Любите плюсы и будьте счастливы."'https://habr.com/share/publication/722668/af144560b956573682159f997dd1b99e/'"['https://habrastorage.org/r/w1560/getpro/habr/upload_files/c45/48f/0c1/c4548f0c1b8117ca8273c4b71f9f4eea.png', 'https://habr.com/share/publication/722668/af144560b956573682159f997dd1b99e/', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/3d7/e13/9c9/3d7e139c9f5f99651c9ea5f692ff02c5.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/56c/c55/2f1/56cc552f1961aa28432f6f2f1380b0b6.png', 'https://habrastorage.org/getpro/habr/avatars/124/684/c28/124684c28892979eabfaf52b22268f4b.png', 'https://habrastorage.org/r/w48/getpro/habr/avatars/124/684/c28/124684c28892979eabfaf52b22268f4b.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/e11/43c/083/e1143c0838b15915f05eb084b79b6df9.png']"
10'722798'Neural Network Optimization: океан в капле'Всех приветствую, меня зовут Антон Рябых, работаю в Doubletapp . Вместе с коллегой Данилом Гальпериным мы написали статью про важный этап в процессе обучения нейронных сетей и получения необходимых...'https://habr.com/ru/post/722798/'"Всех приветствую, меня зовут Антон Рябых, работаю в Doubletapp. Вместе с коллегой Данилом Гальпериным мы написали статью про важный этап в процессе обучения нейронных сетей и получения необходимых нам результатов — оптимизацию модели. Зачем нужно оптимизировать модель, если и так все работает? Но как только вы начнете разворачивать модель на устройстве, которое будет ее обрабатывать, перед вами встанет множество проблем.

Более крупные модели занимают больше места для хранения, что затрудняет их распространение. Более крупные модели требуют больше времени для работы и могут потребовать более дорогого оборудования. Это особенно важно, если вы создаете модель для приложения, работающего в реальном времени.

Оптимизация моделей направлена на уменьшение размера моделей при минимизации потерь в точности и производительности.

Сценарии использования

Увеличение пропускной способности (снижение задержки), полезно как для облачных сервисов, так и для edge-девайсов в виде мобильных устройств, интернета вещей.

Развертывание моделей на edge-устройствах с ограничениями по обработке, памяти и / или энергопотреблению.

Уменьшение размера модели для ускоренного обновления модели и снижения затрат на хранение моделей.

Оптимизация модели полезна для оборудования с ограничениями или оптимизацией для операций с фиксированной точкой.

А также для аппаратных ускорителей специального назначения.

Методы оптимизации

В статье обзорно рассмотрим следующие методы оптимизации:

Pruning — устранение части параметров нейронной сети.

Quantization — уменьшение точности обрабатываемых типов данных.

Knowledge distillation — обновление топологии исходной модели до более эффективной, с уменьшенным количеством параметров и более быстрым выполнением.

Weight clustering — сокращение количества уникальных параметров в весах модели.

OpenVino, TensorRT — фреймворки, с помощью которых можно оптимизировать модели.

Pruning

Устранение части параметров нейронной сети — это метод сжатия, при котором происходит удаление весов из обученной модели. Удаление может производиться как на целых нейронах, так и на отдельных весах. Мы рассмотрим основные методы прунинга нейронной сети.

Одним из способом является прунинг весов, когда некоторым параметрам устанавливается значение в ноль, тем самым создается разреженная сеть. Это уменьшает количество параметров модели, при этом сохраняется целостность архитектуры.Мы получаем сеть с меньшим количеством параметров, но для ее эффективности требуются разреженные вычисления, для которых необходима поддержка оборудования.

Вторым способом является удаление из сети целых узлов (нейронов). Такой способ уменьшает архитектуру сети и позволяет выполнять плотные, более оптимизированные вычисления. Можно работать без разреженных вычислений, и такие вычисления лучше поддерживаются на оборудовании. Однако такой прунинг способен навредить нейронной сети — удалить важные нейроны.

Во время процесса оптимизации модели главное — поддерживать уровень точности на том же уровне или хотя бы ненамного хуже, чем было. Это можно сделать, удалив те элементы модели, которые в меньшей степени влияют на результат работы сети. Существует множество методов для решения поставленной задачи, но для большего понимания подойдут эвристические алгоритмы.

Интуитивно понятно, что можно обрезать те веса модели (свести их к нулю), которые и так имеют достаточно низкое значение по модулю. Для преднамеренного обучения модели заглушать незначимые веса используют регуляризацию L1 или L2.

Подобную логику имеет и удаление нейронов из сети. При запуске набора данных мы можем собрать некоторую статистику активаций. Те нейроны, которые не выдают высокие значения, — редко используются сетью и следовательно могут быть удалены. Помимо величины весов проверяется схожесть с другими выходами текущего слоя, если значения двух выходов статистически повторяются, то можно предположить, что они делают одно и то же. Следовательно, можно удалить один из них, и функциональность при этом не изменится.

В идеале все параметры и активации модели были бы уникальными, и тогда бы не было избыточность сети.

В качества примера можно рассчитать, как изменится сложность небольшой нейронной сети с одним скрытым слоем.

У нас имеется 3 слоя. 1-й слой имеет 6 узлов, 2-й слой — 4 узла и 3-й слой — 2 выходных узла. Для возможности вычисления активации скрытого слоя необходимо произвести 6 операций умножения-накопления (Multiply accumulate — MLA) для каждого узла. Всего необходимо произвести [6*4] + [4*2] = 24 + 8 = 32 MLA операции и держать в памяти 32 параметра.

Предположим, что по каким-то критериям решили удалить красный нейрон. Тогда количество операций изменится на [6*3] + [3*2] = 18 + 6 = 24 MLA операции и также нужно хранить в памяти 24 параметра. Удаление одного нейрона в такой простой сети способствовало снижению вычислительной мощности и объема потребляемой памяти на 25%.

Имея представление об условии удалении весов или узлов, можно применить этот подход к сверточным сетям. Если значения параметров матрицы ядра свертки достаточно малы, то предположительно его активация тоже мала и следовательно оказывает малое влияние на дальнейший результат, а значит, от такого канала можно отказаться.

Теперь вы спросите: «А зачем создавать избыточную архитектуру, которую впоследствии нужно как-то сокращать? Почему бы сразу не построить заведомо меньшую архитектуру без дальнейших оптимизаций?»

Но на самом деле очень сложно обучить меньшую модель с той же точностью, что и более крупную. У крупной модели имеется большее количество пространств поиска оптимального решения, и многое упирается в начальную инициализацию весов.

Quantization

Квантование модели — это популярный метод оптимизации глубокого обучения, при котором данные модели — как параметры сети, так и активации — преобразуются из представления с плавающей запятой в представление с более низкой точностью, например, с использованием 8-битных целых чисел

Это дает несколько преимуществ:

При обработке 8-битных целочисленных данных графические процессоры NVIDIA используют более быстрые и дешевые 8-битные тензорные ядра для вычисления операций свертки и умножения матриц. Это дает большую пропускную способность вычислений.

Перемещение данных из памяти в вычислительные элементы (потоковые мультипроцессоры в графических процессорах NVIDIA) требует времени и энергии, а также выделяет тепло. Снижение точности данных активации и параметров с 32-битных чисел с плавающей запятой до 8-битных целых чисел приводит к 4-кратному сокращению данных, что экономит электроэнергию и снижает выделяемое тепло.

Уменьшение объема памяти означает, что модель требует меньше места для хранения, меньше параметров для обновления, использование кэша выше и т.д.

Методы квантования

Квантование имеет много преимуществ, но снижение точности параметров может легко навредить точности модели. 32-битный тип с плавающей точкой может представлять примерно 4 миллиарда чисел в интервале [-3.4e38, 3.40e38]. Этот интервал представимых чисел также известен как динамический диапазон. Расстояние между двумя соседними представляемыми числами — это точность представления.

В моделях глубокого обучения параметры и данные имеют высокую массу распределения в диапазоне [-1, 1], вероятность, что значение входит в этот диапазон, достаточно высока.

Используя 8-битное целочисленное представление, вы можете представить только 256 различных значений. Эти 256 значений могут быть распределены равномерно или неравномерно, например, для более высокой точности около нуля.

Чтобы преобразовать представление тензора с плавающей запятой x в 8-битное представление x q , необходимо вычислить коэффициент масштабирования (s) и смещения (z). В итоге квантованное значение будет иметь следующий вид:

Теперь поймем, как получить эти коэффициенты. Для этого обозначим, какой диапазон значений имеет изначальный тип x и диапазон квантованных значений x q . Нам нужно решить систему линейных уравнений:

Где β — это верхняя граница диапазона х, β q — верхняя граница диапазона x q , α — нижняя граница диапазона x, α q — нижняя граница диапазона x q .

Отсюда,

Посмотрим на примере: допустим входные значения лежат в диапазоне x [-500, 2050] с типом данных fp32. Нам необходимо преобразовать в signed int8, который лежит в диапазоне x q [-128, 127]. Отсюда s = (2050 + 500)/(127 + 128) = 10,

а z = (-500*127-2050*(-128))/(2050+500)=78.

Когда мы имеем коэффициенты преобразования, мы можем рассчитать любое входное значение:

На практике при процессе квантования есть шанс, что исходное значение находится за пределами допустимого диапазона, таким образом, квантованное значение x q также будет вне диапазона. Поэтому нам необходима операция отсечения значений, не входящих в квантованный диапазон значений:

Использование разных типов для квантования

Тип квантования в основном зависит от операции. Переход от float32 к int8 — не единственный вариант, есть и другие, например, от float32 к float16. Их также можно комбинировать. Например, вы можете квантовать умножения матриц до int8, а активации — до float16.

Квантование — это приближение. В целом, чем ближе приближение, тем меньшее снижение точности можно ожидать. Если вы все квантуете до float16, вы сократите память вдвое и, вероятно, не потеряете точность, но и не получите очень сильного ускорения. С другой стороны, квантование с помощью int8 может привести к гораздо более быстрой обработке, но точность результатов, вероятно, будет хуже. В крайних случаях это даже не сработает и может потребоваться обучение с учетом квантования.

Квантование на практике

Чтобы устранить влияние квантования на качество модели, были разработаны различные методы квантования. Эти методы можно классифицировать как принадлежащие к одной из двух категорий: квантование после обучения (PTQ) или обучение с учетом квантования (QAT).

Как следует из названия, PTQ выполняется после обучения высокоточной модели. С помощью PTQ квантовать веса очень просто — у вас есть доступ к тензорам весов, и вы можете измерить их распределения.

Количественное определение активаций является более сложной задачей, поскольку распределения активаций необходимо измерять с использованием реальных входных данных. Для этого обученная модель с плавающей запятой оценивается с использованием небольшого набора данных, представляющего реальные входные данные задачи, и собирается статистика о распределениях активаций. На заключительном этапе масштабы квантования тензоров активации модели определяются с использованием одной из нескольких целей оптимизации. Этот процесс является калибровкой, и используемый репрезентативный набор данных — набором данных калибровки.

Иногда PTQ не может достичь приемлемой точности на задаче. Тогда вы можете подумать об использовании QAT. Идея QAT проста: вы можете повысить точность квантованных моделей, если включите ошибку квантования в фазу обучения. Это позволяет сети адаптироваться к квантованным весам и активациям.

Существуют различные подходы выполнения QAT — от начала с необученной модели до начала с предварительно обученной модели. Все подходы изменяют режим обучения, чтобы включить ошибку квантования в потери при обучении, вставляя операции ложного квантования в обучающий граф для имитации квантования данных и параметров. Эти операции называются «фальшивыми», потому что они квантуют данные, но затем немедленно деквантовывают данные, чтобы вычисление операции оставалось с точностью до числа с плавающей запятой. Этот трюк добавляет квантование без особых изменений в структуре глубокого обучения.

PTQ — более популярный метод из двух, потому что он прост и является более быстрым методом. Однако QAT почти всегда дает лучшую точность, а иногда это единственный приемлемый метод.

Knowledge distillation

Перенос чрезвычайно огромной модели с миллионами или миллиардами параметров, обученной с помощью высокопроизводительных графических процессоров, на устройство обработки реальных данных может быть невозможен из-за ограничений в ресурсах периферийного устройства.

Поэтому был разработан метод извлечения знаний из большой модели с большим количеством параметров в более легковесную модель. Такая модель учится повторять поведение крупной модели, ее выходные результаты на каждом слое. Обычно такую комбинацию называют «ученик — учитель».

Посмотрим на примере задачи классификации. При передаче знаний от учителя к ученику минимизируется функция потерь распределения классов, предсказанных моделью учителя. Обычно, в случае точных моделей, когда предсказание вероятности одного из классов (верного) близко к 1, а всех остальных — приближены к 0, такие данные мало помогут сети ученика, так как они практически не отличаются от исходной разметки. Поэтому был придуман softmax temperature, который помогает сети ученика повторять не разметку классификации, а вероятностное распределение, что позволяет модели ученика лучше перенять поведение учителя.

Отличия от обучения с нуля

Очевидно, что при использовании более сложных моделей теоретическое пространство поиска больше, чем у меньшей сети. Однако если мы предположим, что такая же (или даже похожая) сходимость может быть достигнута с использованием меньшей сети, то пространство сходимости сети учителя должно перекрываться с пространством решений сети ученика.

К сожалению, это само по себе не гарантирует сходимость сети ученика. Сеть-ученик может иметь сходимость, которая может сильно отличаться от сходимости сети учителя. Однако если сеть-ученик направлена ​​​​на то, чтобы воспроизвести поведение сети учителя (которая уже провела поиск в большем пространстве решений), ожидается, что ее пространство сходимости перекроется с исходным пространством сходимости учительской сети.

Сети учителей и учеников — как это реализовать?

Обучите сеть-учитель. Очень сложная сеть-учитель сначала обучается отдельно с использованием полного набора данных. Это может быть очень сложная и глубокая сеть, которую можно использовать в качестве сети учителя.

Установите соответствие. При проектировании сети-ученика необходимо установить соответствие между промежуточными выходами сети-ученика и учительской сети. Это соответствие может включать в себя непосредственную передачу результата слоя в сети учителя в сеть ученика или выполнение некоторого преобразования данных перед их передачей в сеть ученика.

Пример установления соответствия

Forward pass через сеть учителя. Пропустите данные через сеть учителя, чтобы получить все промежуточные результаты. Back propogation через ученическую сеть. Теперь используйте выходные данные из учительской сети и отношение соответствия для обратного распространения ошибки в ученической сети, чтобы она могла научиться воспроизводить поведение учительской сети.

Weight clustering

Кластеризация весов — это метод уменьшения объема хранения вашей модели путем замены многих уникальных значений параметров меньшим количеством уникальных значений. Наряду с поддержкой платформы и аппаратного обеспечения, кластеризация весов может дополнительно уменьшить требуемые объем памяти и увеличить скорость обработки.

Вот объяснение схемы. Представьте, например, что слой в вашей модели содержит матрицу весов 4×4. Каждый вес сохраняется с использованием значения float32. Когда вы сохраняете модель, вы сохраняете на диск 16 уникальных значений float32.

Кластеризация весов уменьшает размер вашей модели, заменяя аналогичные веса в слое с тем же значением. Эти значения находятся путем запуска алгоритма кластеризации по обученным весам модели. Пользователь может указать количество кластеров (в данном случае 4). Этот шаг показан в разделе «Get centroids» на диаграмме выше, а 4 значения центроидов показаны в таблице «Центроиды». Каждое значение центроида имеет индекс (0–3).

Затем каждый вес в весовой матрице заменяется индексом его центроида. Этот шаг показан в разделе «Assign indices». Теперь вместо сохранения исходной матрицы весов алгоритм кластеризации весов может сохранять модифицированную матрицу, показанную в «Pull indices» (содержащие индекс значений центроидов), и сами значения центроидов.

В этом случае мы уменьшили размер с 16 уникальных чисел с плавающей запятой до 4 чисел с плавающей запятой и 16 2-битных индексов. Экономия увеличивается с увеличением размера матрицы.

Обратите внимание, что даже если мы все еще сохранили 16 чисел с плавающей запятой, теперь у них есть только 4 различных значения. Общие инструменты сжатия (например, zip) теперь могут использовать преимущества избыточности данных для достижения более высокого сжатия.

Преимущества кластеризации весов

Кластеризация весов имеет непосредственное преимущество в сокращении веса модели и размера передачи между форматами сериализации. После кластеризации модели можно дополнительно уменьшить ее размер, пропустив ее через любой обычный инструмент сжатия.

Результаты сжатия и точности

Эксперименты проводились на нескольких популярных моделях, демонстрирующих преимущества сжатия при кластеризации веса. Могут применяться более агрессивные оптимизации, но они уменьшат точность. Хотя в таблице ниже приведены измерения для моделей TensorFlow Lite, аналогичные преимущества наблюдаются и для других форматов сериализации.

В таблице ниже показано, как была настроена кластеризация для достижения результатов. Некоторые модели были более склонны к снижению точности из-за агрессивной кластеризации, и в этом случае выборочная кластеризация использовалась на слоях, которые более устойчивы к оптимизации.

Фреймворки для оптимизации под конечное устройство

OpenVINO

OpenVINO toolkit (или Intel Distribution of OpenVINO Toolkit) — это открытый бесплатный набор инструментов, который помогает ускорить разработку высокопроизводительных решений для использования в различных видеосистемах.

Этот комплексный набор инструментов поддерживает весь спектр решений для компьютерного зрения, который оптимизирует развертывание глубокого обучения и обеспечивает простое исполнение на различных платформах Intel.

OpenVINO решает самые разнообразные задачи, включая детектирование лица, автоматическое распознавание объектов, текста и речи, обработку изображений и многое другое.

Производительность OpenVINO при вычислении сетей на платформах Intel в разы выше по сравнению с популярными фреймворками. Также значительно ниже требования по используемой памяти, что актуально для ряда приложений: на некоторых платформах невозможно запустить сеть с использованием фреймворков по причине нехватки памяти.

Какие есть инструменты в OpenVINO

Deep Learning Model Optimizer (Оптимизатор моделей глубокого обучения) — кроссплатформенный инструмент для импорта моделей и подготовки их к оптимизированному выполнению. Оптимизатор моделей конвертирует и оптимизирует модели популярных фреймворков (таких как Caffe, TensorFlow, MXNet, Kaldi и ONNX) во внутренний формат IR, который используется для представления модели внутри OpenVINO.

Оптимизатор моделей глубокого обучения включает два компонента: Model Optimizer — компонент для конвертации предварительно обученных моделей из формата какого-либо обучающего фреймворка в промежуточный формат (Intermediate Representation, IR) OpenVINO. Поддерживаемые форматы моделей: ONNX, TensorFlow, Caffe, MXNet, Kaldi Inference Engine — компонент для эффективного инференса (запуска) моделей.

Open Model Zoo — открытый репозиторий обученных моделей для решения различных задач. Содержит набор широко известных публичных моделей (более 20) и моделей, решающих различные задачи компьютерного зрения и обученных сотрудниками компании Intel (более 100). В составе можно обнаружить множество примеров и демоприложений, демонстрирующих использование доступных моделей.

Предсобранный OpenCV — версия OpenCV, скомпилированная для оборудования Intel.

Post-training Optimization tool — инструмент для калибровки модели и последующего ее инференса с точностью INT8.

Deep Learning Workbench — веб-графическая среда, позволяющая легко использовать различные сложные компоненты набора инструментов OpenVINO toolkit.

Demo applications — набор примеров.

TensorRT

TensorRT — специальный фреймворк, который максимально утилизирует мощь видеокарты для нейронных сетей.

Приложения на основе TensorRT работают до 40 раз быстрее, чем платформы, использующие только CPU. С помощью TensorRT вы можете оптимизировать модели нейронных сетей, обученные во всех основных средах, провести квантование и развернуть решение в гипермасштабируемых центрах обработки данных, edge-девайсах или автомобильных платформах.

TensorRT построен на CUDA, модели параллельного программирования NVIDIA, и позволяет оптимизировать операции, используя библиотеки, инструменты разработки и технологии CUDA-X для искусственного интеллекта, автономных машин, высокопроизводительных вычислений и графики. С новыми графическим процессорами с архитектурой NVIDIA Ampere TensorRT также использует разреженные тензорные ядра, обеспечивая дополнительный прирост производительности.

TensorRT предоставляет INT8 вычисления с использованием Quantization Aware Training и Post Training Quantization, а также оптимизацию FP16 для производственных развертываний приложений для глубокого обучения, таких как потоковое видео, распознавание речи, рекомендации, обнаружение фрода, генерация текста и обработка естественного языка. Квантование значительно снижает время обработки, что является требованием для многих сервисов, работающих в реальном времени, а также для встроенных приложений.

TensorRT интегрирован с PyTorch и TensorFlow, поэтому вы можете добиться ускорения работы сетей в кратчайшие сроки.

Заключение

В заключение можно сказать, что область оптимизации нейронных сетей значительно продвинулась за последние годы благодаря развитию усовершенствованных методов, таких как прунинг, квантование, дистилляция знаний и кластеризация весов. Эти методы позволяют улучшить производительность и эффективность нейронных сетей, а также уменьшить их размер и вычислительные требования. Сочетая эти методы, мы можем создавать модели, которые одновременно точные и легкие, что делает их идеальными для развертывания на edge-устройствах и других ресурсно-ограниченных средах.

Поскольку область машинного обучения продолжает развиваться, ясно, что оптимизация нейронных сетей останется ключевой областью исследований и разработок. Следуя новейшим методам и лучшим практикам, мы можем продолжать улучшать точность и эффективность наших моделей, что делает искусственный интеллект более доступным и значимым, чем когда-либо ранее.

Источники:"'https://habrastorage.org/getpro/habr/upload_files/509/757/8ca/5097578caead0033105f2710410cde37.jpeg'"['https://habrastorage.org/r/w1560/getpro/habr/upload_files/951/0c8/6d6/9510c86d613ea455449d863dc40c332f.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/4a5/280/170/4a5280170982fb3514471200ed7497ee.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/branding/7f5/e66/21b/7f5e6621bf6f14c09866293c84c584e1.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/986/04d/b21/98604db21cdd570defdb511586e02a0b.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/a3a/148/89a/a3a14889aca6a0d31d621b1e825b17ff.png', 'https://habrastorage.org/r/w48/getpro/habr/avatars/7b4/37c/db5/7b437cdb596a005bcf43a9417b37fb80.jpg', 'https://habrastorage.org/getpro/habr/company/c4c/09b/f12/c4c09bf12bb3370ad4202ee1ebdff59d.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/8a7/77f/88f/8a777f88f8ae105b2331e2fee7458a10.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/011/148/ca0/011148ca026ad34005ddd1376c5f9329.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/060/ab2/d24/060ab2d248b602583b7db9412ec1e9c4.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/83f/435/da8/83f435da8b43992fb6c06a7764256960.png', 'https://habrastorage.org/getpro/habr/upload_files/509/757/8ca/5097578caead0033105f2710410cde37.jpeg', 'https://habrastorage.org/getpro/habr/avatars/7b4/37c/db5/7b437cdb596a005bcf43a9417b37fb80.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/c57/aab/522/c57aab52205813c33f98613e52ca5b61.png', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/601/3e8/6db/6013e86db72d04bc342189533ab56ae2.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/b58/7b3/dfc/b587b3dfcd5810c8488052a66ad3bb78.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/625/e4a/414/625e4a4144045eaf806252bc16aa80c2.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/023/cfe/e6c/023cfee6caf43f2bbd904c89c9e25916.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/350/d6c/6b7/350d6c6b74d0ddc4013fbbe5afc85752.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/90e/331/892/90e3318922d471c92dd1a9b88b0d7b82.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/d89/eb6/cc2/d89eb6cc227da55b98abec581035e675.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/a70/481/25e/a7048125e28bc93a853d4e0de3961258.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/479/b70/0c0/479b700c0475c8d9c6d55002f5a10892.png']"
11'722902'Инструкция по установке LineageOS 20 + MicroG на Google Pixel 6 Pro'LineageOS - это одна из самых популярных прошивок с открытым исходным кодом для Android-устройств, которая основана на исходном коде операционной системы Android. Вот несколько причин, по я считаю...'https://habr.com/ru/post/722902/'"LineageOS - это одна из самых популярных прошивок с открытым исходным кодом для Android-устройств, которая основана на исходном коде операционной системы Android. Вот несколько причин, по я считаю LineageOS лучшим выбором по сравнению с другими прошивками:

Свобода выбора: LineageOS предоставляет пользователям свободу выбора, которую не предоставляет официальная прошивка от производителя устройства. Вы можете устанавливать и настраивать прошивку по своему усмотрению.

Безопасность: LineageOS активно обновляется и исправляет уязвимости безопасности. Это означает, что вы получаете более безопасную операционную систему, чем официальная прошивка.

Поддержка старых устройств: LineageOS поддерживает широкий диапазон устройств, включая старые модели, которые больше не обновляются официально. Это означает, что вы можете продлить жизнь своего устройства, получив последнюю версию Android и новые функции.

Нет предустановленных приложений: LineageOS не включает предустановленные приложения от производителя устройства или поставщика услуг.

В целом, LineageOS представляет собой отличный выбор для пользователей Android, которые хотят больше контроля над своим устройством и не хотят ограничиваться официальной прошивкой от производителя.

Для удобства, инструкция разделена на этапы.

Прежде чем начинать что-то делать обязательно нужно обновить телефон до Android 13, а так же я столкнулся с проблемой, я не смог закрыть загрузчик(bootload lock). Мне пришлось восстанавливать устройство через утилиту от компании Google прежде чем выполнять все действия ниже.

Этап 1. Разблокировка загрузчика

Загрузчик (bootloader) - это программа, которая запускается при старте устройства Android и предназначена для подготовки системы к запуску операционной системы. Загрузчик находится на специальном разделе во внутренней памяти устройства, и он ответственен за выполнение ряда задач, таких как проверка целостности системных файлов, загрузка ядра операционной системы и загрузочного образа (boot image), настройка системных переменных и запуск пользовательской среды.

Нужно на устройстве активировать режим разработчика.

Активация режима разработчика позволяет получить дополнительные функции и возможности для настройки и управления устройством. Этот режим предназначен для разработчиков Android, но может быть полезен и для обычных пользователей, которые хотят получить больше контроля над своим устройством.

В режиме разработчика нам нужно:

Разрешить разблокировку загрузчика - это позволяет разблокировать загрузчик устройства и установить на него неофициальные прошивки и модификации.

Разрешить отладку по USB - это позволяет подключить устройство к компьютеру и управлять им через Android Debug Bridge (adb), что может быть полезно для установки приложений, отладки и тестирования на устройстве.

Теперь выполняем в устройстве

Настройки -> О телефоне -> Номер сборки(нажимаем до тех пор пока не увидим надпись ""Не нужно, вы уже разработчик""

Теперь переходим

Настройки -> Система -> Для разработчиков -> Заводская разблокировка(включаем эту опцию) -> Включить -> Отладка по USB(включаем эту опцию) -> OK

Этап 2. Подготовка инструментов

Прошивку моего устройства я делал из операционной системы Windows 11.

Нам нужна специальная утилита ADB, качаем последнюю версию от сюда для операционной системы Windows.

ADB (Android Debug Bridge) - это инструмент для отладки и тестирования приложений на устройствах Android. Он позволяет разработчикам подключаться к Android-устройствам по USB или Wi-Fi и выполнять различные задачи из командной строки компьютера.

После того как скачали архив, его нужно извлечь в удобное для вас место, я буду извлекать в корень диска C:\

Так же нам нужен будет драйвер для того что бы устройство через adb корректно определялось и работало. Скачивать от сюда

Подключаем устройство к компьютеру через USB кабель. Открываем диспетчер устройств( Winx+X -> Диспетчер устройств ) и проверяем, определилось ли устройство в системе, если устройство определилось, то продолжаем, если нет, то нужно указать вручную устройству где брать драйвер.

Так же нам необходимо скачать рекавери(vendor_boot.img) и сам образ LineageOS(lineage-20... смотрим на самую свежую дату в названии) по этой ссылке

Этап 3. Подготовка устройства

Если все выполнено правильно, то нужно проверить определит ли adb наше устройство. Запускаем командную строку или power shell, для удобства я буду везде говорить терминал. У меня не получилось быстро сделать так, что бы в терминале утилита adb работала всегда, поэтому я буду запускать утилиту adb из папки куда я ее извлек.

Это делать не обязательно

Если кого то смущает что нужно все время писать adb.exe , то добавьте директорию с утилитами adb в системную переменную PATH. Для этого нажмите Win+R на клавиатуре, введите ""sysdm.cpl"" и нажмите ""ОК"", чтобы открыть свойства системы. Нажмите на кнопку ""Переменные среды"" и найдите переменную PATH в разделе ""Системные переменные"". Щелкните на этой переменной и выберите ""Изменить"". Добавьте путь к директории с утилитами ADB в конец поля ""Значение переменной"", разделяя его от других путей точкой с запятой (;). Например, если у вас директория находится по адресу C:\platform-tools, то добавьте "";C:\platform-tools"" в конец значения переменной PATH.

Переходим в каталог с adb cd C:\platform-tools

Выполняем команду в терминале .\adb.exe devices

После выполнения команды .\adb.exe devices в терминале будут отображены все подключенные устройства Android, у которых разрешен режим отладки по USB. Каждое устройство будет иметь уникальный идентификатор, известный как ""Device ID"".

Пример вывода в консоль:

List of devices attached

emulator-5554 device

Если в терминале устройство отобразилось, то все хорошо.

Теперь перезагружаем устройство в bootloader

.\adb.exe reboot bootloader

Как только устройство загрузилось нужно проверить находит ли его компьютер с помощью утилиты fastboot

.\fasboot.exe devices

Если выводе в командной строки устройство отобразилось, то все хорошо.

Разблокируем загрузчик .\fastboot.exe flashing unlock

Качельками громкости выбираем что хотим открыть загрузчик, клавишей блокировки соглашаемся.

Этап 4. Прошивка

Прошиваем vendor_boot.img fastboot flash vendor_boot <vendor_boot>.img не забываем указать полный путь к файлу vendor_boot.img на диске

Грузимся в Recovery. Качельками громкости выбираем Recovery Mode, клавишей блокировки подтверждаем выбор. Ждем пока загрузится режим Recovery. Обязательно должен быть логотип LineageOS в Recovery Mode.

Делаем FactoryReset и FormatData, так же выбрав этот пункт качельками громкости и клавишей блокировки.

Factory reset -> Format data/factory reset -> Format data

Выходим назад, нажав на экране назад.

Теперь прошиваем сам образ LineageOS

Apply update -> Apply from ADB

Выполняем команду в терминале

.\adb.exe sideload <filename>.zip меняем на путь куда скачан образ LineageOS включая само название файла образа

У меня загрузка все время зависала на 6, 7, 8% пока я не подключил устройство через обычный USB-C - USB-A кабель. Из коробки с устройством идет USB-C - USB -C кабель.

После того как прошивка прошла перезагружаем устройство, делаем первичные настройки и подключаемся к беспроводной сети.

Этап 5. Magisk

Magisk - это программа, которая позволяет пользователям Android получать root-права на своих устройствах без изменения системных файлов. Это означает, что Magisk позволяет пользователю получить полный доступ к системе и выполнить различные задачи, такие как установка модификаций системы, настройка различных параметров, скрытие рут-прав от приложений, которые проверяют их наличие, и т.д., не изменяя саму систему и не нарушая ее целостность.

Скачиваем по этой ссылке Magisk для того что бы получить полные права в операционной системе. Нужно изменить у скаченного файла расширение с .apk на .zip и перезагрузить телефон в Recovery Mode.

.\adb.exe reboot bootloader

Грузимся в Recovery. Качельками громкости выбираем Recovery Mode, клавишей блокировки подтверждаем выбор. Ждем пока загрузится режим Recovery.

Выходим назад и выбираем

Apply update -> Apply from ADB

Выполняем команду в командной строке

.\adb.exe sideload <filename>.zip нужно заменить на название файла приложения Magisk

После того как процесс установки завершится перезагружаем устройство.

Запускаем приложение Magisk и в настройках включаем Zygisk

Zygisk - позволяет изолировать процессы приложений, которые требуют root-доступа, от остальных процессов на устройстве. Это позволяет предотвратить возможные конфликты с другими процессами и уменьшить нагрузку на систему.

Magisk -> Настройки -> Zygisk

Скачиваем последнюю версию LSPosed Zygisk(не путать с riru) по ссылке

LSPosed для Magisk позволяет пользователям использовать хуки, чтобы изменять и настраивать поведение приложений на устройстве Android, добавлять дополнительные функции, скрывать рекламу и многое другое. Кроме того, LSPosed для Magisk поддерживает модули, которые можно загрузить из репозитория, чтобы добавить дополнительную функциональность или изменить поведение системы.

Скаченный файл нужно скопировать на телефон, я скопировал в папку Downloads.

Запускаем Magisk

Magisk -> Модули -> Установить из хранилища -> выбрать скопированный файл(LSPosed) -> Перезагрузка

Свайпаем шторку, нажимаем на уведомление от LSPosed и нажимаем на

Ярлык -> Добавить на главный экран

Открываем приложение LSPosed

LSPosed -> Репозитории -> ищем модуль FakeGApps -> Версии -> Установить

Модуль FakeGApps LSPosed содержит набор имитированных сервисов Google Play, таких как Google Cloud Messaging, Google Location Services, Google Drive, Google Play Games и другие, которые могут использоваться в приложениях, требующих Google Play Services.

После того как модуль установился включаем его.

Этап 6. Установка MicroG

После установки и настройки LineageOS нужно опять включить режим разработчика и включить отладку USB.

Теперь почему microG, а не gapps.

MicroG - это альтернативный пакет приложений Google, который имеет открытый исходный код и позволяет использовать функциональность Google без необходимости устанавливать полный набор Google Apps (GApps). Это может быть полезно для пользователей, которые хотят использовать Android без Google, но все еще хотят иметь доступ к некоторым функциям, таким как магазин приложений Google Play. MicroG включает в себя ряд замененных Google сервисов, таких как Google Cloud Messaging (GCM), Google Maps API и Google Play Services.

OpenGApps - это пакет приложений и служб Google, доступный для установки на устройства с открытым исходным кодом операционной системы Android, такие как LineageOS. Эти приложения включают в себя Google Play, YouTube, Google Chrome и другие популярные приложения и сервисы Google.

Основная разница между MicroG и OpenGApps заключается в том, что MicroG предоставляет открытую реализацию сервисов Google, которая не требует установки настоящих сервисов Google, тогда как Open GApps предоставляет полный пакет приложений и сервисов Google, включая Google Play Store и Google Play Services.

Для установки нам понадобится утилита adb

Скачиваем по этой ссылке zip архив с названием microG_Installer_Revived.zip и копируем файл на телефон

Открываем Magisk

Magisk -> Модули -> Установить из хранилища -> выбрать скопированный файл(microG_Installer_Revived.zip) -> Перезагрузка

Теперь открываем настройки microG

Настройки microG -> Проверка работоспособности

Должны стоять галочки везде.

Этап 7. Магазины

Скачиваем по этой ссылки магазин opensource приложений F-Droid.

F-Droid представляет собой альтернативу Google Play Store и предоставляет пользователю доступ к бесплатным и открытым приложениям, которые могут быть загружены и установлены на устройство без каких-либо ограничений. В каталоге F-Droid представлены приложения, которые соответствуют определенным стандартам, таким как свобода использования, безопасность и конфиденциальность данных, и которые не содержат встроенной рекламы или шпионских приложений.

С помощью командной строки устанавливаем все приложения

.\adb.exe install <filename>.apk за место указываем название каждого скаченного apk файла

Я не буду устанавливать Play Market , все нужные мне приложения такие как банкинги я буду брать с помощью приложения Aurora Store приложение ищем в F-Droid и устанавливаем его. Авторизацию в Aurora Store я не использую, скачиваю приложения анонимно.

Этап 8. Блокируем загрузчик

Upd*

По поводу загрузчика, у меня так и не получилось его закрыть именно на этом устройстве, единственный способ который я нашел это само-подписанную сборку LineageOS

Перезагружаем в режим bootloader через терминал

.\adb.exe reboot bootloader

выполняем команду

.\fastboot.exe flashing lock

Качельками громкости выбираем что хотим закрыть загрузчик, клавишей блокировки соглашаемся."'https://habrastorage.org/getpro/habr/upload_files/4b6/0d4/829/4b60d4829dfa46c6523dd5ce4268d44c.png'"['https://habrastorage.org/r/w48/getpro/habr/avatars/0cb/707/8db/0cb7078dbc1c5b86254838a6ec08599c.jpg', 'https://habrastorage.org/getpro/habr/upload_files/4b6/0d4/829/4b60d4829dfa46c6523dd5ce4268d44c.png', 'https://habrastorage.org/getpro/habr/avatars/0cb/707/8db/0cb7078dbc1c5b86254838a6ec08599c.jpg', 'https://mc.yandex.ru/watch/24049213']"
12'722880'[Перевод] 5 паттернов параллельного программирования в GO, которые сделают ваш следующий проект лучше'Параллельное программирование — одна из самых интересных фич, которые может предложить вам Golang. Идея, лежащая в основе параллелизма, заключается в одновременной работе над несколькими разными...'https://habr.com/ru/post/722880/'"Параллельное программирование — одна из самых интересных фич, которые может предложить вам Golang.

Идея, лежащая в основе параллелизма, заключается в одновременной работе над несколькими разными процессами, что помогает избежать застревания в задачах, выполнение которых занимает много времени. Среди основных инструментов, которые предлагает GO для воплощения этого подхода программирования, можно выделить:

Горутины (goroutines): они представляют из себя функции, которые выполняются независимо. Мы можем представить себе их в виде процессов, выполняющихся в другом легковесном и дешевом потоке.

Каналы (channels): они позволяют обмениваться данными между работающими параллельно функциями и при необходимости синхронизировать их.

Горутины в сочетании с каналами дают нам простой способ реализации выполнения независимых задач и общения между ними.

За время работы GO-разработчиком я успел освоить несколько паттернов параллельного программирования, решающих распространенные проблемы, с которыми мы обычно сталкиваемся. В этой статье я собираюсь поделиться с вами пятью из них, которые, как я считаю, будут для вас особенно полезны.

Конструкция For-Select-Done

Но прежде чем говорить о самих паттернах, думаю, очень важно будет обратить ваше внимание на относительно простую структуру, которая активно использоваться в этой статье.

Поскольку мы оперируем несколькими горутинами, нам нужно быть особенно внимательными, чтобы не допустить никаких утечек памяти в нашей программе. На всякий случай напомню, что утечка происходит всякий раз, когда горутина остается висеть в нашей программе после того, как она нам больше не нужна. Решение заключается в том, чтобы посылать горутине сигнал, который дает ей знать, что она может завершиться.

Наиболее распространенным способом реализации этого механизма является объединение цикла for-select с отдельным каналом, который отправляет сигнал завершения горутине. Обычно его называют done-каналом.

func printIntegers(done <-chan struct{}, intStream <-chan int) { for{ select { case i := <-intStream: fmt.Println(i) case <-done: return } } }

forselectdone.go

В этом примере, как только мы запускаем функцию printIntegers в отдельной горутине, она сразу начинает прослушивать канал integerStream . Когда работающая в фоновом режиме функция printIntegers нам больше не нужна, мы можем устранить ее, просто закрыв done-канал.

Следует отметить, что мы можем добиться того же эффекта, используя пакет Context:

func printIntegers(ctx context.Context, intStream <-chan int) { for{ select { case i := <-intStream: fmt.Println(i) case <-ctx.Done(): return } } }

forselectctx.go

Больше информации о функциях пакета Context вы можете найти в моем блогпосте “ Как использовать пакет Context в Go ”.

Когда контекст отменяется, функция printIntegers завершается. Так как конструкция for-select-done служит в качестве основы для остальных паттернов, разобравшись с ней, мы можем смело приступать к их изучению.

1. Паттерн Fan-In (слияние)

Допустим, мы получаем данные из нескольких каналов. Мы можем захотеть перенаправить данные из нескольких источников в один поток.

Диаграмма паттерна Fan-In. Изображение автора

Благодаря этому решению нам нужно будет управлять только одним каналом передачи данных вместо целого множества. Этот паттерн был представлен Робом Пайком (Rob Pike) в докладе о паттернах параллельного программирования в GO (который вы можете посмотреть на YouTube ).

На практике этого можно реализовать, создав новый выходной канал и запустив горутины для всех источников данных, которые будут просто такими же каналами.

Каждая горутина будет отвечать за прослушивание определенного канала и пересылку полученных данных на выходной канал. Таким образом, мы объединяем все имеющиеся у нас каналы в один. В приведенном ниже коде показан пример реализации этого паттерна:

func fanIn(ctx context.Context, fetchers ...<-chan interface{}) <-chan interface{} { combinedFetcher := make(chan interface{}) // 1 var wg sync.WaitGroup wg.Add(len(fetchers)) // 2 for _, f := range fetchers { f := f go func() { // 3 defer wg.Done() for{ select{ case res := <-f: combinedFetcher <- res case <-ctx.Done(): return } } }() } // 4 // Удаление каналов go func() { wg.Wait() close(combinedFetcher) } () return combinedFetcher }

fan-in.go

Давайте детальнее разберем приведенный выше код:

В начале мы создаем waitGroup , которая должна дождаться завершения всех слушателей, сгенерированных внутри цикла for . Цикл for будет повторяться n раз, где n — количество фетчеров, полученных на вход. В каждой итерации мы будем запускать новую горутину, которая будет прослушивать фетчер. Горутина-слушатель представляет из себя конструкцию for-select-done. На каждой итерации она либо отправляет новые данные в канал combinedFetcher , либо завершится, когда контекст будет отменен. В конце мы запускаем еще одну горутину. Она ожидает завершения всех слушателей для того, чтобы закрыть канал combinedFetcher .

2. Берем первые n значений (Take First N) из потока

Следующий паттерн может нам пригодиться, когда нам нужно взять только первые 5, 10 или n значений, поступающие из канала.

Берем первые N значений. Изображение автора

Идея заключается в том, чтобы создать канал с ограниченным количеством выходов. Допустим, мы читаем сообщения из потока данных, и нас интересуют только первые 5 сообщений. Также давайте предположим, что мы не контролируем сам источник данных, мы можем только читать из него. Этот паттерн является хорошим решением этой задачи:

func takeFirstN(ctx context.Context, dataSource <-chan interface{}, n int) <-chan interface{} { // 1 takeChannel := make(chan interface{}) // 2 go func() { defer close(takeChannel) // 3 for i := 0; i< n; i++ { select { case val, ok := <-dataSource: if !ok{ return } takeChannel <- val case <-ctx.Done(): return } } }() return takeChannel }

take.go hosted

Давайте разберемся, что происходит внутри функции TakeFirstN :

Мы создаем новый канал, который будет доставлять полученные данные. Создается новая горутина. Кроме прочего, она добавляет отложенный вызов для закрытия канала takeChannel . Внутри горутины реализуется конструкция for-select-done с небольшой модификацией. Вместо того, чтобы запускать бесконечный цикл for, мы ограничиваем его итерации до n входных данных.

Пример реализации:

func main() { done := make(chan struct{}) defer close(done) // Генерирует канал, отправляющий целые числа от 0 до 9 range10 := rangeChannel(done, 10) for num := range takeFirstN(done, range10, 5) { fmt.Println(num) } }

takemain.go

0 1 2 3 4

Нам удалось получить первые 5 входных данных канала range10 без необходимости закрывать его или каких-либо других манипуляций.

3. Паттерн подписки (Subscription)

Речь идет о паттерне, представленном в докладе Google I/O 2013 — Advanced Go Concurrency Patterns Talk (который вы можете посмотреть здесь ). Этот паттерн представлен здесь почти в том же виде, что и в докладе, но с небольшими изменениями, благодаря которым, с моей точки зрения, он работает немного лучше.

Давайте представим, что мы хотим слушать событие, которое генерируется на регулярной основе. Например: нам нужно получать обновления из API каждые 15 секунд. Этот паттерн использует интерфейс Subscription , отвечающий только за доставку новых данных:

type Subscription interface { Updates() <-chan Item }

Мы также собираемся использовать другой интерфейс в качестве абстракции для получения нужных нам данных (фетчер):

type Fetcher interface { Fetch() (Item, error) }

Для каждого из них у нас будет конкретный тип, реализующий их.

Для подписки:

func NewSubscription(ctx context.Context, fetcher Fetcher, freq int) Subscription { s := &sub{ fetcher: fetcher, updates: make(chan Item), } // Запуск задачи, предназначенной для получения наших данных go s.serve(ctx, freq) return s } type sub struct { fetcher Fetcher updates chan Item } func (s *sub) Updates() <-chan Item { return s.updates }

То, что происходит внутри метода serve , мы рассмотрим чуть ниже.

Для фетчера:

func NewFetcher(uri string) Fetcher { f := &fetcher{ uri: uri, } return f } type fetcher struct { uri string }

Внутри метода serve

Метод serve состоит из уже привычной для нас конструкции for-select-done:

func (s *sub) serve(ctx context.Context, checkFrequency int) { clock := time.NewTicker(time.Duration(checkFrequency) * time.Second) type fetchResult struct { fetched Item err error } fetchDone := make(chan fetchResult, 1) for { select { // Таймер, который запускает фетчер case <-clock.C: go func() { fetched, err := s.fetcher.Fetch() fetchDone <- fetchResult{fetched, err} }() // Случай, когда результат фетчера готов к использованию case result := <-fetchDone: fetched := result.fetched if result.err != nil { log.Println(""Fetch error: %v

Waiting the next iteration"", result.err.Error()) break } s.updates <-fetched // Случай, когда нам нужно закрыть сервер case <-ctx.Done(): return } } }

serve.go

Давайте пройдемся по этому коду, чтобы понять, что тут происходит:

В первую case-ветку нашего switch-оператора мы будем регулярно попадать по таймеру. Что касается фетчера, мы собираемся запускать его внутри другой горутины, чтобы мы оказались не заблокировали, если это займет немного больше времени. Когда результат фетчера, наконец, будет готов к отправке, он будет получен во второй case-ветке. В случае ошибки она прерывает оператор select и ожидает следующей итерации. Третья case-ветка — это стандартный случай ожидания завершения контекста.

3.5 Улучшенный паттерн подписки

Паттерн подписки, в его первоначальном виде, прекрасно работал в моем коде. Но на самом деле в нем есть один баг, который не так просто отловить.

В строке 26 мы отправляем в канал updates данные, который ожидает получить подписчик. Проблема заключается в том, что если подписчик не готов считывать данные, эта строка кода останется заблокированной.

К счастью, есть способ обойти эту проблему, превратив эту строку в отдельную case-ветку в select-операторе.

case s.updates <- fetched:

И объявив получаемую переменную перед циклом for-select.

Но что, если полученные данные не готовы к отправке?

Как оказалось, здесь можно использовать свойства nil-channel’ов. Всякий раз, когда мы пытаемся писать в nil-channel, он остается заблокированным.

var fetchResponseStream chan Item fetched := Item{//Some data here//} fetchResponseStream <- fetched // здесь блокируется

Кроме того, мы собираемся создать флаг pending , чтобы отслеживать, можно ли отправить полученный элемент:

var fetched Item var fetchResponseStream chan Item var pending bool ... for { if pending { fetchResponseStream = s.updates } else { fetchResponseStream = nil } select { ... case: fetchResponseStream <- fetched: pending = false

Давайте соберем все вместе:

func (s *sub) serve(ctx context.Context, checkFrequency int) { clock := time.NewTicker(time.Duration(checkFrequency) * time.Second) type fetchResult struct { fetched Item err error } fetchDone := make(chan fetchResult, 1) var fetched Item var fetchResponseStream chan Item var pending bool for { if pending { fetchResponseStream = s.updates } else { fetchResponseStream = nil } select { // Таймер, который запускает фетчер case <-clock.C: // Ждем следующей итерации, если pending истина if pending { break } go func() { fetched, err := s.fetcher.Fetch() fetchDone <- fetchResult{fetched, err} }() // Случай, когда результат фетчера готов быть считанным case result := <-fetchDone: fetched = result.fetched if result.err != nil { log.Println(""Fetch error: %v

Waiting the next iteration"", result.err.Error()) break } pending = true // Данные можно отправлять по каналу case fetchResponseStream <- fetched: pending = false // Случай, когда нам нужно закрыть сервер case <-ctx.Done(): return } } }

serve-fixed.go

С помощью переменной pending в канале fetchResponseStream , мы смогли снизить вероятность блокировки внутри одной из case-веток.

В функции main :

func main() { ctx, cancel := context.WithCancel(context.Background()) subscription := NewSubscription(ctx, NewFetcher(""http.example.url.com""), 4) time.AfterFunc(3*time.Second, func() { cancel() fmt.Println(""Cancelling context:"") }) for item := range subscription.Updates() { fmt.Println(item) } }

serve-main.go

Map, Filter

Когда мы имеем дело с потоками данных, нам может понадобиться как-то обрабатывать входящие значения. Умножение, деление, ограничение до фиксированной границы – возможностей много.

В современном Javascript массивы имеют некоторые встроенные методы, такие как map и filter , которые используются для обработки или выбора данных из массива. Почему бы не применить эту концепцию для обработки потока, поступающего из канала?

4. Паттерн Map

На примере простого умножения на 2:

Диаграмма паттерна Map. Изображение автора

Чтобы не усложнять наш пример без особой на то необходимости предположим, что тип данных на входе и на выходе — это канал целых чисел.

func Map(done <-chan struct{}, inputStream <-chan int, operator func(int)int) <-chan int { // 1 mappedStream := make(chan int) go func() { defer close(mappedStream) // 2 for { select { case <-done: return // 3 case i, ok := <-inputStream: if !ok { return } //4 select { case <-done: return case mappedStream <- operator(i): } } } }() return mappedStream

map.go

Разбор кода:

Он начинается с создания нового канала, запуска новой горутины и отсрочки закрытия канала внутри нее. Конструкция for-select-done слушает inputStream . Мы проверяем, закрыт ли inputStream , и если это так, то функция возвращает значение. Второй select-оператор внутри будет ожидать сигнала done или возможности переслать данные на канал mappedStream . Этот последний шаг нужен для того, чтобы гарантировать, что горутина всегда завершается, когда done-канал посылает свой сигнал.

Небольшое отличие заключается в том, что для завершения горутины вместо контекста мы используем done-канал.

Этот паттерн может пригодиться при построении конвейера данных, который должен проходить несколько этапов. Он позволяет нам разделить различную логику работы на несколько шагов.

Пример реализации:

func main() { done := make(chan struct{}) defer close(done) // Генерирует канал, отправляющий целые числа от 0 до 9 range10 := rangeChannel(done, 10) multiplierBy10 := func(x int) int { return x * 10 } for num := range Map(done, range10, multiplierBy10) { fmt.Println(num) } }

mapmain.go

0 10 20 30 40 50 60 70 80 90

5. Паттерн Filter

Предположим, у нас есть поток целых чисел, и мы хотим получить только значения больше 10. Мы можем применить этот паттерн, чтобы пропускать только интересующие нас значения:

Паттерн Filter. Изображение автора

Как и в паттерне Map, мы предполагаем, что входные и выходные типы являются каналом целых чисел, но это пример можно расширить для работы с любым другим типом:

func Filter(done <-chan struct{}, inputStream <-chan int, operator func(int)bool) <-chan int { filteredStream := make(chan int) go func() { defer close(filteredStream) for { select { case <-done: return case i, ok := <-inputStream: if !ok { return } if !operator(i) { break } select { case <-done: return case filteredStream <- i: } } } }() return filteredStream }

filter.go hosted

Здесь мы видим почти ту же структуру, что и в паттерне Map . Разница только в том, что функция operator возвращает логическое значение. Если она возвращает true, то отправляем его в выходной поток, иначе просто отбрасываем.

Пример реализации:

func main() { done := make(chan struct{}) defer close(done) // Генерирует канал, отправляющий целые числа от 0 до 9 range10 := rangeChannel(done, 10) isEven := func(x int) bool { return x % 2 == 0 } for num := range Filter(done, range10, isEven) { fmt.Println(num) } }

filtermain.go

0 2 4 6 8

Заключение

Эти пять паттернов являются строительными блоками для создания более крупных и сложных Golang-приложений. Это готовые решения для задач, с которыми вы непременно столкнетесь при реализации параллелизма в GO. Кроме того, вы можете изменять, расширять и создавать новые паттерны на их основе!

Спасибо, что дочитали до конца! Если эта статья была вам полезна, подпишитесь на меня на Medium , где вы можете найти больше таких статей!

Ссылки

Вот ссылки, которые я использовал, чтобы написать эту историю:

Сегодня вечером в OTUS пройдет Mock-собеседование со студентом курса Golang, которое можно посетить бесплатно. На этой встрече проведем собеседование по различным темам, связанным с масштабированием нагрузки, отказоустойчивостью систем, внутреннего устройства баз данных, а также различных паттернов разработки бэкенда. Приглашаем всех желащих. Записаться можно по ссылке."'https://habrastorage.org/getpro/habr/upload_files/08e/d5a/b58/08ed5ab585a89cd82a3abb679433d6fb.png'"['https://habrastorage.org/getpro/habr/company/2d5/0ed/b57/2d50edb57cf45fa07cc4f39f53b78395.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/1d6/92f/0e4/1d692f0e41f0595936bd308af9389a94.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/c9f/cdf/612/c9fcdf612da8315ef7aa2d82d7ce2d12.png', 'https://habrastorage.org/getpro/habr/avatars/b9f/baf/5f9/b9fbaf5f96ae52973706a0716bd9216e.jpg', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/upload_files/08e/d5a/b58/08ed5ab585a89cd82a3abb679433d6fb.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/bdb/547/a20/bdb547a20a43551b4d0e4e4fc0086421.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/c56/97a/039/c5697a0391f6cc7be4d91e9edcfa49b7.png', 'https://habrastorage.org/r/w48/getpro/habr/avatars/b9f/baf/5f9/b9fbaf5f96ae52973706a0716bd9216e.jpg']"
13'722860'Как chatGPT меняет привычный интерфейс'С появлением больших языковых моделей, нас, похоже, ждут большие изменения в том, что до сих пор подразумевалось под «интуитивно понятным» и «дружественным пользователю»...'https://habr.com/ru/post/722860/'"С появлением больших языковых моделей, нас, похоже, ждут большие изменения в том, что до сих пор подразумевалось под «интуитивно понятным» и «дружественным пользователю» интерфейсом.

Список и форма долгое время были основой того самого «интуитивно понятного» интерфейса. Но теперь у нас есть чат. Например такой:

... задали вопрос

... получили ответ

... задумались о том, что вопрос не совсем точный

... уточнили, но пока еще не то, что нужно

... теперь нормально

... и так тоже нормально.

В отличие от всего, что было раньше, взаимодействие с базой данных через языковую модель в режиме чата, и в самом деле интуитивно понятно! И дружественнее не бывает.

Еще пара примеров:

То, что я вам здесь продемонстрировал, работает в 1С:Предприятие 8.3, в типовых конфигурациях Управление торговлей, Комплексная автоматизация и ERP. Но в принципе, это будет работать точно также с любой базой данных. Языковой модели, как вы понимаете, все равно.

Вполне возможно, что очень скоро чат станет основным элементом интерфейса в большинстве приложений, оттеснив формы и списки в область решения узкоспециализированных задач.

Единственное, что не дает мне покоя, когда я гляжу на этот чат. Но ведь это же... да, да, она самая, командная строка. Вот оно — развитие по спирали!"'https://habr.com/share/publication/722860/5eee9de03c1e3ce5e6122b91b2e1a60d/'"['https://habrastorage.org/r/w1560/getpro/habr/upload_files/6e8/a49/373/6e8a49373620d3898bbae7ef0b758eac.png', 'https://habr.com/share/publication/722860/5eee9de03c1e3ce5e6122b91b2e1a60d/', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/db5/a31/aba/db5a31aba2228b04ef1248296705c597.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/avatars/e9c/ef7/d1d/e9cef7d1dccb5f27f30c925790052e87.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/0c8/c3f/00a/0c8c3f00a73ddc27ce808d66bb24385a.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/1ed/a77/d4c/1eda77d4c6c2e409e701c94ae2a09bc9.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/809/460/77b/80946077bafed899273f4d4065c4fe1f.png', 'https://habrastorage.org/r/w48/getpro/habr/avatars/e9c/ef7/d1d/e9cef7d1dccb5f27f30c925790052e87.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/076/ba0/ca0/076ba0ca0c389f992976e984ba134d11.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/eaf/13e/bfd/eaf13ebfd8cddbfdcf5878e0452d7952.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/52e/08a/8ee/52e08a8ee84bc96dc5c1d46ba6838831.png']"
14'722870'Сахар для слоненка -  быстрый старт c PostgreSQL для команд в НЛМК'На протяжении более чем 10 лет работы с PostgreSQL, периодически наблюдаю, как команды на начальном этапе, зачастую, не уделяют внимание ролевой модели базы,...'https://habr.com/ru/post/722870/'"На протяжении более чем 10 лет работы с PostgreSQL, периодически наблюдаю, как команды на начальном этапе, зачастую, не уделяют внимание ролевой модели базы, или как вся команда работает под суперпользователем postgres и забывает про версионирование схемы.

В процессе общения с различными командами в НЛМК у меня появилась идея предложить им «преднастроенный PostgreSQL». Как в итоге сделали — под катом.

Сахар для слоненка

Ролевая модель

Основная задача заключалась в том, чтобы составители миграций, наши пользователи, не выдавали права на объекты в миграциях, а использовали простой и понятный подход вне их зоны контроля.

Поэтому мы попробовали собрать требования к ролевой модели:

единицей владения выступает схема и объекты внутри нее;

у каждой схемы свой владелец — роль (пользователь), под которой создаются и меняются объекты в схеме;

отдельная роль (группа) с правами на запись в объекты схемы (таблицы, sequence), но не имеющая права менять схему;

отдельная роль (группа) на чтение.

Итого, разделяя объекты по схемам, у нас есть роль на чтение всех объектов в ней и на запись. Максимально просто.

Шаблон имен наших ролей имеет вид:

${prefix}${db_name}_${schema_name}_${role_name}

prefix — префикс для ролей (опционально);

db_name — имя базы данных;

schema_name — имя схемы, для которой создана роль;

role_name — имя роли.

Рассмотрим необходимые нам роли (role_name из шаблона выше):

owner — владелец схемы. Роль типа NOLOGON. Эта роль непосредственно никому не присваивается, а передается через sudo роль (ниже будет пример);

sudo — роль типа NOLOGON и NOINHERIT (роль не наследует права ролей, членом которых она является), ей присвоена роль owner. Переключение на роль owner выполняется через SET ROLE;

view — роль типа NOLOGON, через которую предоставляется доступ только на чтение к сущностям в схеме;

write — роль типа NOLOGON, через нее предоставляется доступ только на запись к сущностям в схеме;

pgm — роль типа NOLOGON для инструмента миграции схем.

Сам скрипт создания ролей лежит на github. В нем мы указываем название базы данных, список требуемых схем. Если надо добавить новую схему, то надо еще раз прогнать скрипт, он идемпотентен.

Возможно, вам придется расширить права для ролей или как-то адаптировать их под себя.

Зачем нужна отдельная sudo роль и owner?

В PostgreSQL есть функционал, который позволяет задать права по умолчанию для создаваемых объектов — DEFAULT PRIVILEGES , но объекты должны быть созданы именно из под этой роли. Мы задаем DEFAULT PRIVILEGES для роли owner. Перед тем, как что‑то создать, необходимо явно сделать set role *_owner .

Если мы просто добавим пользователя в эту роль, то он сможет создавать объекты во всех схемах и есть риск, что может забыть сделать set role .

Чтобы решить эту проблему, мы сделали отдельную роль _sudo со свойством NOINHERIT , которая является членом роли owner, и пользователь вынужден всегда делать set role явно, чтобы получить нужные права.

Миграция схем

После создания схем и базовых ролей, следующим важным компонентом является инструмент миграции. Наш выбор пал на PGmigrate от Yandex. Он прост, позволяет использовать нашу ролевую модель и подход gitops.

В git для проекта мы создаем следующую структуру:

<db_name>/ <schema_name>/ migrations.yml <migrations> <callbacks>/ <afterEach>/ 00_after_each.sql <beforeEach>/ 00_before_each.sql ...

Для генерации структуры можно воспользоваться скриптом из того же репозитория в github.

В файле migrations.yml мы указываем запросы, которые будут выполнены до и после каждой версии миграции.

migrations.yml callbacks:

beforeEach:

- callbacks/beforeEach

afterEach:

- callbacks/afterEach

conn: dbname=${db_name}

schema: ${schema_name}

00_before_each.sql , делаем SET ROLE на владельца схемы и устанавливаем search_path в имя схемы.

00_before_each.sql SET ROLE ${prefix}${db_name}_${schema_name}_owner;

SET search_path = '${schema_name}';

В 00_after_each.sql , сбрасываем роль и search_path .

00_after_each.sql RESET ROLE;

SET search_path TO DEFAULT ;

Пример такой структуры можно посмотреть в директории migrations на github.

О PGmigrate.... Мы используем немного измененный скрипт PGmigrate, но создали issue и надеемся скоро опять использовать официальный.

Пример запуска примера из репозитория с github # Запускаем postgres и pgadmin $ docker-compose up -d postgres pgadmin # Создаем новую базу данных с именем dwh $ docker-compose exec -u postgres postgres psql -c 'CREATE DATABASE dwh' CREATE DATABASE # Создаем схемы и ролевую модель $ docker-compose exec -u postgres postgres /bin/bash /opt/scripts/create_schema.sh Schema: dwh_raw GRANT dwh_raw_view,dwh_raw_write TO dwh_raw_owner GRANT dwh_raw_view TO dwh_raw_sudo GRANT dwh_raw_owner TO dwh_raw_sudo GRANT CONNECT ON DATABASE dwh TO dwh_raw_view GRANT dwh_raw_view TO dwh_raw_write GRANT usage ON SCHEMA raw TO dwh_raw_view GRANT ALL ON SCHEMA raw TO dwh_raw_owner GRANT ALL ON SCHEMA raw TO dwh_raw_pgm GRANT dwh_raw_sudo TO dwh_raw_pgm ALTER DEFAULT PRIVILEGES FOR ROLE dwh_raw_owner IN SCHEMA raw GRANT SELECT ON SEQUENCES TO dwh_raw_view ALTER DEFAULT PRIVILEGES FOR ROLE dwh_raw_owner IN SCHEMA raw GRANT SELECT ON TABLES TO dwh_raw_view ALTER DEFAULT PRIVILEGES FOR ROLE dwh_raw_owner IN SCHEMA raw GRANT ALL ON SEQUENCES TO dwh_raw_write ALTER DEFAULT PRIVILEGES FOR ROLE dwh_raw_owner IN SCHEMA raw GRANT EXECUTE ON FUNCTIONS TO dwh_raw_write ALTER DEFAULT PRIVILEGES FOR ROLE dwh_raw_owner IN SCHEMA raw GRANT INSERT,UPDATE,DELETE,TRUNCATE ON TABLES TO dwh_raw_write Schema: dwh_ods GRANT dwh_ods_view,dwh_ods_write TO dwh_ods_owner GRANT dwh_ods_view TO dwh_ods_sudo GRANT dwh_ods_owner TO dwh_ods_sudo GRANT CONNECT ON DATABASE dwh TO dwh_ods_view GRANT dwh_ods_view TO dwh_ods_write GRANT usage ON SCHEMA ods TO dwh_ods_view GRANT ALL ON SCHEMA ods TO dwh_ods_owner GRANT ALL ON SCHEMA ods TO dwh_ods_pgm GRANT dwh_ods_sudo TO dwh_ods_pgm ALTER DEFAULT PRIVILEGES FOR ROLE dwh_ods_owner IN SCHEMA ods GRANT SELECT ON SEQUENCES TO dwh_ods_view ALTER DEFAULT PRIVILEGES FOR ROLE dwh_ods_owner IN SCHEMA ods GRANT SELECT ON TABLES TO dwh_ods_view ALTER DEFAULT PRIVILEGES FOR ROLE dwh_ods_owner IN SCHEMA ods GRANT ALL ON SEQUENCES TO dwh_ods_write ALTER DEFAULT PRIVILEGES FOR ROLE dwh_ods_owner IN SCHEMA ods GRANT EXECUTE ON FUNCTIONS TO dwh_ods_write ALTER DEFAULT PRIVILEGES FOR ROLE dwh_ods_owner IN SCHEMA ods GRANT INSERT,UPDATE,DELETE,TRUNCATE ON TABLES TO dwh_ods_write Schema: dwh_cdm GRANT dwh_cdm_view,dwh_cdm_write TO dwh_cdm_owner GRANT dwh_cdm_view TO dwh_cdm_sudo GRANT dwh_cdm_owner TO dwh_cdm_sudo GRANT CONNECT ON DATABASE dwh TO dwh_cdm_view GRANT dwh_cdm_view TO dwh_cdm_write GRANT usage ON SCHEMA cdm TO dwh_cdm_view GRANT ALL ON SCHEMA cdm TO dwh_cdm_owner GRANT ALL ON SCHEMA cdm TO dwh_cdm_pgm GRANT dwh_cdm_sudo TO dwh_cdm_pgm ALTER DEFAULT PRIVILEGES FOR ROLE dwh_cdm_owner IN SCHEMA cdm GRANT SELECT ON SEQUENCES TO dwh_cdm_view ALTER DEFAULT PRIVILEGES FOR ROLE dwh_cdm_owner IN SCHEMA cdm GRANT SELECT ON TABLES TO dwh_cdm_view ALTER DEFAULT PRIVILEGES FOR ROLE dwh_cdm_owner IN SCHEMA cdm GRANT ALL ON SEQUENCES TO dwh_cdm_write ALTER DEFAULT PRIVILEGES FOR ROLE dwh_cdm_owner IN SCHEMA cdm GRANT EXECUTE ON FUNCTIONS TO dwh_cdm_write ALTER DEFAULT PRIVILEGES FOR ROLE dwh_cdm_owner IN SCHEMA cdm GRANT INSERT,UPDATE,DELETE,TRUNCATE ON TABLES TO dwh_cdm_write # Создаем пользователя для выполнения миграций и добавляем в _pgm группы схем $ docker-compose exec -u postgres postgres psql -c ""create user pgmigrate with password '1234' in group dwh_raw_pgm,dwh_ods_pgm,dwh_cdm_pgm;"" CREATE ROLE # Выполняем миграции $ docker-compose run pgmigrate bash /opt/scripts/do_migrate.sh + pgmigrate -d /opt/migrations/dwh/raw -v -m raw --check_serial_versions -t latest migrate + pgmigrate -d /opt/migrations/dwh/ods -v -m ods --check_serial_versions -t latest migrate + pgmigrate -d /opt/migrations/dwh/cdm -v -m cdm --check_serial_versions -t latest migrate # Подключаемся к базе dwh с помощью psql $ docker-compose exec -u postgres postgres psql -d dwh # Наши созданные скриптом create_schema.sh схемы dwh=# \dn List of schemas Name | Owner --------+------------------- cdm | postgres ods | postgres public | pg_database_owner raw | postgres (4 rows) # Таблица foo создана pgmigrate c владельцем dwh_raw_owner # Таблица schema_version - техническая для pgmigrate, без Access privileges dwh=# \dt raw.* List of relations Schema | Name | Type | Owner --------+----------------+-------+--------------- raw | foo | table | dwh_raw_owner raw | schema_version | table | pgmigrate dwh=# \dp raw.* Access privileges Schema | Name | Type | Access privileges | Column privileges | Policies --------+----------------+-------+-------------------------------------+-------------------+---------- raw | foo | table | dwh_raw_view=r/dwh_raw_owner +| | | | | dwh_raw_write=awdD/dwh_raw_owner +| | | | | dwh_raw_owner=arwdDxt/dwh_raw_owner | | raw | schema_version | table | | (2 rows)

Проверки в CI

Перед миграцией в master мы выполняем следующие проверки:

не изменены файлы уже примененных версий миграций;

в скритах миграций нет set role или reset role;

имя файла в миграции соответствует требованиям PGmigrate: V<version>__<description>.sql;

версии в миграциях последовательно возрастают (pgmigrate умеет это проверять);

изменяется только одна схема в директории со схемой (pgmigrate умеет это проверять, но непосредственно в момент применения миграций);

наличие в скрипте миграции первой строкой указания использовать utf-8( /* pgmigrate-encoding: utf-8 */ ), чтобы избежать проблем с non‑ascii символами;

применение всех миграций в CI на временной БД для проверки на наличие ошибок в SQL коде.

После чего для каждой схемы, где были изменения, выполняем миграции с помощью команды:

pgmigrate.py -d <db_name>/<schema_name> -v -m <schema_name> --check_serial_versions -t latest migrate

Пользователи и группы

Мы стараемся везде использовать LDAP аутентификацию и не создавать локальных учетных записей.

На каждую роль *_view , *_write и *_sudo создается своя группа в AD. Периодически скриптом мы синхронизируем членов этой группы с соответствующими группами в PostgreSQL, создаем отсутствующих пользователей. Наш скрипт умеет работать со вложенными группами, и это позволяет в качестве членов групп использовать другие группы в AD, например группа «все разработчики BI».

Полезно также иметь общую группу в AD на каждый сервер, включающую все эти группы, чтобы в pg_hba.conf или в PGAdmin дополнительно ограничить доступ с помощью ldapsearchfilter:

Пример из pg_hba.conf: # Все пользователи группы g-dbx-users, в том числе и во вложенных группах host all all 0.0.0.0/0 ldap ldapserver=ldap.expample ldapsearchfilter=""(&(samAccountName=$username)(memberof:1.2.840.113556.1.4.1941:=CN=g-dbx-users,OU=pg,....))""

Реальный пример

Рассмотрим пример проекта, который использует описанный выше подход.

Его архитектура представлена ниже:

В базе данных созданы три схемы (слоя): raw, ods, cdm. На каждую схему в PostgreSQL созданы 4 роли(+1 для миграций — pgm).

В LDAP на каждую схему создано три группы, соответствующие *_view, *_write и *_sudo.

Код миграций лежит в GIT. Миграции выполняются под доменным пользователем dbx‑pgmigrate , который входит в локальные группы dbx_raw_pgm , dbx_ods_pgm , dbx_cdm_pgm в PostgreSQL.

Перекладка данных между схемами/слоями осуществляется сервисом Airflow, который подключается к БД под пользователем dbx‑airflow и имеет только write права на схемы.

Также на схеме присутствует BI сервис, который подключается к PostgreSQL под пользователем bi‑user и имеет права только на чтение в схему CDM. И сервис PGAdmin, через который пользователь dbx‑dev‑user1 может подключаться к PostgreSQL и выполнять запросы на чтение во всех трех схемах.

Итого

Команды постоянно вынуждены изучать новое, стек применяемых технологий только множится, и иногда на проекте, особенно на старте, может просто не быть выделенного DBA. Поэтому мы постарались проработать и упростить начало использования PostgreSQL в разрабатываемых сервисах.

Надеюсь, предложенный подход вам будет полезен!"'https://habrastorage.org/getpro/habr/upload_files/ab3/dfb/9f4/ab3dfb9f403797389663910e087619d0.png'"['https://habrastorage.org/getpro/habr/company/f94/8cb/bd3/f948cbbd32b9bb3aaffe413086a85906.png', 'https://habrastorage.org/getpro/habr/upload_files/ab3/dfb/9f4/ab3dfb9f403797389663910e087619d0.png', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/81a/69d/0ed/81a69d0edf81a6c3669d8ba78a9c0717.jpg', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/ab3/dfb/9f4/ab3dfb9f403797389663910e087619d0.png', 'https://habrastorage.org/r/w48/getpro/habr/avatars/40c/d1a/5fa/40cd1a5fa1e5a6174662177bf730e2fe.jpg', 'https://habrastorage.org/getpro/habr/avatars/40c/d1a/5fa/40cd1a5fa1e5a6174662177bf730e2fe.jpg']"
15'722892'SRE: управление инцидентами'Автор статьи: Рустем Галиев IBM Senior DevOps Engineer & Integration Architect. Официальный DevOps ментор и коуч в IBM Привет Хабр! Не так давно общался с SRE в нашей команде и он...'https://habr.com/ru/post/722892/'"Автор статьи: Рустем Галиев IBM Senior DevOps Engineer & Integration Architect. Официальный DevOps ментор и коуч в IBM

Привет Хабр! Не так давно общался с SRE в нашей команде и он рассказал мне о базовых принципах процесса управления инцидентами, теперь я поделюсь этим с вами, быть может кому‑то поможет.



Управление инцидентами включает в себя мониторинг, анализ, планирование и выполнение. SRE работают с операционными группами, экспертами по техническим вопросам, разработчиками, инженерами DevOPs, владельцами приложений и другими.

При оценке инцидентов SRE обращают внимание на такие критерии, как импакт и частота повторения — для того, чтобы определить, какие инциденты требуют дальнейшего анализа.



Есть некоторые практики, которых придерживаются в нашей команде:

Прежде чем произойдет инцидент, сотрудничайте с другими, настроив уведомления о предупреждениях и информационные панели, чтобы во время события уведомлялись нужные люди и предпринимались правильные действия. Во время инцидента SRE несут ответственность за решение инцидента; выполнение заявок на обслуживание; мониторинг каналов событий и журналов; и анализ информационных панелей, корреляций событий, данных о тикетах и тенденциях. Необходимость меняться. После разрешения инцидента дополнительные действия приводят к управлению изменениями, управлению проблемами и обновлениям управления конфигурацией, чтобы снизить вероятность возникновения подобных инцидентов в будущем.



Автоматизация, связанная с проблемами, постоянно обновляется, чтобы предотвратить повторение инцидента или быстрее решить его, если все же он произойдет снова. Ниже приведена архитектура, которой мы пользуемся (больше как референс).

Концепции управления инцидентами

Инфраструктура всегда в мониторинге, что выявляет отклонения от нормального поведения, такие как уменьшение времени отклика, и оповещает ops об инцидентах. Первые респондеры, которые всегда на связи, выявляют неисправный компонент и восстанавливают обслуживание как можно быстрее. Они делают это с помощью автоматизации и runbook (скрипт, который фиксит ту или иную проблему, если совсем уж просто), чтобы устранить зависимость и риски, связанные с ручным выполнением задач. В то время как первые респондеры используют информационные панели, которые обеспечивают обзор приложения, они не смотрят на консоли в ожидании сигналов тревоги. Вместо этого они уведомляются о проблемах через алерты. Эти алерты объединяются различными системами мониторинга, сопоставляются и дополняются соответствующей информацией, такой как имя приложения, затронутые пользователи и заинтересованные стороны, а также информация о соглашении об уровне обслуживания (SLA). Эти алерты являются и должны быть действенными, в идеале с четким описанием мер по смягчению последствий. Используя call‑rotation и списки on‑call, алерт отправляется правильному первому респонденту, который предпринимает необходимые действия. Оповещения, которые не могут быть быстро определены для смягчения последствий, требуют дополнительного анализа. SME в нескольких доменах сотрудничают, чтобы изолировать инцидент и определить эффективный ответ. Такие технологии, как ChatOps, помогают в этом сотрудничестве. Инструменты DevOps и управления услугами также интегрируются через бот‑агентов. Командир инцидента (да у нас есть и такие роли) координирует эти задачи и поддерживает прозрачную связь с пострадавшими заинтересованными сторонами. Целью управления инцидентами является восстановление службы. Команда не тратит время на анализ основной причины проблемы. Этот анализ проводится на следующем этапе: управление проблемами.

Подходы к управлению инцидентами включают перезапуск микросервиса, настройку балансировщика нагрузки для игнорирования отказавшего инстанса или откат к предыдущей версии. Типичные принципы DevOps, такие как blue‑green deployment (CD), упрощают реализацию этих подходов.

Инструменты процесса управления инцидентами

Целью мониторинга инцидента является обнаружение простоев, сатурация производительности и т. д. Поскольку неэффективно просто заставлять наших сотрудников постоянно следить за консолями, следующим важным элементом является настройка алертов, чтобы нужный SME уведомлялся, когда что-то идет не так. В этих случаях тулчейн играет важную роль.



SRE часто должны сотрудничать с SME, чтобы изолировать проблему и определить стратегию смягчения последствий. Вместо того, чтобы полагаться на электронную почту и телефоны, SRE используют платформы уведомлений и совместной работы, из такой практики как ChatOps.







В дополнение к мониторингу и активному исследованию служб и API SRE также должны отслеживать файлы журналов служб. Этот мониторинг может помочь выявить проблемы до того, как они повлияют на службу. Это также может ускорить этап идентификации и разрешения инцидента.

По мере увеличения нагрузки и усложнения приложений, первые респондеры начинают получать слишком много предупреждений. Они получают оповещения, связанные с симптомами и причинами. Некоторые предупреждения могут не действовать. События могут не предоставлять достаточный контекст для быстрого реагирования, например соглашение об уровне обслуживания (SLA) или данные об импакте. На этом этапе должен быть “event management” в тулчейне. Управление событиями сопоставляет связанные события, удаляет” шум”, чтобы отображались только предупреждающие действия, и дополняет эти события дополнительным контекстом.

Усовершенствованная цепочка инструментов с управлением событиями будет выглядеть так, как показано здесь.



Чтобы быстро реагировать на проблемы, нужны runbook и средства автоматизации. Runbook могут вызываться автоматически либо для запуска диагностических команд, либо для устранения проблемы. Runbook также может запускаться вручную первым ответчиком и специалистом по разрешению инцидентов. Чтобы избежать ручного входа в систему и риска неправильного набора команд, автоматизированные и полуавтоматические runbook обеспечивают безопасное и согласованное выполнение.

По мере добавления новых инструментов SR-инженерам требуется обзор всего “ландшафта”. Эта видимость не заменяет существующие пользовательские интерфейсы продукта (UI), а вместо этого дополняет их и обеспечивает комбинированное представление среды на панелях управления для конкретных пользователей. В идеале в этих представлениях также отображается дополнительная информация, например действия по развертыванию или информация об уровне обслуживания.

Кроме того, информация об инцидентах постоянно отслеживается в инструментах оформления заявок, которые являются источником достоверной информации для расчета SLA. Предприятиям особенно необходимо вести журнал аудита для всех инцидентов. Отслеживаются начало и конец инцидента, а также любые важные обновления. Интеграция всей цепочки инструментов автоматизирует заполнение этого журнала действий.





Коммуникация по управлению инцидентами во время простоя

Прозрачность — ключевой аспект завоевания и поддержания доверия пользователей вашего сервиса. Поскольку предприятия полагаются на доступность и качество услуг, они заинтересованы в получении информации о любых проблемах или инцидентах, влияющих на качество этих услуг. В дополнение к незапланированным отключениям любое плановое техническое обслуживание должно следовать той же парадигме. Руководящими принципами любого сообщения о сбоях являются точность, ясность и своевременное предоставление информации.



Можно использовать правило: “How? Who? When?”



How: Поставщики услуг информируют своих пользователей об инцидентах несколькими способами:

Веб-страница с информацией о статусе,

Информация о статусе через социальные сети, такие как Twitter,

Эмейл,

Программный API (веб-хук),

Инженерный блог компании.

Предоставить нужно информацию об инциденте в сочетании этих каналов. Не будем думать, что пользователи всегда знают, где искать.

Who: При составлении плана передачи информации о состоянии четко определите, кто отвечает за выполнение каждой задачи. Важно определить зоны ответственности, чтобы общение осуществлялось быстро и эффективно.



Типичной ролью, отвечающей за коммуникацию и координацию, является руководитель инцидента.

Этот человек олицетворяет культуру прозрачности, честности и подотчетности перед клиентами в качестве руководящих принципов.

Сосредоточив общение в одной роли, убедитесь, что общение последовательное и не конфликтное или, что еще хуже, противоречивое.

Для своевременного предоставления обновлений этот человек должен иметь возможность предоставлять обновления без трудоемкого процесса утверждения.

Использование готовых сценариев или шаблонов постов — хорошая стратегия для заблаговременного завершения проверки и утверждения.

When: Также важно определить, когда общаться. Как только произойдет сбой, подтвердите инцидент и сообщите об этом пользователям. Пользователи в любом случае узнают об инциденте, и если предоставленная поставщиком страница состояния отображает что “все хорошо”, люди будут чувствовать, что им лгут.

После подтверждения сбоя часто обновляйте статус.

Найдите баланс между регулярными и содержательными обновлениями. Как только статус изменится или появится значимая информация для обмена, отправьте обновление.

Следующее обновление может занять некоторое время, но пользователи хотят видеть, что команда все еще работает над инцидентом.

Рекомендуется обновлять информацию по истечении определенного времени, например, каждые 30 минут, даже если новая информация недоступна. Однако будьте осторожны, чтобы не показывать один и тот же ответ для нескольких итераций.

После восстановления службы опубликуйте уведомление об окончании сбоя.

Характеристики инцидента

Каким бы надежным ни был сервис, иногда возникают проблемы, которые могут повлиять на качество и доступность. Управление инцидентами — это процесс, посредством которого мы восстанавливаем поврежденный сервис. Работая вместе в команде, мы можем определить характеристики сервиса, чтобы как можно быстрее вернуть сервис в нужное русло.

Некоторые инциденты требуют дальнейшего анализа:

Когда проблемы возникают более одного раза (частота),

Когда сбой может затронуть многих пользователей (импакт),

Когда система не работает так, как задумано.

Каскадный сбой — это сбой, который со временем нарастает в результате положительной обратной связи. Когда одна часть всей системы выходит из строя, возрастает вероятность того, что другие части системы также откажут.Этот шаблон может создать эффект домино или каскада, который отключает все функции службы.



Снижение производительности: падение производительности относится к службам, которые не работают должным образом. Обнаружение и устранение ухудшения производительности может быть затруднено, но SRE несут ответственность за обнаружение и устранение проблем с ухудшением производительности.



Сбой функционала: Иногда команда создает необходимую функцию, и она не работает должным образом.



Воздействие вышестоящих и нижестоящих зависимостей



Зачем учитывать восходящие и нисходящие зависимости при обработке инцидента?

Для вас важно учитывать восходящие и нисходящие зависимости, влияющие на микросервис, при обработке инцидента, поскольку они имеют решающее значение для разрешения инцидента в целом.

Примеры восходящих проблем включают в себя:

Проблема с сетью,

Проблема аутентификации.

Примеры нижестоящих проблем включают:

Проблема с облачным объектным хранилищем,

Проблема с блочным хранилищем,

Сбой в базе данных, на которую повлияла проблема микросервиса.

Зная, какие восходящие и нисходящие зависимости находятся в микросервисе, и как исправить любые проблемы с ними, вы приблизитесь к разрешению общего инцидента.

Распределенная трассировка

Один из методов, который SRE могут использовать для специального изучения зависимостей приложения — это распределенная трассировка. Его можно использовать для идентификации неудачной транзакции и отслеживания потока транзакции через приложение микрослужбы.

Распределенная трассировка — это метод, позволяющий регистрировать информацию в приложениях на основе микрослужб.

Уникальный идентификатор транзакции передается через цепочку вызовов каждой транзакции в распределенной топологии.

Одним из примеров транзакции является взаимодействие пользователя с веб-сайтом.

Уникальный идентификатор генерируется в точке входа транзакции.

Затем идентификатор передается каждой службе, используемой для завершения задания, и записывается как часть информации журнала служб.

Не менее важно включать временные метки в сообщения журнала вместе с идентификатором.

Идентификатор и отметка времени объединяются с действием, которое выполняет служба, и состоянием этого действия.

Создание Runbook для устранения неполадок и смягчения последствий распространенных инцидентов

С помощью инструментов Runbook, интегрированных с уведомлениями о событиях, SRE могут определять автоматические процедуры, которые будут выполняться при возникновении определенного события. Рассмотрим, что происходит, когда система управления событиями получает событие, указывающее на сбой службы. Система может выполнять автоматизированные действия, определенные в runbook. Например, когда система получает событие сбоя службы, сценарий runbook предлагает сделать снимок системы. SRE могут автоматизировать следующие действия:

Использование системного имени хоста и учетных данных для входа на сервер,

Получение списка процессов, памяти, использования cpu, и другой информации,

Извлечение любых связанных журналов и сообщений трассировки.

Результаты действий могут быть немедленно отправлены первому ответчику, как только действие будет завершено. Выявление неполадок с помощью автоматизированных runbook — это низкий барьер, поскольку команды доступны только для чтения и обычно не наносят вреда системам. Со временем команда может писать сценарии для автоматического решения проблем без ручного вмешательства.

Этапы зрелости Runbook

По мере взросления команды и приложения могут развиваться и runbook. Зрелость runbook включает следующие этапы:

Ad hoc: это начальное состояние характеризуется отдельными ручными действиями без документации или согласованности.

Repeatable: стандартные действия задокументированы и согласованы во всей организации. Эти действия по-прежнему выполняются вручную.

Defined: действия принудительно выполняются. Действия становятся доступными, поскольку сценарии и задачи предоставляются оператору в контексте инструментов управления.

Managed: система предлагает правильное действие для события. Используя базовые функции if/then , система автоматически запускает действия.

Optimized: на самом высоком уровне применяется аналитика, чтобы определить, когда и что автоматизировать.

Управление модулями Runbook

Чтобы runbook был оптимальным, SRE должны управлять его содержимым и поддерживать его. Приложение может быть обновлено до версии, требующей других действий. Технологии и инфраструктура могут измениться, что приведет к изменениям в командах runbook. При выборе решения runbook рассмотрите возможность управления отзывами пользователей, управления библиотеками, контролем доступа, API, отчетами и аудитом.



User feedback: Когда инженеры используют runbook, собирайте их отзывы.

Вы можете задать им следующие вопросы:

Помог ли runbook решить проблему?

Как можно улучшить модуль runbook?

Library management: Некоторые решения Runbook предоставляют библиотеку «строительных блоков» Runbook, которые можно использовать для создания Runbook. В зависимости от инструмента библиотека может быть предоставлена поставщиком или представлять собой набор библиотек, предоставленных другими пользователями.

Например, Red Hat® Ansible поставляется с модулями для выполнения общих задач. Разработчик может использовать модуль для более быстрой разработки Runbook.



Access control: Для средних и крупных организаций решающее значение имеет управление пользователями и ресурсами. Конкретным пользователям или группам пользователей требуется доступ для работы с определенной группой ресурсов. Если необходим аудит, аудитору может потребоваться доступ только для чтения к большей группе ресурсов.

Другой формой управления доступом является имя пользователя и пароль для средства Runbook для выполнения действий с управляемыми ресурсами. Может потребоваться несколько уровней контроля доступа.

Например, чтобы прочитать состояние сервера, используйте обычные учетные данные для входа, но для изменения конфигурации сервера учетная запись должна иметь привилегии root. Runbook могут предоставить средства безопасного делегирования привилегированных команд администраторам.



API: В рамках интеграции Runbook система управления событиями может активировать Runbook. Runbook также можно запускать в результате действия инструмента на панели мониторинга топологии, как действие из системы ChatOps или как задачу в конвейере непрерывной доставки. Решение Runbook должно предоставлять набор API, чтобы другой компонент интеграции мог вызывать правильный Runbook с соответствующими полномочиями.



Report and audit: При рассмотрении решения Runbook обратите внимание на его возможности отчетности и аудита. Полезно знать, какие Runbook выполняются чаще всего или получают самые высокие оценки при решении проблем.. Вашей организации может потребоваться аудит Runbook в рамках деятельности по обеспечению соответствия требованиям.

Информация о выполнении модуля Runbook может быть передана группе SRE из отчета, чтобы расставить приоритеты в отношении необходимых работ по улучшению приложений или инфраструктуры.

Runbook или пофиксить?

В SRE подход к решению повторяющейся проблемы заключается в устранении ее источника. Runbook может быть хорошим краткосрочным тактическим решением, когда исправление требует дополнительного времени и усилий.

SRE может использовать отчет, созданный решением Runbook, и работать с приоритетным списком исправлений. По мере реализации большего количества исправлений требуется запускать меньше Runbook. Помните, что лучше предотвратить проблему до того, как она возникнет, чем решать ее после того, как она возникла. Однако иногда runbook является решением.

Организация может инвестировать в коммерческое программное обеспечение, которое команда SRE не может изменить напрямую, или SRE может использовать решение Runbook для автоматизации повторяющихся задач, выходящих за рамки управления инцидентами.

Методы устранения неполадок и постоянное совершенствование

Устранение неполадок не зависит от удачи. Наша основная теория заключается в том, что SRE могут изучать и обучать эффективным стратегиям. Теория, лежащая в основе эффективного устранения неполадок, заключается в том, что этому можно научиться и чему можно научить.



Процесс устранения неполадок — это способность выдвигать гипотезы о причинах сбоя и тестировать решения.

Помните об этих важных моментах:

Не все неудачи одинаково вероятны — при прочих равных условиях наиболее вероятным решением может быть простое или очевидное решение.

Корреляция не является причинно-следственной связью.

Шаги в модели устранения неполадок:

Получаем сообщение о проблеме, что-то не так с системой.

Просмотрим телеметрию и логи, чтобы понять ее текущее состояние.

Определим некоторые возможные причины.

Активно «лечим» систему — то есть изменяем систему контролируемым образом и наблюдаем за результатами.

Повторно тестируем, пока не будет выявлена основная причина.

Примем меры для предотвращения повторения.

Напишите репорт о решении, задокументируем это.

Так и работаем.

Техническая поддержка пользователей - это всегда стресс. Хочу порекомендовать вам бесплатный вебинар от моих коллег, в рамках которого эксперт OTUS Сергей Караткевич расскажет и покажет на практике, как именно можно связать базу знаний, мониторинг, скрипты поддержки и выстроить эскалационную цепочку."'https://habrastorage.org/getpro/habr/upload_files/e45/339/a3e/e45339a3e31ef2ee3a20a07fb2141cd3.png'"['https://habrastorage.org/r/w780q1/getpro/habr/upload_files/7aa/f5e/951/7aaf5e951e37ffe7a473107ddd0090ba.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/dcf/29b/e8d/dcf29be8dc08066155f72aa86d094508.png', 'https://habrastorage.org/getpro/habr/company/2d5/0ed/b57/2d50edb57cf45fa07cc4f39f53b78395.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/5fa/4e3/365/5fa4e33650cf0166aff9247b8759db69.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/12a/a06/c24/12aa06c24eca43bb2b9ded941dec1ab8.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/avatars/b9f/baf/5f9/b9fbaf5f96ae52973706a0716bd9216e.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/0a0/54e/333/0a054e333d55220415ff7b7af4d0a8e1.png', 'https://habrastorage.org/r/w48/getpro/habr/avatars/b9f/baf/5f9/b9fbaf5f96ae52973706a0716bd9216e.jpg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/868/7d2/ee5/8687d2ee5f7c3d3a57b7fb26ccc8a7e8.png', 'https://habrastorage.org/getpro/habr/upload_files/e45/339/a3e/e45339a3e31ef2ee3a20a07fb2141cd3.png', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/630/bca/76a/630bca76abab8d74edb1c665ad777b4d.png']"
16'722890'Очередной проект для себя или что такое MPS'Здравствуйте, Хабровцы... Хабровичане... В общем, всем привет! В последнее время я помогал другу с его соц. сетью и столкнулся с кучей проблем, поняв что мыслю быстрее времени, но при этом отстаю от...'https://habr.com/ru/post/722890/'"Здравствуйте, Хабровцы... Хабровичане... В общем, всем привет! В последнее время я помогал другу с его соц. сетью и столкнулся с кучей проблем, поняв что мыслю быстрее времени, но при этом отстаю от него, ведь у меня уже давно расписаны проекты решающие все эти проблемы, но не реализованы... Дак вот, относительно недавно я наконец полностью доделал один из них и хочу узнать ваше мнение и получить пару советов по его улучшению.

О чём речь

My Programming System (MPS) — Это набор инструментов для обеспечения работы серверов на Linux и Windows, обрабатывающих большое количество разнообразных сетевых подключений. Вот список этих инструментов:

Interpretator — Интерпретатор собственного языка программирования, программа на котором будет обрабатывать запросы.

Language — Сам язык программирования, на котором будут писаться программы запускаемые интерпретатором.

Server — ПО для получения сетевых подключений, то есть просто для работы сервера.

Понимаю что звучит не очень, но лучше и достаточно кратко я написать, не смог. Так что, читайте дальше, более подробную и понятную версию.

Interpretator (часть 1)

Что интересно, данная идея началась с интерпретатора собственного языка программирования для удобного создания Web-сайтов, что-то типа PHP, но работающего всё время и потому не теряющего данные между подключениями, ещё в те времена когда я писал эти самые сайты на заказ... И эта идея сохранилась, но вместо Web-запросов должны обрабатываться локальные. Приведу пример:

Пользователь подключается по вашему адресу (например 123.123.123.123:80) ↓ Любое ПО (например MPSS) принимает его и отправляет запрос к интерпретатору ↓ Интерпретатор вызывает определённую функцию у работающей программы ↓ Программа делает что хочет и возвращает ответ

Как вы надеюсь поняли, интерпретатор и запущенная им программа (на моём языке программирования) работает в фоне, с момента запуска и до остановки вами. Для работы с ним есть консольная команда:

mpsi — Получить базовую информацию о команде mpsi compilation — Скомпилировать программу и классы в специальный архив mpsi info — Получить подробную информацию об интерпретаторе и программе mpsi request <данные запроса> — Выполнить запрос к программе mpsi start <аргументы для программы> — Запустить программу mpsi stop — Остановить программу

Окей, с консолью всё понятно, но где же найти код запускаемой программы? Всё просто, загляните в корневой каталог и найдите там папку mps, у неё будет примерно такая структура:

mps/ interpretator/ — Каталог для файлов интерпретатора program/ — Каталог для файлов программы *.mpsp — Файл с кодом программы *.mpsc — Файлы с кодом классов программы modules/ — Каталог для модулей *.mpsm — Модули для интерпретатора program.log — Файл с логом программы server/ — Каталог для файлов сервера data/ — Каталог для данных сервера config.json — Конфигурационный файл сервера

Ничего не понятно?) Что за классы? Что за сжатые данные? Что за логи? Что за конфиги? Чтобы ответить на эти вопросы, давайте перейдём к самому языку программирования!

Language

Структура программы проста, есть основной код (самой программы), а есть код классов, которые работают отдельно и у которых программа может вызывать функции (то есть программа + дополнительные программы-помощники). Что насчёт самого кода, у него очень простая необязательная структура: Препроцессорнные функции, объявление глобальных переменных, функции (а точнее методы, об этом см. дальше).

Давайте по порядку, есть всего 4 препроцессорнных функции, одна из которых недоступна в классах (вы сами поймёте почему):

#define <название> <значение> — Начать заменять все совпадения с ""названием"" на ""значение"" #import <путь к файлу> [имя] — Подключить класс и если указан принудительно установить имя (именно эта функция недоступна в классах) #setting <название> <значение> — Изменить настройки программы/класса (об этом позже) #undefine <название> — Прекратить замену совпадений

Сразу скажу, define и undefine я позаимствовал у любимой мною C-шки, так что думаю останавливаться на них нет смысла. По поводу import хочу сказать только что она поддерживает любые пути к файлам с кодами классов, хоть локальные (например ""my_super_class.mpsc"" либо ""/path/to/my_super_class.mpsc""), хоть web (например ""https://my-site.ru/my_super_class.mpsc""), хоть ftp (например ""ftp://root:my-password@my-ftp.ru/my_super_class.mpsc""). А вот о setting нужно рассказать подробнее, у программы и классов есть некоторые параметры, которые они могут изменять:

(string) NAME — Название программы/класса (по умолчанию название файла с кодом без расширения) (string) DESCRIPTION — Описание программы/класса (по умолчанию """") (string) VERSION — Версия программы/класса (по умолчанию ""1.0"") Дополнительно для классов: (boolean) STATIC — Статический ли класс (по умолчанию true)

Зачем они? Во первых они доступны как системные макросы (то есть те которые созданы заранее, но вы можете их переопределить как и любые другие (просто вызвав define снова с тем же названием)):

Доступно везде (и в программе, и в классах): MODULES -> Массив подключённых модулей (это массив массивов, содержащих название, описание и версию каждого модуля) PROGRAM_NAME -> Название программы PROGRAM_DESCRIPTION -> Описание программы PROGRAM_VERSION -> Версия программы SYSTEM_VERSION -> Версия интерпретатора Доступно лишь в программе: CLASSES -> Массив подключённых классов (это массив массивов, содержащих название, описание и версию каждого класса) Доступно лишь в классах: CLASS_NAME -> Название класса (того в котором замена) CLASS_DESCRIPTION -> Описание класса (того в котором замена) CLASS_VERSION -> Версия класса (того в котором замена) CLASS_STATIC -> Статичный ли класс (тот в котором замена)

Во вторых, это просто прикольно и можно где-нибудь использовать :) Теперь, прежде чем перейти к синтаксису языка, давайте разберём какие вообще есть типы данных и как с ними работать:

array — Массив данных ([...]) boolean — Логическое значение (true | false) char — Одиночный символ ('?') class — Экземпляр динамического класса (того у которого параметр STATIC равен false) double — Десятичное число (?.?) empty — Ничего (null) integer — Целое число (?) string — Строка (""..."")

Ну а использовать их можно как и в большинстве языков программирования, так что я просто приведу несколько примеров:

variable @example = [ ""строковой ключ"" : ""значение"", 65456 : ""← цифровой ключ, значение"" ]; // array @example = ExampleClass(); // class @example = null; // empty

Но вас наверняка интересует, динамическая или статическая типизация? Ответ, динамическая.

Таблица ""превращений"" array -> boolean: если нет элементов, то false, иначе true array -> char: 'a' array -> class: класс Array array -> double: если нет элементов, то -1.0, иначе 1.0 array -> empty: null array -> integer: если нет элементов, то -1, иначе 1 array -> string: ""array(<элементы массива через запятую>)"" boolean -> array: [<значение>] boolean -> char: 'b' boolean -> class: класс Boolean boolean -> double: если false, то -1.0, иначе 1.0 boolean -> empty: null boolean -> integer: если false, то -1, иначе 1 boolean -> string: ""boolean(<значение>)"" char -> array: [<значение>] char -> boolean: true char -> class: класс Char char -> double: 1.0 char -> empty: null char -> integer: 1 char -> string: ""char(<значение>)"" class -> array: [<значение>] class -> boolean: true class -> char: 'c' class -> double: 1.0 class -> empty: null class -> integer: 1 class -> string: ""class(<название класса>)"" double -> array: [<значение>] double -> boolean: если меньше 0.0, то false, иначе true double -> char: 'd' double -> class: класс Double double -> empty: null double -> integer: <целая часть значения> double -> string: ""double(<значение>)"" empty -> array: [] empty -> boolean: false empty -> char: 'e' empty -> class: класс Empty empty -> double: 0.0 empty -> integer: 0 empty -> string: ""null"" integer -> array: [<значение>] integer -> boolean: если меньше 0, то false, иначе true integer -> char: 'i' integer -> class: класс Integer integer -> double: <значение>.0 integer -> empty: null integer -> string: ""integer(<значение>)"" string -> array: [<символы строки отдельными элементами>] string -> boolean: если нет символов, то false, иначе true string -> char: 's' string -> class: класс String string -> double: если нет символов, то -1.0, иначе 1.0 string -> empty: null string -> integer: если нет символов, то -1, иначе 1

Ну и наконец, можно переходить к синтаксису!

#<строка>; — Вызов препроцессорной функции <строка>([аргументы]); ЛИБО <строка>.<строка>([аргументы]); ЛИБО @<строка>.<строка>([аргументы]); — Вызов метода //<строка>

— Однострочный комментарий /*<строка>*/ — Многострочный комментарий variable @<строка>; ЛИБО variable @<строка> = <значение>; — Объявление переменной method <строка>([аргументы]) {[тело]}; — Объявление метода if (<условие>) <тело> ЛИБО if (<условие>) {[тело]}; — Выполнить тело при условии elseif (<условие>) <тело> ЛИБО elseif (<условие>) {[тело]}; — Выполнить тело при условии и если предыдущий if не выполнился else <тело> ЛИБО else {[тело]}; — Выполнить тело при условии и если все предыдущие if и elseif не выполнились loop (<условие>) {[тело]}; ЛИБО loop (<условие>) <тело> ЛИБО {[тело]} loop (<условие>); ЛИБО <тело> loop (<условие>);— Выполнять код повторно пока условие верно return <строка>; ЛИБО return; — Вернуть значение либо null continue; — Пропустить выполнение оставшегося тела в цикле break; — Прекратить работу цикла static <строка>; — Сделать переменную либо метод статической (в программе: сохранить при остановке и загрузить при запуске, в классе: сделать доступной всем экземплярам) dynamic <строка>; — Сделать переменную либо метод динамической (в программе: ничего, в классе (недоступно в статическом классе): сделать индивидуальной для каждого экземпляра) public <строка>; — Сделать переменную либо метод доступным из вне protected <строка>; — Сделать переменную либо метод недоступным из вне, но доступным для системы private <строка>; — Сделать переменную либо метод недоступным из вне @<строка> = <строка>; — Присвоение значения переменной @<строка> += <строка>; — Прибавление числа к значению переменной и присвоение результата @<строка> -= <строка>; — Вычитание числа из значения переменной и присвоение результата @<строка> *= <строка>; — Умножение числа на значение переменной и присвоение результата @<строка> **= <строка>; — Возведение значения переменной в степень число и присвоение результата @<строка> /= <строка>; — Деление значения переменной на число и присвоение результата @<строка> \= <строка>; — Деление значения переменной на число и присвоение остатка (<строка> == <строка>) — Если объекты равны (<строка> === <строка>) — Если типы объектов равны (<строка> != <строка>) — Если объекты не равны (<строка> !== <строка>) — Если типы объектов не равны (<строка> > <строка>) — Если первое число больше второго (<строка> >= <строка>) — Если первое число больше либо равно второму (<строка> < <строка>) — Если первое число меньше второго (<строка> <= <строка>) — Если первое число меньше либо равно второму <условие> && <условие> — Условие верно если оба условия верны <условие> ^^ <условие> — Условие верно если верно либо первое условие, либо второе (но не оба) <условие> || <условие> — Условие верно если хоть одно из условий верно <число> + <число> — Сложение двух чисел <число> - <число> — Вычитание второго числа из первого <число> * <число> — Умножение чисел <число> ** <число> — Возведение первого числа в степень второе число <число> / <число> — Деление первого числа на второе <число> \ <число> — Деление первого числа на второй и получение остатка -<число> — Смена знака числа !<строка> — Поменять на противоположное логическое значение method <строка>([аргументы, ]array @<строка>...) — Все аргументы после объявленых аргументов поместить в массив @<строка> method <строка>([аргументы, ]<строка> @<строка> = <строка>[, аргументы]) — Если не указан аргумент то присвоить ему значение method <строка>([аргументы, ]@<строка>[, аргументы]) — Разрешить передавать аргумент любого типа method <строка>([аргументы, ]<строка> @<строка>[, аргументы]) — Разрешить передовать аргумент только определённого типа method <строка>([аргументы, ]<строка>|<строка> @<строка>[, аргументы]) — Разрешить передовать аргумент одного из нескольких типов

Ну и в завершение описания MPSL, нужно обязательно упомянуть системные классы и методы (то есть функции). Системные классы не надо подключать, они все (на данный момент) статичные и их можно использовать также как обычные.

Список системных классов Array — Класс для работы с массивами { exist(array @array, string | integer @key) — Проверить есть ли значение с ключём key в массиве array key(array @array, @value) — Получить ключ по значению value из массива array merge(array @arrays...) — Объеденить неограниченное количество массивов search(array @array, @value) — Проверить есть ли ключ со значением value в массиве array size(array @array) — Получить количество значений в массиве array sort(array @array, integer @type) — Отсортировать массив array } Char — Класс для работы с одиночными символами { convert(integer @type, char @char) — Конвертировать в другую кодировку символ get(integer @number) — Получить символ по номеру toLower(char @char) — Перевести символ в нижний регистр toUpper(char @char) — Перевести символ в верхний регистр } Class — Класс для работы с экземплярами классов { clone(class @class) — Создать новый экземпляр того же класса что и class destroy(class @class) — Уничтожить экземпляр класса class } Double — Класс для работы с десятичными числами { getDecimal(double @double) — Получить целую часть десятичного числа double getWhole(double @double) — Получить десятичную часть десятичного числа double max(double @double1, double @double2) — Получить наибольшее число из двух десятичных maxValue() — Получить максимальное возможное десятичное число min(double @double1, double @double2) — Получить наименьшее число из двух десятичных minValue() — Получить минимальное возможное десятичное число round(double @double) — Округлить десятичное число } Integer — Класс для работы с челыми числами { max(integer @integer1, integer @integer2) — Получить наибольшее число из двух целых maxValue() — Получить максимальное возможное целое число min(integer @integer1, integer @integer2) — Получить наименьшее число из двух целых minValue() — Получить минимальное возможное целое число } Language — Класс для работы с кодом на других языках программирования { execute(string @language, string @code) — Выполнить код на другом языке программирования exist(string @language) — Проверить можно ли выполнить код на таком языке программирования } Log — Класс для работы с логом { read(integer @lines) — Получить lines строк из лога, считая с низу size() — Получить количество строк в логе write(string @line) — Записать строку в лог } Network — Класс для работы с сетью { connect(integer @type, string @address) — Подключиться к серверу address createServer(integer @type, string @address, array @settings) — Создать сервер по адресу address с настройками settings (какие методы будут вызываться и когда, время ожидания ответа и т.д.) типа type destroyServer(integer @id) — Уничтожить сервер id disconnect(integer @id) — Отключиться от сервера id read(integer @id) — Получить входящие данные от сервера id write(integer @id, string @packet) — Отправить данные серверу id } Random — Класс для работы со случайными данными { boolean() — Получить случайное значение типа boolean char(string @chars = ""abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.,@#$_&-+()/*\""':;!?~|^={}\\%[]<>"") — Получить случайное значение типа char double(double @min = Double.minValue(), double @max = Double.maxValue()) — Получить случайное значение типа double integer(integer @min = Integer.minValue(), integer @max = Integer.maxValue()) — Получить случайное значение типа integer string(integer @length, string @chars = ""abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.,@#$_&-+()/*\""':;!?~|^={}\\%[]<>"") — Получить случайное значение типа string time(string @format = ""d.m.y s:M:h"", string @min = Time.minValue(@format), string @max = Time.maxValue(@format)) — Получить случайное значение типа time } Security — Класс для обеспечения безопасности чего-либо { decrypt(integer @type, string @string, string @key) — Дешифровать строку string с помощью ключа key алгоритмом type encrypt(integer @type, string @string, string @key) — Зашифровать строку string с помощью ключа key алгоритмом type hash(integer @type, string @string) — Хешировать строку string алгоритмом type } Storage — Класс для работы с хранилищами { copy(string @old_path, string @new_path) — Копировать каталог/файл в другое место createDir(string @path, integer @permissions) — Создать новый каталог createFile(string @path, integer @permissions) — Создать новый файл exist(string @path) — Проверить существует ли каталог/файл info(string @path) — Получить информацию о каталоге/файле isDir(string @path) — Проверить является ли каталогом isFile(string @path) — Проверить является ли файлом list(string @path) — Получить список файлов в каталоге lock(string @path) — Заблокировать для чтения и записи файл read(string @path) — Получить содержимое файла remove(string @path) — Удалить каталог/файл rename(string @path, string @name) — Переименовать каталог/файл unlock(string @path) — Разблокировать для чтения и записи файл write(string @path, string @content) — Записать строку в файл } String — Класс для работы со строками { convert(integer @type, string @string) — Конвертировать в другую кодировку строку toLower(string @string) — Перевести все символы в строке в нижний регистр toUpper(string @string) — Перевести все символы в строке в верхний регистр length(string @string) — Получить количество символов в строке search(string @string, string | char @search) — Проверить присутствует ли в строке текст size(string @string) — Получить количество байт в строке substring(string @string, integer @start_position = 0, integer @end_position = String.length(@string) - 1) — Получить содержимое строки со start_position до end_position replace(string @string, string @search, string @value) — Заменить все совпадения в строке } System — Класс для работы с интерпретатором, программой и классами { consoleExecute(string @command) — Выполнить команду в консоли getLocale() — Получить системный язык getOS() — Получить название и версию текущей ОС import(string @path) — Подключить класс loadModule(string @path) — Загрузить модуль stop() — Остановить работу программы } Thread — Класс для работы с потоками { create(string @method) — Создать отдельный поток, который выполнит метод method destroy(integer @id) — Уничтожить поток id join(integer @id) — Дождаться выполнения потока id list() — Получить список существующих потоков и их id start(integer @id) — Запустить поток id } Time — Класс для работы с датами и временем { get(string @format = ""d.m.y s:M:h"") — Получить текущее время в определённом формате getTimeZone() — Получить текущую временную зону convert(string @old_format, string @new_format, string @time) — конвертировать дату из одного формата в другой max(string @format, string @time1, string @time2) — Получить наибольшее время из двух предоставленных в одном формате maxValue(string @format = ""d.m.y s:M:h"") — Получить максимальное возможное время в определённом формате min(string @format, string @time1, string @time2) — Получить наименьшее время из двух предоставленных в одном формате minValue(string @format = ""d.m.y s:M:h"") — Получить минимальное возможное время в определённом формате }

Ну а системные методы, это четыре для программы и два для классов метода, которые обязательно должны присутствовать и которые СОВЕТУЕТСЯ НЕ ВЫЗЫВАТЬ ГДЕ-ЛИБО В КОДЕ:

Для программы: protected dynamic method Error(integer @code, array @arguments) — Вызывается при ошибке во время работы программы (code- код ошибки, arguments- аргументы ошибки (строка, символ, сообщения и т.д.)) protected dynamic method Request(integer @type, array @arguments) — Вызывается при запросе к программе (type- тип запроса (внешний/системный), arguments- аргументы запроса)) protected dynamic method Start(array @arguments) — Вызывается при запуске программы (arguments- аргументы переданные при запуске) protected dynamic method Stop() — Вызывается при остановке программы Для классов: protected dynamic method Constructor(array @arguments = []) — Вызывается если статический класс, то при запуске программы, иначе при создании экземпляра (arguments- аргументы запуска/переданные при создании экземпляра) protected dynamic method Destructor() — Вызывается если статический класс, то при остановке программы, иначе при уничтожении экземпляр

Interpretator (часть 2)

Прежде чем начать читать Вообще, я изначально не хотел делить раздел Interpretator на две части, но так как алгоритм интерпретации кода на моём языке программирования будет не понятен без знания основ самого языка, мне пришлось сделать эту ужасную вещь :)

Раз вы теперь знаете как работают программа и классы на моём языке программирования, то можно узнать что происходит во время запуска, то есть, во время интерпретации. Как ни странно, всё начинается с... запуска интерпретатора! Он обрабатывает введённые аргументы и если ему сказано запустить программу проверяет не запущена ли уже она, если да, то выдаёт ошибку, если нет, то ""заглядывает"" в каталог /mps/interpretator/program/ и ищет там файл с расширением .mpsp, если находит несколько, то выдаёт ошибку, если не находит вообще, то начинает искать файл .mpsa, о котором мы поговорим позже, ну и если всё ок, то получает содержимое (если не получилось либо в нём ничего нет, то выдаёт ошибку :) и передаёт его препроцессору, начиная тем самым интерпретацию:

Препроцессор получил данные и начинает их обработку, которая делится на два этапа:

Удаляются все комментарии и выполняются все препроцессорные функции ПОЛНОСТЬЮ ИГНОРИРУЯ КОД. Код дополняется где нужно, приводя его в единый вид (например ""if (...) ...;"" будет заменено на ""if (...) {...};""), но уже ничего не игнорируя.

Что на что меняется Для программы: variable...; -> public dynamic variable...; method...; -> public dynamic method...; static variable...; -> public static variable...; static method...; -> public static method...; dynamic variable...; -> public dynamic variable...; dynamic method...; -> public dynamic method...; public variable...; -> public dynamic variable...; public method...; -> public dynamic method...; protected variable...; -> protected dynamic variable...; protected method...; -> protected dynamic method...; private variable...; -> private dynamic variable...; private method...; -> private dynamic method...; return; -> return null; else if... -> elseif... if (...) ...; -> if (...) {...}; elseif (...) ...; -> elseif (...) {...}; else ...; -> else {...}; loop (...) ...; -> loop (...) {...}; ... loop (...); -> {...} loop (...); Для классов: variable...; -> public static variable...; method...; -> public static method...; static variable...; -> public static variable...; static method...; -> public static method...; dynamic variable...; -> public dynamic variable...; dynamic method...; -> public dynamic method...; public variable...; -> public static variable...; public method...; -> public static method...; protected variable...; -> protected static variable...; protected method...; -> protected static method...; private variable...; -> private static variable...; private method...; -> private static method...; return; -> return null; else if... -> elseif... if (...) ...; -> if (...) {...}; elseif (...) ...; -> elseif (...) {...}; else ...; -> else {...}; loop (...) ...; -> loop (...) {...}; ... loop (...); -> {...} loop (...); Тажке вставляются пробелы где нужно, удаляются переносы строк, линие табы и лишние пробелы, добавляются return; в методы где они отсутствуют.

Дальше в дело вступает анализатор, он проверяет весь код и если где-то что-то не так (например не хватает точки с запятой) выдаёт ошибку. Потом, отформатированные и проверенный код передаётся лексеру, который разбивает его на токены и передаёт оптимизатору, который как понятно из названия оптимизирует их как может. Потом, уже оптимизированные токены получает парсер, он составляет из них специальные структуры (или если хотите таблицы) и передаёт их воркеру, который в свою очередь сохраняет их где нужно и вызывает метод Start с полученными в самом начале аргументами, а, и не подумайте что классы не интерпретируются, они проходят такую же цепочку. Помните я упоминал какие-то файлы с расширением .mpsa? Это архивы, в которые можно сохранить специальные структуры, создаваемые парсером. Если указать аргументом compilation вместо start, произойдёт то же самое (без поиска файлов .mpsa), но парсер не отдаст сгенерированные структуры воркеру, а сохранит их в файл, чтобы можно было при частом запуске экономить время.

Server

Заранее говорю, не обязательно использовать MPSS для получения запросов из сети (вы можете использовать любое другое ПО, которое сможет это сделать, либо просто создавать сервер прямо в программе, используя системный класс Network), просто так будет удобнее, проще и надёжнее. Давайте начнём с конфигурационного файла, вот его примерная структура:

{ ""<название сервера>"": { ""address"": ""<адрес этого сервера>"", ""port"": <порт этого сервера>, ""type"": ""<тип этого сервера (пока что поддерживаются: web, ftp, ssh)>"", ""settings"": { // Этот объект имеет ещё индивидуальные значения для каждого типа серверов ""mpsi"": true } } }

Вопросов стало только больше? Это хорошо, в ""самом верхнем"" объекте хранятся объекты с информацией о серверах, которые должен создавать MPSS. Например такой конфиг:

{ ""my_web_server"": { ""address"": ""127.0.0.1"", ""port"": 80, ""type"": ""web"", ""settings"": { ""mpsi"": false, ""php"": true, ""perl"": true } }, ""my_ssh_server"": { ""address"": ""127.0.0.1"", ""port"": 1234, ""type"": ""ssh"", ""settings"": { ""mpsi"": true } } }

Создаст web-сервер на порту 80, поддерживающий PHP и Perl скрипты, и ssh-сервер запросы к которому будет обрабатывать MPSI. Вы спросите, где же данные для web-сервера? А они хранятся в каталоге /mps/server/data/<название сервера>/ или же в нашем случае в /mps/server/data/my_web_server/. Но если с ним всё вроде бы понятно, со вторым есть пара интересных моментов, и чтобы их объяснить давайте попробуем подключиться к нему (для этого я буду использовать обычный SSH-клиент на Linux из консоли). Что по вашему произойдёт на ""стороне сервера"" после ввода этих команд:

ssh root@127.0.0.1 -p 1234

Думаете MPSS запросит пароль? Или вообще не будет запрашивать? Нет, он отправит запрос MPSI, что будет идентично такому коду на MPSL:

Request(1, [ ""header"" : ""MPSS SSH-SERVER"", ""type"" : 0, // Тип: авторизация ""data"" : ""root:localhost"" // Имя пользователя, от кого ]);

Если он вернёт false либо -1, то клиента пошлют нафиг клиенту просто откажут в доступе, если true либо 0, то ему скажут что всё окей, а если 1, то запросят пароль. Предположим что в нашем случае программа вернула 0 и я (как клиент) получил это:

root@127.0.0.1's password:

И я введу например ""my-super-mega-password"", то MPSS опять сделает запрос к MPSI:

Request(1, [ ""header"" : ""MPSS SSH-SERVER"", ""type"" : 1, // Тип: подтверждение пароля ""data"" : ""my-super-mega-password"" // Полученный от клиента пароль ]);

Теперь, если вернут false либо -1, то мне скажут что пароль неверный, а если true либо 1, то скажут что всё окей. И давайте предположим что программа вернула 1 и мне ""пустили на сервер"", а потому сразу введём какую-нибудь команду (:

rm -rf /

Думаете MPSS её выполнит? Или сделает запрос к нашей программе? Если вы подумали второе, то вы уже начинаете понимать, ведь это правда и будет выглядеть вот так:

Request(1, [ ""header"" : ""MPSS SSH-SERVER"", ""type"" : 2, // Тип: выполнение команды ""data"" : ""rm -rf /"" // Сама команда ]);

В этот раз возврат должен быть немного другим, если false либо -1, то нас просто отключат, если же строка, то её вернут как ответ. Только для примера давайте нам вернут это:

Дорогой ssh-клиент, иди ка отсюда с такими командами! С уважением, программа на MPSL.

Надеюсь с этим всё понятно, так что давайте посмотрим на команду:

mpss — Получить базовую информацию о команде mpss info <название сервера> — Получить подробную информацию о сервере mpss start <название сервера> — Запустить сервер mpss stop <название сервера> — Остановить сервер

Ну что-ж, думаю пояснять смысл аргументов команды не стоит, да и про файловое устройство рассказывать не надо (ведь в разделе ""Interpretator (часть 1)"" я уже сказал про него всё важное), а так как статья и так получилась слишком большая, будем уже закругляться.

Дополнительно

Расширения файлов и их значение:

.mpsp — Код программы на MPSL .mpsc — Код класса на MPSL .mpsa — Архив со специальными структурами программы и классов на MPSL .mpsm — Код модуля для MPSI

Подробная документация по всему этому когда-нибудь появится.

Заключение

Очень надеюсь что статья получилась понятной и что я ничего не забыл написать, ну и под конец хочу повторить, что очень надеюсь на то что вы в комментариях напишете своё мнение, чтобы я смог улучшить MPS и потом со спокойной совестью выпустить в свет :) Также хочу сказать, что если вам понравилась эта статья, я могу рассказать о других проектах, которые ещё не реализованы либо реализованы не до конца."'https://habr.com/share/publication/722890/b779ade9d9bd0e68ec47c3ca87a6d768/'"['https://mc.yandex.ru/watch/24049213', 'https://habr.com/share/publication/722890/b779ade9d9bd0e68ec47c3ca87a6d768/']"
17'722852'Книга «Kafka Streams и ksqlDB: данные в реальном времени»'Привет, Хаброжители! Работа с неограниченными и быстрыми потоками данных всегда была сложной задачей. Но Kafka Streams и ksqlDB позволяют легко и просто создавать приложения потоковой обработки. Из...'https://habr.com/ru/post/722852/'"Эта книга адресована специалистам по обработке данных, желающим научиться создавать масштабируемые приложения потоковой обработки для перемещения и преобразования больших объемов данных в режиме реального времени. Подобные умения часто необходимы для поддержки интеллектуальной обработки данных, аналитических конвейеров, обнаружения угроз, обработки событий и многого другого. Специалисты по данным и аналитики, занимающиеся анализом потоков данных в реальном режиме времени и желающие усовершенствовать свои навыки, тоже смогут почерпнуть немало полезного из этой книги. В ней автору удалось отойти от привычной пакетной обработки, которая обычно доминировала в этих областях. Предварительный опыт работы с Apache Kafka не требуется, хотя некоторое знакомство с языком программирования Java облегчит знакомство с Kafka Streams.

"'https://habr.com/share/publication/722852/6767acb6d954660500c5bc52486322f4/'"['https://habrastorage.org/r/w1560/webt/we/l1/fq/wel1fqwa4oqzzz7kemwbom7sjog.png', 'https://habrastorage.org/r/w1560/webt/d8/ie/zz/d8iezzdyikc3cirswtjkdbzjiiu.png', 'https://habr.com/share/publication/722852/6767acb6d954660500c5bc52486322f4/', 'https://habrastorage.org/r/w48/getpro/habr/avatars/3be/b11/2b0/3beb112b076924a916b27c845d88dcfa.jpg', 'https://habrastorage.org/r/w780q1/webt/tp/qb/ia/tpqbiahkevbp4edc3f6swdgoi9s.jpeg', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w1560/webt/hq/s3/py/hqs3pyko19l5rc2hsp-lzbersc0.png', 'https://habrastorage.org/r/w1560/webt/7l/cg/pv/7lcgpv2fxwkftg3zci8aewnyitw.png', 'https://habrastorage.org/getpro/habr/avatars/3be/b11/2b0/3beb112b076924a916b27c845d88dcfa.jpg', 'https://habrastorage.org/r/w1560/webt/aw/ti/se/awtisev8cyhhurlr5g4zq_octs0.png', 'https://habrastorage.org/getpro/habr/company/e4d/50a/630/e4d50a630923dbf6e6b786d26b9da6d7.png', 'https://habrastorage.org/r/w1560/webt/s4/v2/sm/s4v2smndmn8vtphc6-bvhwh1tkc.png']"
18'722882'Дублирующий скрипт: как с его помощью мы ускорили бизнес-процесс с двух рабочих дней до семи минут'Как известно, при создании промышленного процесса, в котором регламентирован каждый шаг, все участвующие подразделения стараются максимально облегчить выполнение своей части работы. Поэтому часто...'https://habr.com/ru/post/722882/'"Как известно, при создании промышленного процесса, в котором регламентирован каждый шаг, все участвующие подразделения стараются максимально облегчить выполнение своей части работы. Поэтому часто применяются упрощения, которые не позволяют учесть все нюансы процесса, отслеживаемые в ручном режиме каждым аналитиком. По сути, перед автоматизаторами стоит задача охватить наибольшее число вариаций и при этом не усложнить процесс так, чтобы с ним было невозможно работать. Под усложнениями понимаются различные блокирующие процесс проверки, многочисленные итерации согласований по той или иной задаче, формы дополнительного ручного ввода данных и т.п.

В итоге формируются упрощенные требования, которые не позволяют в полной мере реализовать контроль как над ручными ошибками пользователей, так и над ошибками, допущенными при разработке требований и алгоритмов автоматизируемого процесса.

Вас приветствуют Гевонд Асадян и Илья Мясников. В банке «Открытие» в управлении риск-технологий мы занимаемся внедрением моделей оценки кредитного риска. В этой статье на примере большого и сложного процесса выдачи экспресс-кредитов мы расскажем, как нам удалось реализовать полноценный дубль процесса на стороне одного проверочного скрипта и ускорить процесс выдачи экспресс-кредитов с двух рабочих дней до семи минут.

История болезни

Коротко о скоринге

При выдаче кредитов зачастую используется скоринговая модель, позволяющая отделить высоко рискованных клиентов от клиентов с приемлемым уровнем риска. Подобная модель позволяет проанализировать множество различных факторов за короткий промежуток времени и вынести вердикт об уровне риска в случае, если банк выдаст кредит.

Скоринговая модель — это настроенный алгоритм, присваивающий клиенту определенное количество баллов (скоров) на основе статистических методов. Общая балльная оценка (скор) используется для отнесения клиента к той или иной группе риска. В основу скоринга могут быть заложены различные статистические или экспертные модели.

Для анализа корпоративных клиентов была разработана многомодульная скоринговая модель. Каждый из модулей анализирует определенное направление деятельности клиента:

Каждый блок содержит в себе определенное количество факторов, по которым производится скоринг клиента. В результате по совокупной балльной оценке принимается решение, возможна ли выдача кредитного продукта клиенту или нет.

Расчет факторов — дело достаточно трудозатратное и ресурсоемкое: например, для оценки кредитной истории клиента необходимо направить платный запрос в бюро кредитных историй. Чтобы попусту не тратить финансовые ресурсы банка, входящий поток заявок ограничивается определенными критериями, они называются стоп-факторами. Причем стоп-факторы могут быть реализованы как до скоринга, так и в процессе скоринга, чтобы ограничить число этапов.

Внешние и внутренние данные

Итак, благодаря безграничной фантазии и бесценному опыту коллег из бизнес-подразделений, подразделение рисков создает перечень критериев, которые отсекают заявки и ограничивают выдачу кредитов. Под эти критерии заложены сложные алгоритмы. Любой алгоритм строится согласно оговоренной методике (правилам), которые фиксируются в определении стоп-фактора или фактора. Тут и начинается история болезни, о которой мы хотим рассказать.

Модель охватывает большое число направлений для анализа. Естественно, банк не может себе позволить ручное заполнение факторов по нескольким тысячам входящих ежедневно заявок. Для этого используются алгоритмы, которые подтягивают накопленные внутри банка данные, а также данные, которые банк получает из внешних источников.

Внутренние данные аккумулируются у отдельных подразделений, каждое из которых хранит их в удобном для своего собственного использования виде. Как правило, внутренние данные хранятся разрозненно, для их систематизации создаются системы хранения данных, в которых решается задача реализации связей между первоисточниками для дальнейшей аналитики.

Внешние данные чаще всего банк закупает у сторонних организаций через специальные API-сервисы. Тут возможны два пути: получение данных напрямую через API либо выгрузка в единое хранилище данных банка через API и дальнейшее их использование через так называемые зеркала внешних баз.

При разработке алгоритмов, по которым внутренние или внешние данные из первоисточников наполняют системы хранения данных, очень важно предметно разбираться в том, какие это данные. Какую специфику они содержат, какие могут быть ограничения по этим данным, как работать с исключениями. Если разработчик алгоритма не знает всех нюансов или они не четко регламентированы в техническом задании, возникает высокий риск ошибок при загрузке и обработке этих данных. Помимо ошибок в алгоритме загрузки данных могут возникать сбои в самом сервисе API. И по тем или иным причинам данные могут не загрузиться вовсе.

Таким образом, возникает первая проблема, которую необходимо решать при расчете факторов для скоринговой модели: проблема корректности и полноты поступивших в расчет фактора данных.

Реализация расчета факторов

Предположим, что все данные получены и загружены в системы, полнота и корректность проверена. Теперь стоит задача рассчитать факторы для скоринговой модели. Перед аналитиком возникает следующая проблема, которую необходимо решить: реализация алгоритма расчета фактора, учитывающая максимальное число исключений из общих правил.

Самый простой пример, который покажет эту проблему на практике — реализация расчета фактора «Общий долг/EBITDA». Данный фактор является распространенной метрикой для оценки долговой нагрузки клиента на основе его финансовой отчетности и широко используется финансовыми аналитиками. Существуют разные способы оценки как общего долга клиента, так и его EBITDA.

Именно поэтому важно расписать методику, по которой необходимо произвести расчет фактора в скоринговой модели, чтобы учесть все составляющие числителя и знаменателя. Отдельно необходимо учесть такие исключения, как возможность равенства нулю знаменателя и описать для автоматизаторов, как поступать в таких случаях.

В скоринговой модели экспресс-кредитования помимо реализации таких простых факторов существует потребность расчета сложных факторов, включающих в себя большие объемы данных, например, транзакции клиентов банка за последние 2-3 года. При расчете таких факторов возникает еще одна проблема — проблема производительности при расчете факторов. Банк не может позволить себе многочасовой расчет факторов по одному клиенту. Поэтому при расчетах важно пользоваться оптимизированными алгоритмами на системах, позволяющих производить расчеты с большими объемами данных.

В процессе реализации расчета факторов разработчик может допустить ошибки, связанные с описанными выше проблемами. Поэтому очень важно:

Прописывать методику расчета факторов так, чтобы она трактовалась однозначно;

Продумать методику тестирования и валидации разработанных скриптов, чтобы покрыть наибольшее число кейсов.

Теперь — подробнее о том, как мы решали все эти проблемы.

Лечение болезни

Многомодульная модель — настолько сложный и многоэтапный процесс, что сбой может возникнуть на любом этапе. Поэтому наиболее эффективный способ контроля процесса выдач в онлайн режиме — разработка дублирующего проверочного скрипта, который будет независимо повторять все шаги за промышленной системой и сигнализировать в нужный момент о проблемах, останавливая процедуру выдачи кредитных продуктов по заявкам.

Архитектура промышленного процесса

Для того, чтобы повторить процесс выдачи экспресс-кредитов, прежде всего необходимо понимать архитектуру промышленного решения. В нашем случае мы разбирались с уже реализованным в кредитном конвейере процессом, однако в идеальной картине мира необходимо все эти процедуры продумывать заранее. На рисунке 1 приведена схематичная архитектура промышленного бизнес-процесса, которую мы попробовали повторить.

Рисунок 1: архитектура процесса скоринга для выдачи экспресс-кредитов

С целью проверки каждого этапа был разработан дублирующий проверочный скрипт, который запускался после этапа принятия решения, но до выдачи. Таким образом процесс, иллюстрированный на рисунке 1, был доработан следующим образом:

Рисунок 2: доработанный процесс скоринга для выдачи экспресс-кредитов

Технология реализации проверочного скрипта

Проверочный скрипт реализован на платформе Mlops в виде развернутого rest-сервиса, работающего по принципу API (запрос-ответ). Для этих целей развернут Docker с интеграцией между кредитным конвейером и внутренними базами данных. Подобный подход позволяет без дополнительных интеграций осуществлять обмен данными между системами (см. рисунок 3).

Рисунок 3: реализация дублирующего проверочного скрипта

Рассмотрим каждый этап по отдельности, с выдержками кода, примерами и описаниями решений.

Формирование входных данных

Любой расчет начинается с данных, которые необходимо подать на вход. При этом для целей дублирующего проверочного скрипта важно сохранить данные в первозданном виде и настроить независимую обработку. Поскольку в процессе кредитования экспресс-продуктов используются большие массивы данных, наиболее удобным способом для передачи такого массива является JSON. В нем зашифрованы данные в том виде, в котором приходят от внешних сервисов.

Входящий JSON, состоящий из разных модулей данных, целесообразно формировать в следующей структуре:

RqUID — идентификатор файла для расчета;

DateTime — дата и время формирования файла для расчета;

app — информация по заявке: Набор атрибутов для расчета по заявке; spark_data — xml СПАРК; pravo_data — список словарей с данными ПРАВО.ру; bki_data — список словарей с данными бюро кредитных историй.

loan_member_express — список словарей с данными по клиенту и связанным лицам из кредитного конвейера для сравнения.

Также с целью упрощения передачи данных реализовано хранение пользовательских справочников в виде JSON, в которых содержится информация следующего рода:

Списки отраслей с экспертными оценками; Перечень типов входных данных для проверки ошибок; Справочник с кодами факторов и стоп-факторов.

Кредитный конвейер по маршруту заявки генерирует запросы во внешние системы и записывает полученную информацию в JSON-файл, затем инициирует запрос к REST-API, который, в свою очередь, ссылается на нужные справочники и производит расчет скоринга и лимитов.

Технология реализации проверочного скрипта

Проверочный скрипт реализован в классовой логике: это означает, что каждый модуль процесса выделен в отдельный класс, вызываемый на основе установленного пайплайна.

Класс DataLoader

В данном классе производится загрузка необходимых для расчета справочников. При инициализации в классе задаются пути к справочникам:

class DataLoader: def __init__(self, industry_classifier_path, stop_industry_list_path, types_path, activity_type_path, gz_stops_path, gz_ec_map_path ): # Инициализация переменных класса, содержащих пути к файлам, содержащим таблицы и словари с параметрами модели self.stop_industry_list_path = stop_industry_list_path #20220524 self.industry_classifier = industry_classifier_path self.types_path = types_path self.activity_type_path = activity_type_path self.gz_stops_path = gz_stops_path self.gz_ec_map_path = gz_ec_map_path

Далее в классе поочередно реализованы функции по загрузке каждого из необходимых справочников, например, функция для загрузки справочника стоп-отраслей:

def load_industry_classifier(self): """""" Загрузка стоп-отраслей ОКВЭД """""" df_bank_okved = pd.read_excel(self.industry_classifier, engine='openpyxl', sheet_name = 'ОКВЭД', usecols = ['КОД', 'Отрасль', 'МБ'], skiprows=1, converters = {'КОД':str}) df_bank_okved['МБ'] = df_bank_okved['МБ'].where(df_bank_okved['МБ'].isnull() == False, '1') df_bank_okved['МБ'] = df_bank_okved['МБ'].apply(lambda x: (x.strip()).capitalize()) df_bank_okved = df_bank_okved[df_bank_okved['МБ']!='1'] return df_bank_okved

Класс CheckInputData

Важным этапом при реализации дублирующего скрипта является проверка входных данных на полноту и корректность. Для этого реализован класс, позволяющий провести проверку входящих данных на целостность, полноту и корректность по форматам. При инициализации класса задается датафрейм, полученный нормализацией входного json и датафрейм-справочник типов входных данных.

class CheckInputData: def __init__(self, df_root, df_types): self.df_types = df_types self.df_root = df_root self.df = None self.RqUID = None self.DateTime = None

Далее в классе реализована функция, которая позволяет производить логирование ошибок во входных данных. В случае, если приходят не такие данные, какие необходимы, скрипт выдает сообщение об ошибке и формирует технический отказ по заявке до момента исправления ошибки и повторного расчета.

def log_errors(self): root_columns = list(self.df_root) if 'RqUID' not in root_columns: result_calculation = json.dumps({'RqUID': None, 'DateTime': str(datetime.utcnow() + timedelta(hours=3)), 'app': [{ 'APP_ID': None, 'Code': ""402"", 'Status': ""Error"", 'Description': ""Несоответствие перечня загруженных полей по модулю модели (отсутствует RqUID)""}]}) result_json = json.loads(result_calculation) return result_json self.RqUID = self.df_root['RqUID'][0]

Пример ошибки, по которой проводится логирование: ""Несоответствие перечня загруженных полей по модулю модели — отсутствует RqUID"".

Класс ExtractData

После загрузки и проверки входных данных на корректность ставится задача по их парсингу. При инициализации задается датафрейм, полученный нормализацией входного json, а также датафрейм-справочник отраслей:

class ExtractData: def __init__(self, df, industry_classifier):

Далее создаются основополагающие датафреймы, с которыми потом работаем в скрипте:

apps_df — датафрейм с информацией по заявке;

df_lm — часть входного json, соответствующая участникам сделки.

Поскольку существует множество разных источников с различной методикой формирования входных данных, в классе реализована функция, которая приводит пустые значения во входящих данных к единообразному виду:

def check_missing(self, field): if isinstance(field, str): #добавил проверку на строку, иначе падал при попытке привести float в нижний регистр if field.lower() in ('nat', 'null', 'nan'): field = np.nan return field

Функция заменяет ""строковые пропущенные значения"" вида ‘nat', 'null', 'nan' на np.nan.

Далее реализована функция def init_extractor(self, df, industry_classifier), которая осуществляет первоначальную загрузку данных для дальнейшего расчета факторов. В этой функции поочередно загружаются данные из входящего JSON для каждого модуля с предварительной обработкой пропущенных значений.

После загрузки и первичной обработки данных производится парсинг данных БКИ, СПАРК, ПРАВО.ру и др. Парсинг данных для каждого источника реализован как отдельная функция.

Парсинг данных БКИ включает в себя анализ платежной строки по полученным от сервиса CREA данным. Первоначально фильтруются нужные договоры клиента по условиям, заложенным в методологии на основе типа договора и срока действия, затем в каждой платежной строке производится поиск нужной информации, например, просроченной на 120+ дней задолженности, который обозначен символом «5».

Анализ осуществляется, если у клиента есть кредитная история. В противном случае формируются пустые датафреймы. Пример парсинга платежной строки приведен ниже:

# Удаляем символ подчеркивания в начале платежной строки self.bki_df['PMTSTRING84M'] = self.bki_df['PMTSTRING84M'].\ apply(lambda x : str(x[1:]) if str(x) != 'nan' and '_' in x else x) # 2.2) отбрасываются договора по которым неизвестна платежная строка (пустая) self.bki_df = self.bki_df[(self.bki_df['PMTSTRING84M'].notnull())&\ (~self.bki_df['PMTSTRING84M'].str.lower().isin(self.missing_list))]

Парсинг данных СПАРК производится в два этапа — сначала определяется список связанных лиц, а затем производится загрузка информации, содержавшейся в методах СПАРК. Для анализа используется информация, содержащаяся в следующих методах API-СПАРК:

GetCompanyExtendedReport;

GetCompanyStructure;

ManagementCompanyINN;

GetCompanyListByPersonINN.

Данные по связанным лицам сохраняются в виде списка списков следующего формата: [ИНН, наименование, ИНН связанного лица на уровень ближе к заемщику, доля владения, роль связанного лица, OKATO, наименование страны].

После того как сформирован список связанных лиц, рассчитываются стоп-факторы СПАРК, которые включают в себя такие критерии, как динамика выручки, наличие сообщений о ликвидациях и банкротствах, проверка статуса компании и срока деятельности, информация о руководителях и об их смене.

Пример реализации парсинга данных-XML для извлечения OKOPF приведен ниже (остальные данные извлекаются аналогичным способом на основе описания каждого метода, изложенного в спецификации API-СПАРК).

if 'GetCompanyExtendedReport' in self.all_spark[inn].keys(): xml_root = self.all_spark[inn]['GetCompanyExtendedReport'] if not (xml_root.find('Data/Report/OKOPF') is None): okopf_dict = xml_root.find('Data/Report/OKOPF') okopf = okopf_dict.get('CodeNew')

Обработка данных ПРАВО.ру производится похожим методом с той лишь разницей, что необходимо обработать JSON вида {«КЛЮЧ»: «ЗНАЧЕНИЕ»: «»}:

def pravo_json_parse(self): if not self.root: return self.pravo_df for page in self.root: try: total_json = page['page'] except KeyError: page_json = -1

Независимая от промышленной системы реализация парсинга данных из первоисточников позволяет исключить эффект ""замыливания глаз"" разработчиков. А дальнейшее тестирование потока заявок в автоматическом режиме позволяет находить расхождения и в результате анализа дорабатывать скрипты, чтобы покрыть выявленный кейс.

Класс FeatureEngineering

В данном классе реализован расчет факторов и стоп-факторов для каждого модуля модели. По соображениям конфиденциальности мы не имеем права раскрывать состав факторов модели, но можем поделиться технологией, по которой такой расчет производится.

В данном классе при инициализации загружаются датафреймы с распарсенными в классе FeatureExtract данными и производятся манипуляции, которые не выходят за рамки применения основных библиотек python для работы с датафреймами (pandas, datetime, numpy): def __init__(self, df_spark, df_apps, df_bki, df_pravo).

Разработка функций для расчета каждого фактора была осуществлена по следующей схеме:

Важным этапом при расчете факторов является тестирование результатов путем сверки полученных значений со значениями в выборке для разработки модели. Также важным является тестирование расчета фактора по сгенерированным искусственно кейсам, которые содержат экстремальные значения. Реализация в виде REST-API дает возможность прогонять такие кейсы пакетно, что недоступно в кредитном конвейере, а значит позволяет решать проблему покрытия максимального количества кейсов.

Класс StopsProcess

В данном модуле производится проверка наличия стоп-факторов и проставляется отказ по заявке в случае их выявления. При инициализации задаются датафреймы с построенными в FeatureEngineering атрибутами. Используются данные по заявке, СПАРК, ЧС, БКИ, ПРАВО, транзакции и др.

Отказы по каждому модулю формируются в отдельной функции, которая проверяет датафрейм с расчетом своих стоп-факторов и при превышении пороговых значений проставляет отказ по сделке. Например, реализация по модулю ПРАВО.ру выглядит следующим образом:

def stops_from_pravo(self): self.stops_pravo = self.pravo self.stops_pravo['PRAVO_CntOpCaseBankrupt36m'] = int(self.pravo['PRAVO_CntOpCaseBankrupt36m'].sum()>0) comment = 'Наличие арбитражных дел, открытых на дату заявки, зарегистрированных на горизонте 36 мес. до даты заявки о банкротстве' if len(self.df_apps['client_inn'][0]) == 12: self.pravo_stops_comments = self.pravo_stops_comments + [comment] if self.pravo['PRAVO_CntOpCaseBankrupt36m'].sum()>0 else self.pravo_stops_comments return self.stops_pravo.copy()

На выходе получаем либо флаг отсутствия стоп-факторов, либо формируем выходной json с информацией об отказе по тому или иному фактору и переходим к проверке наличия расхождений с данными основного процесса.

Класс CalculateScore

В данном классе осуществляется расчет скорингового балла по клиенту. Рассчитываются флаги наличия данных в источниках.

Общая схема расчета скорингового балла выглядит одинаково:

Выгружаются значения нескольких факторов;

Задаются коэффициенты линейной модели и значения для трансформации в WOE;

Факторы бинаризируются (строковые в соответствии со списком, числовые атрибуты — по промежуткам) и проставляется значение WOE;

Скор по модулю получается линейной комбинацией значений WOE и 1.

Класс CalculateLimit

В данном классе производится расчет лимита. Аналогично классу FeatureEngineering осуществляется расчет факторов для расчета лимита: в каждую отдельную функцию выделен отдельный фактор. Затем каждая функция вызывается в итоговой функции расчета лимита.

Результаты и отчет о расхождениях

По итогам прохождения пайплайна формируются две сущности, с которыми в дальнейшем работает аналитик: отчет с результатом расчета и отчет с расхождениями по сравнению с основным процессом.

Для этих целей выделено три класса:

MakeReport — класс для построения отчета по итогам расчета;

MakeDiffList — класс формирования отчета о расхождениях;

ModelESDSWrapper — класс формирования выходного json, содержащего в себе два отчета, указанных выше.

При инициализации класса MakeRport задаются датафреймы с данными по заявке, все данные из внешних источников и трансакциям, а также скорам, стопам, и лимитам. Итогом работы класса является JSON со структурой: {«report»: [{«app_id»: НОМЕР; «client_name»: НАИМЕНОВАНИЕ; «Score»: СКОР; и т.д.}], «conn_participants»: [……]} Отчет содержит результаты по каждому модулю модели.

При инициализации класса MakeDiffList задаются первоначальный входной датафрейм, все данные из внешних источников и транзакциям, а также скорам, стопам и лимитам. Передается словарь с кодами причин отказа для стопов. Затем формируется датафрейм, содержащий код фактора, первоначальное значение из кредитного конвейера и значение, рассчитанное в дублирующем скрипте. По каждой строке этого датафрейма сравниваются значения факторов по основному процессу и по дублирующему. В случае наличия расхождений формируется JSON, содержащий все пары расходящихся значений с кодами факторов.

Удобство для клиентов и пользователей

На текущий момент система экспресс-кредитования банка, благодаря существованию оптимальных алгоритмов и проверочных скриптов, позволяет обеспечить принятие решения по выдаче кредита клиента за семь минут с момента заведения заявки. Клиенту достаточно зайти на сайт банка, заполнить короткую анкету и получить решение. При этом клиентам нужно предоставить минимальное количество документов и согласий.

С точки зрения пользователей процесса внутри банка тоже удалось добиться значительного снижения трудозатрат при выдаче экспресс-кредитов. Больше нет необходимости проверять каждую заявку в ручном режиме и принимать решения индивидуально. Благодаря REST-API кредитный конвейер самостоятельно инициирует запрос расчета в дублирующем скрипте, получает отчет и, в случае отсутствия расхождений, пропускает заявку дальше. Таким образом, аналитикам приходится рассматривать только небольшой перечень заявок, по которым возникают расхождения.

В дальнейшем внедрение подобных технологий и их распространение на процессы классического кредитования также позволит сократить время до выдачи кредита, что будет способствовать устойчивому и быстрому развитию малого и среднего бизнеса в России."'https://habrastorage.org/getpro/habr/upload_files/598/789/535/5987895350f59cffd9c50d1244f9c077.png'"['https://habrastorage.org/r/w1560/getpro/habr/upload_files/a50/3c8/b95/a503c8b95dea691782dbcffa0fcf322f.png', 'https://habrastorage.org/getpro/habr/upload_files/598/789/535/5987895350f59cffd9c50d1244f9c077.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/getpro/habr/upload_files/92c/7a4/28d/92c7a428d2dab0962f721396970a6900.PNG', 'https://habrastorage.org/getpro/habr/company/cc1/43e/85f/cc143e85f89fc1f36cc741c053cf9c2e.jpg', 'https://habrastorage.org/getpro/habr/upload_files/e79/ae3/72d/e79ae372dd8057dc7a174adb438cd6b9.PNG', 'https://habrastorage.org/getpro/habr/upload_files/b32/d08/867/b32d08867b5c816187e19988df4cec1b.PNG', 'https://habrastorage.org/getpro/habr/upload_files/653/456/e0d/653456e0dc40065bea10940d126a490e.PNG']"
19'722872'Менеджмент сертификатов – как застраховаться от просрочки'Для обеспечения защищенного соединения между партнерами, подписи документов, а также организации защищенных каналов связи используются сертификаты. Они позволяют шифровать трафик либо...'https://habr.com/ru/post/722872/'"Для обеспечения защищенного соединения между партнерами, подписи документов, а также организации защищенных каналов связи используются сертификаты. Они позволяют шифровать трафик либо ставить цифровую подпись, которая равнозначна рукописной подписи. Так как это дело обычное и частое, то в компаниях, которые так или иначе связаны с онлайном, таких сертификатов великое множество.

Эти сертификаты имеют срок годности, они выпускаются не на 100 лет (технически это возможно, но по понятным причинам вариант не пользуется популярностью), а обычно на год. И частенько они просрочиваются — срок сертификата подходит к концу, об этом забывают и пропускают его смену, что приводит к потере денег или времени. Боль, на самом деле, повсеместная, и немногие пытаются успешно с ней бороться. У нас это получилось, и мы хотим поведать вам о нашем пути, который еще не закончен.

Как мы работали до внедрения единой системы управления сертификатами

У нас в QIWI была похожая боль. Весь обмен с партнерами ведется с использованием сертификатов, а ещё есть сертификаты для связи с ЦБ и другими организациями (Госуслугами, и так далее), соответственно, этих сертификатов очень много, за ними надо следить. Сейчас в нашей системе их более 16500.

Раньше каждый продукт и каждая команда сами пытались следить за своими сертификатами в формате «Кто во что горазд», поэтому иногда эти сертификаты просто-напросто протухали, и никто этого не замечал. В итоге мы не могли нормально взаимодействовать с нашими партнерами до момента замены сертификата. А чтобы его выпустить - приходилось “попрыгать”, причем иногда достаточно долго. :-( .

В конце концов мы осознали необходимость создания единой системы учета сертификатов, токенов и других сущностей, которые имеют срок действия и могут привести к потере времени и ресурсов Компании.

Начали мы со своей собственной разработки: сделали приложение с базой данных, которая хранила информацию о сертификатах, принимала её из разных источников, в которых у нас лежат сертификаты, агрегировала всё. А затем позволяла ставить какие‑то триггеры и оповещения — писать в трекер задач, в ту же Jira, чтобы об этих сертификатах вовремя вспоминали и производили их замену. Первая версия системы была написана достаточно быстро и не отличалась большой гибкостью в плане настроек эскалации и обработки данных по сертификатам. Многие настройки были зашиты в коде самой системы, и для их изменения и добавления приходилось отвлекать разработку. Соответственно, с системой было не очень удобно работать, но как первый шаг она была очень хороша.

Осознав, что данная система уже не отвечает нашим запросам, мы подумали, что неплохо посмотреть, что есть из подобных систем на рынке, и, возможно, купить какую‑то готовую.

Как мы думали, наверняка проблема с сертификатами есть не только у нас, и кто‑то уже писал подобное в виде коробочного решения.

Мы не ошиблись — такие решения присутствовали на рынке, но их было немного. Мы рассматривали как платные варианты, так и условно бесплатные (сама софтина была бесплатной, а вот саппорт уже платный). Попадались неплохие варианты в целом, но вот с кастомизацией у них была беда: или ты просто берешь готовую коробку и пользуешься только ею, или, при желании что‑то допилить под себя, платишь весьма и весьма серьезные суммы. Да ещё иногда при этом и неприлично долго ждешь.

Так мы пришли к пониманию того, что необходимо брать за основу уже существующий программный комплекс и доводить его до ума своими силами. В связи с этим нам надо было минимизировать сумму покупки и дальнейшего поддержания. То есть это должно было быть приложение условно бесплатное или с открытыми исходниками, с которым смогут разобраться наши специалисты.

В итоге мы нашли такое решение.

Что у нас сейчас

Выбрали мы систему CMDBuild . Это open‑source система — конструктор, который позволяет нам создать свою базу для учета любых активов. Все это дело при желании и необходимости кастомизируется и настраивается достаточно легко, наши специалисты очень быстро разобрались и приступили к донастройке коробочного решения.

В итоге мы перевезли из старого приложения сертификаты, настройки эскалации и механизмы обработки информации о сертификатах в CMDB. Также мы реализовали дополнительный сервис — CMDB‑интегратор, написан у нас в компании как дополнение к cmdb. Он производит обработку данных, входящих и хранящихся в CMDB, и на основе этих данных осуществляет процессы эскалации. Ещё в его задачи входит предоставление API для загрузки данных о сертификатах из источников и их обработка.

Вот как всё выглядит с точки зрения архитектуры:

CMDB является хранилищем, причем хранилищем как сертификатов, так и настроек; CMDB‑интегратор на основе этих настроек всю информацию обрабатывает.

Сейчас мы знаем почти обо всех наших сертификатах и токенах и точно видим, когда они должны закончиться. А если даже забываем, то построенная нами система автоматически напомнит об этом.

Делать она это может различными методами, например, мы можем совершенно спокойно отправлять уведомления нашим партнерам о том, что у них сертификат помирает, причем можно настроить письмо как в плане отправки по времени (например, автоматом рассылать за N дней до окончания срока действия сертификата), так и в плане контента — в письмо можно подставлять любые данные из текущего сертификата и любой текст.

У нас есть эскалация в тикеты в Jira, по каждому сертификату: если он протухает, то за настраиваемое время до его протухания можно создать необходимое количество тикетов в нужные подразделения. Дальше по этим тикетам начинает идти работа уже внутри задействованного в смене сертификата подразделении.

Эскалация в тикеты, кстати, тоже полностью настраиваемая, можно настроить:

тип тикета,

проект,

текст, который прилетит в тикет.

Система также умеет подтягивать информацию по клиенту из внутренней CRM‑системы компании, так как данные по сертификату мы получаем не совсем полные, знаем лишь, что это за сертификат, но не всегда понимаем, к какому партнеру он относится. Система по данным внутри сертификата может обратиться в CRM и получить оттуда информацию о партнере, которому принадлежит этот сертификат. Это дало нам возможность выдавать в тикетах более полную информацию. То есть подразделениям, которые обрабатывают смену сертификата, не надо залезать в основной процессинг или в CRM‑систему и искать партнера, к которому надо постучаться, чтобы он выпустил новый сертификат — вся информация уже есть в тикете.

Информация по сертификату в CMDB:

Тикет на смену сертификата:

Сертификаты в большинстве своем у нас лежат на серверах в хранилищах сертификатов. Так что наши специалисты написали скрипты и приложения, которые собирают все эти сертификаты из мест хранения и отправляют к нам. На момент написания поста у нас более 16 500 сертификатов в базе.

Что получили и что дальше

Сейчас доработка системы управления сертификатами идет полным ходом, и мы частенько находим новые нюансы и возможности.

Например из самого свежего — недавно удалось закрыть одну из болей, которая доставляла изрядно неудобств. Мы сделали возможность убирать из эскалации сертификаты, которые заменили ранее срока его смены. А то раньше была проблема — в некоторых системах сертификаты меняются автоматом, но в соответствующие подразделения прилетают тикеты о том, что уже замененный сертификат надо сменить. То есть мы видели, что сертификат сменился, так как начинал прилетать в систему новый, а старый не прилетал, но система никак на это не реагировала. В итоге сейчас мы знаем, когда сертификат сменили (он перестает к нам приходить) и автоматически убираем старый из эскалации.

Теперь у нас все очень хорошо. После введения этой системы мы получили очень приятную статистику:число каких‑то внезапных замен, когда нам нужно обновить сертификат в течение суток, сократилось до пары раз в год. Но это, скорее не проблема того, что сервис плохо отработал, а проблема любой большой компании — не всегда удается донести информацию до всех. Но мы работаем и в этом направлении и проводим регулярные активности по рекламе нашего сервиса внутри QIWI.

Сейчас этот процесс работает и дает полную информацию о сертификатах, о том, когда истекает срок, всё это трекается, в общем, стало сильно прозрачнее и мы всё видим и не забываем о своевременной смене.

Но, как вы понимаете, сделать только систему — это лишь половина пути. И, соответственно, мы сделали еще и процесс, который позволяет нам вовремя производить смены сертификатов:

Первичная эскалация идет в подразделения, которые занимаются сменой этого сертификата. Для контроля, чтобы что‑то не повисло, по каждому типу сертификатов за 5 дней ставится тикет в дежурную смену вида «Ребята, что‑то здесь не так, если этот тикет по смене сертификата еще не закрыт». Мы это делаем для того, чтобы дежурная смена потрясла нужное подразделение и подтолкнула его к тому, чтобы быстрее сменить сертификат. Еще 2–3 дня идет эскалация в руководство подразделений, которые занимаются сменой сертификатов. Мне как менеджеру изменений подсвечивается, что у нас сертификат скоро просрочится, а по нему какое‑то затишье. Не к добру это. Здесь идет более ощутимая пропинговка с нашей стороны, чтобы этот сертификат сменили.

Последний пункт уже стал редкостью, но иногда всё же бывает.

Кстати, если вы каким‑то образом тоже решаете проблемы с актуальностью сертификатов, позволю себе дать небольшой совет: не стоит при проверках грести все сертификаты, которые у вас есть, в одну кучу, попробуйте сразу понимать, что именно за сертификат перед вами, для чего он используется, что с ним надо делать. Это поможет в дальнейшем правильно настраивать эскалацию по таким видам сертификатов.

В ближайшем будущем мы хотим сделать интерфейс для загрузки сертификатов (интерфейс cmdb слишком перегружен). Сейчас это все уже есть в виде API, и если ребятам надо загружать сертификаты, то они делают либо какое‑то приложение, либо какой‑то скрипт, который нам грузит это. А хочется сделать интерфейс, чтобы человек мог просто руками набрать минимум информации и отправить ее нам. Ну и, конечно, хочется затащить как можно больше сертификатов в эту систему, и чтобы об этой системе знала вся компания. Но это — уже всего лишь вопрос времени :)"'https://habrastorage.org/getpro/habr/upload_files/c98/d0a/8ca/c98d0a8ca03d431ad06a5b97b70cb529.jpeg'"['https://habrastorage.org/r/w780q1/getpro/habr/upload_files/c98/d0a/8ca/c98d0a8ca03d431ad06a5b97b70cb529.jpeg', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/4e6/a52/682/4e6a5268245c636cdb1f407342643cab.png', 'https://habrastorage.org/getpro/habr/company/ccc/1f6/77c/ccc1f677cb751f34b5f261784b4a9a35.png', 'https://mc.yandex.ru/watch/24049213', 'https://habrastorage.org/r/w1560/getpro/habr/upload_files/7eb/a86/a61/7eba86a6162ffa8be672ac320f10d614.png', 'https://habrastorage.org/r/w780q1/getpro/habr/upload_files/763/352/805/763352805066fcee19c9f12105fbab5c.jpeg', 'https://habrastorage.org/getpro/habr/upload_files/c98/d0a/8ca/c98d0a8ca03d431ad06a5b97b70cb529.jpeg']"
